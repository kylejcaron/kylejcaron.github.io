<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kyle Caron</title>
<link>https://github.com/kylejcaron/kylejcaron.github.io/index.html</link>
<atom:link href="https://github.com/kylejcaron/kylejcaron.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Welcome to my blog! I'm a self-taught data scientist interested in causal inference and bayesian methods. I mainly use this blog to formalize what I learn, but hopefully others find it helpful as well.</description>
<generator>quarto-1.1.163</generator>
<lastBuildDate>Mon, 31 Oct 2022 04:00:00 GMT</lastBuildDate>
<item>
  <title>Useful Tools for Weibull Survival Analysis</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/weibull_survival_analysis/weibull_survival_tools.html</link>
  <description><![CDATA[ 




<section id="weibull-survival-analysis" class="level1">
<h1>Weibull Survival Analysis</h1>
<p>The Weibull distirbution is an excellent choice for many survival analysis problems - it has an interpretable parameterization that is highly flexible to a large number of phenomenon. The main advantage is that it can model how the risk of failure accelerates over time. This post will focus on the <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BWeibull%7D(k,%20%5Clambda)"> parameterization, although I hope to cover the Gumbel reparameterization in the future.</p>
<p>This post requires some base understanding of survival analysis. I’ll try to have another post in the future that discusses survival analysis at a more introductory level</p>
<section id="a-quick-recap-on-survival-curves" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-recap-on-survival-curves">A (quick) Recap on Survival Curves</h2>
<p>Survival Curves model time to some event (such as a failure) - they can tell you the probability of a binary event occurring at each future time point in somethings lifetime. An example survival curve is shown below:</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> typing <span class="im" style="color: #00769E;">import</span> <span class="op" style="color: #5E5E5E;">*</span></span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> pymc <span class="im" style="color: #00769E;">as</span> pm</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">import</span> matplotlib.ticker <span class="im" style="color: #00769E;">as</span> mtick</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sns</span>
<span id="cb1-7"><span class="im" style="color: #00769E;">from</span> weibull <span class="im" style="color: #00769E;">import</span> Weibull</span>
<span id="cb1-8"><span class="im" style="color: #00769E;">import</span> scipy</span>
<span id="cb1-9">plt.style.use(<span class="st" style="color: #20794D;">"seaborn"</span>)</span>
<span id="cb1-10"></span>
<span id="cb1-11">k <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span></span>
<span id="cb1-12">lambd <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span></span>
<span id="cb1-13"></span>
<span id="cb1-14">dist <span class="op" style="color: #5E5E5E;">=</span> Weibull(k, lambd)</span>
<span id="cb1-15">t <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">101</span>)</span>
<span id="cb1-16">St <span class="op" style="color: #5E5E5E;">=</span>  dist.survival(t)</span>
<span id="cb1-17"></span>
<span id="cb1-18">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">5</span>))</span>
<span id="cb1-19">ax.plot(t,St, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Survival Curve"</span>)</span>
<span id="cb1-20">ax.yaxis.set_major_formatter(mtick.PercentFormatter(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">0</span>))</span>
<span id="cb1-21">ax.set_xlabel(<span class="st" style="color: #20794D;">"Time"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb1-22">ax.set_ylabel(<span class="st" style="color: #20794D;">"Survival probability"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb1-23">ax.set_title(<span class="st" style="color: #20794D;">"How to Read a Survival Curve"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb1-24">ax.legend(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb1-25"></span>
<span id="cb1-26"><span class="co" style="color: #5E5E5E;"># Annotations</span></span>
<span id="cb1-27">xlim <span class="op" style="color: #5E5E5E;">=</span> ax.get_xlim()</span>
<span id="cb1-28">ylim <span class="op" style="color: #5E5E5E;">=</span> ax.get_ylim()</span>
<span id="cb1-29">x <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">20</span></span>
<span id="cb1-30">ax.vlines(x, <span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.05</span>, St[x<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>], color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"k"</span>, ls<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"--"</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>, lw<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.5</span>)</span>
<span id="cb1-31">ax.hlines(St[x<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>], <span class="dv" style="color: #AD0000;">0</span>, x, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"k"</span>, ls<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"--"</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>, lw<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">1.5</span>)</span>
<span id="cb1-32">ax.tick_params(axis<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'both'</span>, which<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'major'</span>, labelsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">14</span>)</span>
<span id="cb1-33"></span>
<span id="cb1-34">ax.annotate(<span class="ss" style="color: #20794D;">f'"There</span><span class="ch" style="color: #20794D;">\'</span><span class="ss" style="color: #20794D;">s a ~60% probability of an event </span><span class="ch" style="color: #20794D;">\n</span><span class="ss" style="color: #20794D;">still not occurring by time t=20"'</span>, </span>
<span id="cb1-35">        xy<span class="op" style="color: #5E5E5E;">=</span>(x,St[x<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>]),</span>
<span id="cb1-36">        xycoords<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"data"</span>, </span>
<span id="cb1-37">        xytext<span class="op" style="color: #5E5E5E;">=</span>(<span class="fl" style="color: #AD0000;">0.4</span>, <span class="fl" style="color: #AD0000;">0.8</span>),</span>
<span id="cb1-38">        textcoords<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'axes fraction'</span>,</span>
<span id="cb1-39">        arrowprops<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">dict</span>(facecolor<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'black'</span>, shrink<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.05</span>),</span>
<span id="cb1-40">        horizontalalignment<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'left'</span>,</span>
<span id="cb1-41">        verticalalignment<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">'top'</span>,</span>
<span id="cb1-42">        fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">12</span>,</span>
<span id="cb1-43">        bbox<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">dict</span>(boxstyle<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Round,pad=0.5"</span>, fc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"white"</span>, ec<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"gray"</span>, lw<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb1-44">)</span>
<span id="cb1-45">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/posts/weibull_survival_analysis/weibull_survival_tools_files/figure-html/cell-2-output-1.png" width="697" height="457"></p>
</div>
</div>
<p>They basically tell you the probability of a unit not observing an event up until some time point.</p>
</section>
<section id="a-quick-recap-on-hazard-curves" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-recap-on-hazard-curves">A (quick) Recap on Hazard Curves</h2>
<p>Hazard Curves are another way to think about survival analysis problems. A hazard curve tells you the instantaneous risk of an event occurring at each time point.</p>
<p>Hazard Curves tend to be very informative as they allow you to see how risk changes over time - sometimes it might decelerate, or sometimes it might even accelerate exponentially. Here’s an example of a hazard curve below.</p>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">k <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span></span>
<span id="cb2-2">lambd <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span></span>
<span id="cb2-3"></span>
<span id="cb2-4">dist <span class="op" style="color: #5E5E5E;">=</span> Weibull(k, lambd)</span>
<span id="cb2-5">t <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="dv" style="color: #AD0000;">1</span>, <span class="dv" style="color: #AD0000;">101</span>)</span>
<span id="cb2-6">ht <span class="op" style="color: #5E5E5E;">=</span>  dist.hazard(t)</span>
<span id="cb2-7"></span>
<span id="cb2-8">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">5</span>))</span>
<span id="cb2-9">ax.plot(t,ht, label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Hazard Curve"</span>)</span>
<span id="cb2-10">ax.set_xlabel(<span class="st" style="color: #20794D;">"Time"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb2-11">ax.set_ylabel(<span class="st" style="color: #20794D;">"Hazard Rate"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb2-12">ax.set_title(<span class="st" style="color: #20794D;">"An Example Hazard Curve"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb2-13">ax.legend(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb2-14"></span>
<span id="cb2-15">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/posts/weibull_survival_analysis/weibull_survival_tools_files/figure-html/cell-3-output-1.png" width="675" height="452"></p>
</div>
</div>
</section>
</section>
<section id="textweibullk-lambda-parameter-interpretation" class="level1">
<h1><img src="https://latex.codecogs.com/png.latex?%5Ctext%7BWeibull%7D(k,%20%5Clambda)"> Parameter Interpretation</h1>
<p>Let’s quickly cover how to interpret the parameters - we’ll do this via simulation.</p>
<section id="the-k-parameter" class="level2">
<h2 class="anchored" data-anchor-id="the-k-parameter">The <img src="https://latex.codecogs.com/png.latex?k"> Parameter</h2>
<p>Sometimes also called <img src="https://latex.codecogs.com/png.latex?%5Crho">, this parameter controls the degree of acceleration of the hazard curve.</p>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">ks <span class="op" style="color: #5E5E5E;">=</span> [<span class="fl" style="color: #AD0000;">0.5</span>, <span class="dv" style="color: #AD0000;">1</span>, <span class="fl" style="color: #AD0000;">1.5</span>, <span class="dv" style="color: #AD0000;">2</span>, <span class="fl" style="color: #AD0000;">2.5</span>]</span>
<span id="cb3-2">lambd <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span></span>
<span id="cb3-3">t <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">51</span>)</span>
<span id="cb3-4"></span>
<span id="cb3-5">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">5</span>))</span>
<span id="cb3-6"></span>
<span id="cb3-7"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(ks)):</span>
<span id="cb3-8">    dist <span class="op" style="color: #5E5E5E;">=</span> Weibull(ks[i], lambd)</span>
<span id="cb3-9">    ht <span class="op" style="color: #5E5E5E;">=</span> dist.hazard(t)</span>
<span id="cb3-10">    ax.plot(t, ht, label<span class="op" style="color: #5E5E5E;">=</span><span class="ss" style="color: #20794D;">f"k=</span><span class="sc" style="color: #5E5E5E;">{</span>ks[i]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>, lw<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb3-11"></span>
<span id="cb3-12">ax.set_xlabel(<span class="st" style="color: #20794D;">"Time"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb3-13">ax.set_ylabel(<span class="st" style="color: #20794D;">"Hazard Rate"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb3-14">ax.set_title(<span class="st" style="color: #20794D;">"Hazard Curves with Varying k Parameters"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb3-15">ax.legend(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb3-16">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/posts/weibull_survival_analysis/weibull_survival_tools_files/figure-html/cell-4-output-1.png" width="682" height="452"></p>
</div>
</div>
<p>As shown above, here’s how different parameter values impact acceleration:</p>
<ul>
<li><strong>When k &lt; 1:</strong> Hazard decelerates</li>
<li><strong>When k = 1:</strong> Hazard is constant. This is equivalent to an exponential distributions hazard function.</li>
<li><strong>When k &gt; 1:</strong> Hazard increases logarithmically</li>
<li><strong>When k = 2:</strong> Hazard increases with constant acceleration</li>
<li><strong>When k &gt; 2:</strong> Hazard increases exponentially</li>
</ul>
<p>Clothing is a great example of something where the hazard increases over time - the risk of a clothing item getting damaged obviously increases over time as the item is worn, washed, gets loose stitching, etc.</p>
</section>
<section id="the-lambda-parameter" class="level2">
<h2 class="anchored" data-anchor-id="the-lambda-parameter">The <img src="https://latex.codecogs.com/png.latex?%5Clambda"> Parameter</h2>
<p>Sometimes also referred to as a <em>scale parameter</em>, this parameter represents the time point where there’s a 63.2% probability of an event having occurred. Weird, I know, its just an arbitrary thing quality</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">k <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span></span>
<span id="cb4-2">lambd <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span></span>
<span id="cb4-3">dist <span class="op" style="color: #5E5E5E;">=</span> Weibull(k, lambd)</span>
<span id="cb4-4">t <span class="op" style="color: #5E5E5E;">=</span> np.linspace(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">100</span>, <span class="dv" style="color: #AD0000;">10000</span>)</span>
<span id="cb4-5">St <span class="op" style="color: #5E5E5E;">=</span> dist.survival(t)</span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;"># choose the point in the survival curve where the time is t=30</span></span>
<span id="cb4-8"><span class="co" style="color: #5E5E5E;"># take the inverse of it so that its the probability of an event having occurred by that time.</span></span>
<span id="cb4-9">(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>St[t<span class="op" style="color: #5E5E5E;">&gt;=</span>lambd][<span class="dv" style="color: #AD0000;">0</span>]).<span class="bu" style="color: null;">round</span>(<span class="dv" style="color: #AD0000;">4</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<pre><code>0.6321</code></pre>
</div>
</div>
<p>Let’s see this visually as well - if you look at the plot below, each survival curve indicates that there’s a 63.2% probability of an event having occurred at the exact time that <img src="https://latex.codecogs.com/png.latex?t=%5Clambda"></p>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1">ks <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span></span>
<span id="cb6-2">lambds <span class="op" style="color: #5E5E5E;">=</span> [<span class="dv" style="color: #AD0000;">15</span>, <span class="dv" style="color: #AD0000;">30</span>, <span class="dv" style="color: #AD0000;">60</span>, <span class="dv" style="color: #AD0000;">120</span>]</span>
<span id="cb6-3">t <span class="op" style="color: #5E5E5E;">=</span> np.arange(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">151</span>)</span>
<span id="cb6-4"></span>
<span id="cb6-5">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">10</span>,<span class="dv" style="color: #AD0000;">5</span>))</span>
<span id="cb6-6"></span>
<span id="cb6-7"><span class="cf" style="color: #003B4F;">for</span> i <span class="kw" style="color: #003B4F;">in</span> <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(lambds)):</span>
<span id="cb6-8">    dist <span class="op" style="color: #5E5E5E;">=</span> Weibull(k, lambds[i])</span>
<span id="cb6-9">    St <span class="op" style="color: #5E5E5E;">=</span> dist.survival(t)</span>
<span id="cb6-10">    ax.plot(t, St, label<span class="op" style="color: #5E5E5E;">=</span><span class="ss" style="color: #20794D;">f"lambd=</span><span class="sc" style="color: #5E5E5E;">{</span>lambds[i]<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>, lw<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>)</span>
<span id="cb6-11">    ax.axvline(lambds[i],ls<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"--"</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.5</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="ss" style="color: #20794D;">f'C</span><span class="sc" style="color: #5E5E5E;">{</span>i<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">'</span>)</span>
<span id="cb6-12"></span>
<span id="cb6-13">ax.axhline(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">0.632</span>, color<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"k"</span>, ls<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"--"</span>, alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.75</span>,</span>
<span id="cb6-14">    label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Point where there's a 63.2% probability</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">of the event having occurred</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">by that time"</span>)</span>
<span id="cb6-15"></span>
<span id="cb6-16">ax.set_xlabel(<span class="st" style="color: #20794D;">"Time"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb6-17">ax.set_ylabel(<span class="st" style="color: #20794D;">"Hazard Rate"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb6-18">ax.set_title(<span class="st" style="color: #20794D;">"Hazard Curves with Varying k Parameters"</span>, fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">16</span>)</span>
<span id="cb6-19">ax.legend(fontsize<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">12</span>, loc<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"upper right"</span>)</span>
<span id="cb6-20">plt.show()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/posts/weibull_survival_analysis/weibull_survival_tools_files/figure-html/cell-6-output-1.png" width="816" height="452"></p>
</div>
</div>
</section>
</section>
<section id="using-learned-parameters-for-applied-statistics" class="level1">
<h1>Using learned parameters for Applied Statistics</h1>
<p>Ok so lets say you have some estimator that learns the parameters of the weibull distribution - the goal here is to show different ways you can work with these parameters to make predictions</p>
<section id="easier-predictions-lifetime-survival-curves-and-hazard-curves" class="level2">
<h2 class="anchored" data-anchor-id="easier-predictions-lifetime-survival-curves-and-hazard-curves">Easier Predictions: Lifetime, Survival Curves, and Hazard Curves</h2>
<p>Average lifetime, Survival Curves, and Hazard Curves are the basic types of predictions for working with survival analysis. Most distributions have formulas that make these things easier to calculate. Below is the underlying code for these functions.</p>
<div class="sourceCode" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="kw" style="color: #003B4F;">class</span> Weibull:</span>
<span id="cb7-2"></span>
<span id="cb7-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, k, lambd):</span>
<span id="cb7-4">        <span class="va" style="color: #111111;">self</span>.k <span class="op" style="color: #5E5E5E;">=</span> k</span>
<span id="cb7-5">        <span class="va" style="color: #111111;">self</span>.lambd <span class="op" style="color: #5E5E5E;">=</span> lambd</span>
<span id="cb7-6"></span>
<span id="cb7-7">    ...</span>
<span id="cb7-8"></span>
<span id="cb7-9">    <span class="kw" style="color: #003B4F;">def</span> expectation(<span class="va" style="color: #111111;">self</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> np.array:</span>
<span id="cb7-10">        <span class="co" style="color: #5E5E5E;">'''Calculates the expectation of the weibull distribution</span></span>
<span id="cb7-11"><span class="co" style="color: #5E5E5E;">        '''</span></span>
<span id="cb7-12">        <span class="cf" style="color: #003B4F;">return</span> <span class="va" style="color: #111111;">self</span>.lambd <span class="op" style="color: #5E5E5E;">*</span> sp.gamma(<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="va" style="color: #111111;">self</span>.k)</span>
<span id="cb7-13"></span>
<span id="cb7-14">    <span class="kw" style="color: #003B4F;">def</span> survival(<span class="va" style="color: #111111;">self</span>, t: np.array) <span class="op" style="color: #5E5E5E;">-&gt;</span> np.array:</span>
<span id="cb7-15">        <span class="co" style="color: #5E5E5E;">'''Outputs the survival probability at each time point T. This is done with the survival function, </span></span>
<span id="cb7-16"><span class="co" style="color: #5E5E5E;">        the complement of the Weibull Distribution's PDF.</span></span>
<span id="cb7-17"></span>
<span id="cb7-18"><span class="co" style="color: #5E5E5E;">        Parameters</span></span>
<span id="cb7-19"><span class="co" style="color: #5E5E5E;">        -----------</span></span>
<span id="cb7-20"><span class="co" style="color: #5E5E5E;">            t: A numpy array with time points to calculate the survival curve,      </span></span>
<span id="cb7-21"><span class="co" style="color: #5E5E5E;">                utilizing the distributions parameters</span></span>
<span id="cb7-22"><span class="co" style="color: #5E5E5E;">        </span></span>
<span id="cb7-23"><span class="co" style="color: #5E5E5E;">        Returns</span></span>
<span id="cb7-24"><span class="co" style="color: #5E5E5E;">        -------</span></span>
<span id="cb7-25"><span class="co" style="color: #5E5E5E;">            St: A survival curve calculated over the inputted time period</span></span>
<span id="cb7-26"><span class="co" style="color: #5E5E5E;">        '''</span></span>
<span id="cb7-27">        CDF <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.cdf(t)</span>
<span id="cb7-28">        St <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">-</span> CDF</span>
<span id="cb7-29">        <span class="cf" style="color: #003B4F;">return</span> St</span>
<span id="cb7-30"></span>
<span id="cb7-31">    <span class="kw" style="color: #003B4F;">def</span> hazard(<span class="va" style="color: #111111;">self</span>, t: np.array) <span class="op" style="color: #5E5E5E;">-&gt;</span> np.array:</span>
<span id="cb7-32">        <span class="co" style="color: #5E5E5E;">'''Outputs the hazard rate at each time point T.</span></span>
<span id="cb7-33"></span>
<span id="cb7-34"><span class="co" style="color: #5E5E5E;">        Parameters</span></span>
<span id="cb7-35"><span class="co" style="color: #5E5E5E;">        -----------</span></span>
<span id="cb7-36"><span class="co" style="color: #5E5E5E;">            t: A numpy array with time points to calculate the survival curve,      </span></span>
<span id="cb7-37"><span class="co" style="color: #5E5E5E;">                utilizing the distributions parameters</span></span>
<span id="cb7-38"><span class="co" style="color: #5E5E5E;">        </span></span>
<span id="cb7-39"><span class="co" style="color: #5E5E5E;">        Returns</span></span>
<span id="cb7-40"><span class="co" style="color: #5E5E5E;">        -------</span></span>
<span id="cb7-41"><span class="co" style="color: #5E5E5E;">            St: A survival curve calculated over the inputted time period</span></span>
<span id="cb7-42"><span class="co" style="color: #5E5E5E;">        '''</span></span>
<span id="cb7-43">        ht <span class="op" style="color: #5E5E5E;">=</span> (<span class="va" style="color: #111111;">self</span>.k<span class="op" style="color: #5E5E5E;">/</span><span class="va" style="color: #111111;">self</span>.lambd)<span class="op" style="color: #5E5E5E;">*</span>(t<span class="op" style="color: #5E5E5E;">/</span><span class="va" style="color: #111111;">self</span>.lambd)<span class="op" style="color: #5E5E5E;">**</span>(<span class="va" style="color: #111111;">self</span>.k<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb7-44">        <span class="cf" style="color: #003B4F;">return</span> ht</span>
<span id="cb7-45">    </span>
<span id="cb7-46">    ...</span></code></pre></div>
<p>We already looked into hazard and survival curves - let’s take a look at estimating the average lifetime.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">k <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span></span>
<span id="cb8-2">lambd <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">30</span></span>
<span id="cb8-3"></span>
<span id="cb8-4">dist <span class="op" style="color: #5E5E5E;">=</span> Weibull(k, lambd)</span>
<span id="cb8-5">Et <span class="op" style="color: #5E5E5E;">=</span> dist.expectation()    </span>
<span id="cb8-6"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Expected Average Lifetime = </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">round</span>(Et,<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Expected Average Lifetime = 27.1</code></pre>
</div>
</div>
<p>Does this align with reality? Lets simulate event times from the weibull distribution to find out</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">event_times <span class="op" style="color: #5E5E5E;">=</span> dist.sample(n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1000000</span>)</span>
<span id="cb10-2"><span class="bu" style="color: null;">print</span>(<span class="ss" style="color: #20794D;">f"Simulated Average Lifetime = </span><span class="sc" style="color: #5E5E5E;">{</span><span class="bu" style="color: null;">round</span>(event_times.mean(),<span class="dv" style="color: #AD0000;">1</span>)<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span></code></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Simulated Average Lifetime = 27.1</code></pre>
</div>
</div>
</section>
<section id="predicting-residual-remaining-life" class="level2">
<h2 class="anchored" data-anchor-id="predicting-residual-remaining-life">Predicting Residual Remaining Life</h2>
<p>Another common and useful prediction you may want to present is the remaining life. There are a few different ways to calculate this:</p>
<pre><code>1. Use the formula</code></pre>
<p><img src="https://latex.codecogs.com/png.latex?%0AMRL(t)%20=%20%5Cfrac%7B%5Clambda%20%5C:%20%5CGamma(1%20+%201/k,%20(%5Cfrac%7Bt%7D%7B%5Clambda%7D)%5Ek)%7D%7BS(t)%7D%20-%20t%0A"></p>
<p>And we can see it implemented in python below:</p>
<div class="sourceCode" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;">class</span> Weibull:</span>
<span id="cb13-2"></span>
<span id="cb13-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, k, lambd):</span>
<span id="cb13-4">        <span class="va" style="color: #111111;">self</span>.k <span class="op" style="color: #5E5E5E;">=</span> k</span>
<span id="cb13-5">        <span class="va" style="color: #111111;">self</span>.lambd <span class="op" style="color: #5E5E5E;">=</span> lambd</span>
<span id="cb13-6"></span>
<span id="cb13-7">    ...</span>
<span id="cb13-8"></span>
<span id="cb13-9">    <span class="kw" style="color: #003B4F;">def</span> mean_residual_life(<span class="va" style="color: #111111;">self</span>, t: np.array) <span class="op" style="color: #5E5E5E;">-&gt;</span> np.array:</span>
<span id="cb13-10">        t <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._normalize_to_array(t)</span>
<span id="cb13-11">        St <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>.survival(t)</span>
<span id="cb13-12">        numerator <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb13-13">            <span class="va" style="color: #111111;">self</span>.lambd </span>
<span id="cb13-14">            <span class="op" style="color: #5E5E5E;">*</span> sp.gammaincc(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="va" style="color: #111111;">self</span>.k, (t<span class="op" style="color: #5E5E5E;">/</span><span class="va" style="color: #111111;">self</span>.lambd)<span class="op" style="color: #5E5E5E;">**</span><span class="va" style="color: #111111;">self</span>.k)</span>
<span id="cb13-15">            <span class="op" style="color: #5E5E5E;">*</span> sp.gamma(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">/</span><span class="va" style="color: #111111;">self</span>.k))</span>
<span id="cb13-16"></span>
<span id="cb13-17">        result <span class="op" style="color: #5E5E5E;">=</span> np.divide(</span>
<span id="cb13-18">            numerator,</span>
<span id="cb13-19">            St,</span>
<span id="cb13-20">            out<span class="op" style="color: #5E5E5E;">=</span>np.zeros_like(numerator),</span>
<span id="cb13-21">            where<span class="op" style="color: #5E5E5E;">=</span>St<span class="op" style="color: #5E5E5E;">!=</span><span class="dv" style="color: #AD0000;">0</span></span>
<span id="cb13-22">        ) <span class="op" style="color: #5E5E5E;">-</span> t[:,<span class="va" style="color: #111111;">None</span>]</span>
<span id="cb13-23">        </span>
<span id="cb13-24">        <span class="co" style="color: #5E5E5E;"># The code above returns negative values when St=0. This clipping corrects those cases</span></span>
<span id="cb13-25">        <span class="cf" style="color: #003B4F;">return</span> np.clip(result, <span class="dv" style="color: #AD0000;">0</span>, np.inf)</span></code></pre></div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">curr_time <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">40</span></span>
<span id="cb14-2">k, lambd <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span>, <span class="dv" style="color: #AD0000;">30</span></span>
<span id="cb14-3">dist <span class="op" style="color: #5E5E5E;">=</span> Weibull(k,lambd)</span>
<span id="cb14-4">remaining_life <span class="op" style="color: #5E5E5E;">=</span> dist.mean_residual_life(curr_time)</span></code></pre></div>
</div>
<pre><code>2. Simulate</code></pre>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb16" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1">T <span class="op" style="color: #5E5E5E;">=</span> dist.sample(n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10000000</span>)</span>
<span id="cb16-2">units_reached_curr_time <span class="op" style="color: #5E5E5E;">=</span> T[T<span class="op" style="color: #5E5E5E;">&gt;</span>curr_time] <span class="op" style="color: #5E5E5E;">-</span> curr_time</span>
<span id="cb16-3">remaining_life_simulated <span class="op" style="color: #5E5E5E;">=</span> units_reached_curr_time.mean()</span></code></pre></div>
</div>
<p>With truncated sampling, this can also be more efficient</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1">T_cond <span class="op" style="color: #5E5E5E;">=</span> dist.sample(n<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">10000000</span>,left_trunc<span class="op" style="color: #5E5E5E;">=</span>curr_time) <span class="op" style="color: #5E5E5E;">-</span> curr_time</span>
<span id="cb17-2">remaining_life_simulated2 <span class="op" style="color: #5E5E5E;">=</span> T_cond.mean()</span></code></pre></div>
</div>
<pre><code>3. Transform the hazard curve</code></pre>
<p>There’s a convenient relationship between the hazard curves, survival curves, and expectations. It turns out that</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AS(t)%20=%20%5Ctext%7Bexp%7D(-Ht)%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?Ht"> is the cumulative hazard function. Additionally, the expectation is just the integral of the survival function</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5BT%5D%20=%20%5Cint_%7Bt=0%7D%5E%5Cinfty%20S(t)%20%5C,%20dt%0A"></p>
<p>Using these relationships, we can 1. take the hazard curve from time 40 onwards 2. turn it into the cumulative hazard 3. transform that into a survival curve 4. integrate the survival curve into an expectation</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1">ht <span class="op" style="color: #5E5E5E;">=</span> dist.hazard(np.arange(curr_time, curr_time<span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1000</span>))</span>
<span id="cb19-2">Ht <span class="op" style="color: #5E5E5E;">=</span> ht.cumsum()</span>
<span id="cb19-3">St <span class="op" style="color: #5E5E5E;">=</span> np.exp(<span class="op" style="color: #5E5E5E;">-</span>Ht)</span>
<span id="cb19-4">remaining_life_from_hazard <span class="op" style="color: #5E5E5E;">=</span> scipy.integrate.simps(St)</span></code></pre></div>
</div>
<p>Comparing all of these methods we end up with the following:</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><span class="bu" style="color: null;">print</span>(</span>
<span id="cb20-2">    <span class="st" style="color: #20794D;">"Remaining Life (from formula) = </span><span class="sc" style="color: #5E5E5E;">{:.4}</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>.<span class="bu" style="color: null;">format</span>(remaining_life.ravel()[<span class="dv" style="color: #AD0000;">0</span>]),</span>
<span id="cb20-3">    <span class="st" style="color: #20794D;">"Remaining Life (from simulation) = </span><span class="sc" style="color: #5E5E5E;">{:.4}</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>.<span class="bu" style="color: null;">format</span>(remaining_life_simulated),</span>
<span id="cb20-4">    <span class="st" style="color: #20794D;">"Remaining Life (from truncated simulation) = </span><span class="sc" style="color: #5E5E5E;">{:.4}</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>.<span class="bu" style="color: null;">format</span>(remaining_life_simulated2),</span>
<span id="cb20-5">    <span class="st" style="color: #20794D;">"Remaining Life (manually calculated from hazard) = </span><span class="sc" style="color: #5E5E5E;">{:.4}</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">"</span>.<span class="bu" style="color: null;">format</span>(remaining_life_from_hazard)</span>
<span id="cb20-6">)</span></code></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Remaining Life (from formula) = 15.05
 Remaining Life (from simulation) = 15.04
 Remaining Life (from truncated simulation) = 15.05
 Remaining Life (manually calculated from hazard) = 14.14
</code></pre>
</div>
</div>
<p>They’re all very close, but it looks like theres some slight error when manually calculating (and its more complicated).</p>
</section>
<section id="calculating-conditional-survival-curves" class="level2">
<h2 class="anchored" data-anchor-id="calculating-conditional-survival-curves">Calculating Conditional Survival Curves</h2>
<p>The last type of prediction we’ll show here is conditional survival. This is basically saying “what’s the survival curve if we’ve made it up to time=t?”</p>
<p>It turns out, all you have to do is calculate the survival curve and normalize it by the eligible area of the distirbution. This basically means calculating a survival curve past the current time, and then dividing it by the survival function at the current time of interest.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AS(t%7C%5Ctext%7Bcurr%20time%7D=40)%20=%20S(t)/S(40)%0A"></p>
<p>It was pretty easy to add that to the <code>Weibull.survival()</code> function from earlier:</p>
<div class="sourceCode" id="cb22" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><span class="kw" style="color: #003B4F;">class</span> Weibull:</span>
<span id="cb22-2"></span>
<span id="cb22-3">    <span class="kw" style="color: #003B4F;">def</span> <span class="fu" style="color: #4758AB;">__init__</span>(<span class="va" style="color: #111111;">self</span>, k, lambd):</span>
<span id="cb22-4">        <span class="va" style="color: #111111;">self</span>.k <span class="op" style="color: #5E5E5E;">=</span> k</span>
<span id="cb22-5">        <span class="va" style="color: #111111;">self</span>.lambd <span class="op" style="color: #5E5E5E;">=</span> lambd</span>
<span id="cb22-6">    </span>
<span id="cb22-7">    ...</span>
<span id="cb22-8"></span>
<span id="cb22-9">    <span class="kw" style="color: #003B4F;">def</span> survival(<span class="va" style="color: #111111;">self</span>, t: np.array, curr_time: Optional[<span class="bu" style="color: null;">int</span>] <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">None</span>) <span class="op" style="color: #5E5E5E;">-&gt;</span> np.array:</span>
<span id="cb22-10">        <span class="co" style="color: #5E5E5E;">'''Outputs the survival probability at each time point T. This is done with the survival function, </span></span>
<span id="cb22-11"><span class="co" style="color: #5E5E5E;">        the complement of the Weibull Distribution's PDF.</span></span>
<span id="cb22-12"></span>
<span id="cb22-13"><span class="co" style="color: #5E5E5E;">        Can also be used to calculate conditional survival with the `curr_time` argument.</span></span>
<span id="cb22-14"></span>
<span id="cb22-15"><span class="co" style="color: #5E5E5E;">        Parameters</span></span>
<span id="cb22-16"><span class="co" style="color: #5E5E5E;">        -----------</span></span>
<span id="cb22-17"><span class="co" style="color: #5E5E5E;">            t: A numpy array with time points to calculate the survival curve,      </span></span>
<span id="cb22-18"><span class="co" style="color: #5E5E5E;">                utilizing the distributions parameters</span></span>
<span id="cb22-19"><span class="co" style="color: #5E5E5E;">            curr_time: Used to calculate the survival curve given already reaching </span></span>
<span id="cb22-20"><span class="co" style="color: #5E5E5E;">                some point in time, `curr_time`.</span></span>
<span id="cb22-21"><span class="co" style="color: #5E5E5E;">        </span></span>
<span id="cb22-22"><span class="co" style="color: #5E5E5E;">        Returns</span></span>
<span id="cb22-23"><span class="co" style="color: #5E5E5E;">        -------</span></span>
<span id="cb22-24"><span class="co" style="color: #5E5E5E;">            St: A survival curve calculated over the inputted time period</span></span>
<span id="cb22-25"><span class="co" style="color: #5E5E5E;">        '''</span></span>
<span id="cb22-26">        <span class="co" style="color: #5E5E5E;"># Normalizing constant used for conditional survival</span></span>
<span id="cb22-27">        norm <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">1</span> <span class="cf" style="color: #003B4F;">if</span> curr_time <span class="kw" style="color: #003B4F;">is</span> <span class="va" style="color: #111111;">None</span> <span class="cf" style="color: #003B4F;">else</span> <span class="va" style="color: #111111;">self</span>.survival(curr_time)</span>
<span id="cb22-28">        </span>
<span id="cb22-29">        <span class="co" style="color: #5E5E5E;"># check inputs</span></span>
<span id="cb22-30">        t <span class="op" style="color: #5E5E5E;">=</span> <span class="va" style="color: #111111;">self</span>._normalize_to_array(t)</span>
<span id="cb22-31">        <span class="cf" style="color: #003B4F;">if</span> curr_time <span class="kw" style="color: #003B4F;">is</span> <span class="kw" style="color: #003B4F;">not</span> <span class="va" style="color: #111111;">None</span> <span class="kw" style="color: #003B4F;">and</span> (t <span class="op" style="color: #5E5E5E;">&lt;</span> curr_time).<span class="bu" style="color: null;">sum</span>() <span class="op" style="color: #5E5E5E;">&gt;</span> <span class="dv" style="color: #AD0000;">1</span>:</span>
<span id="cb22-32">            <span class="cf" style="color: #003B4F;">raise</span> <span class="pp" style="color: #AD0000;">ValueError</span>(<span class="st" style="color: #20794D;">'t&lt;curr_time. t must be greater than or equal to curr_time'</span>)</span>
<span id="cb22-33">        </span>
<span id="cb22-34">        St <span class="op" style="color: #5E5E5E;">=</span> (<span class="dv" style="color: #AD0000;">1</span> <span class="op" style="color: #5E5E5E;">-</span> <span class="va" style="color: #111111;">self</span>.cdf(t))<span class="op" style="color: #5E5E5E;">/</span>norm</span>
<span id="cb22-35">        <span class="cf" style="color: #003B4F;">return</span> St</span></code></pre></div>
<p>And as we can see below, it lines up perfectly with an empirically calculated kaplan meier curve</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><span class="im" style="color: #00769E;">from</span> lifelines <span class="im" style="color: #00769E;">import</span> KaplanMeierFitter</span>
<span id="cb23-2"></span>
<span id="cb23-3">curr_time <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">40</span></span>
<span id="cb23-4">T_cond <span class="op" style="color: #5E5E5E;">=</span> dist.sample(<span class="dv" style="color: #AD0000;">1000000</span>, left_trunc<span class="op" style="color: #5E5E5E;">=</span>curr_time) <span class="op" style="color: #5E5E5E;">-</span> curr_time</span>
<span id="cb23-5">kmf <span class="op" style="color: #5E5E5E;">=</span> KaplanMeierFitter()</span>
<span id="cb23-6">kmf.fit(T_cond)</span>
<span id="cb23-7">kmf.plot_survival_function(lw<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">5</span>, ls<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"--"</span>)</span>
<span id="cb23-8"></span>
<span id="cb23-9">plt.plot(dist.survival(t<span class="op" style="color: #5E5E5E;">=</span>np.arange(<span class="dv" style="color: #AD0000;">40</span>, <span class="dv" style="color: #AD0000;">200</span><span class="op" style="color: #5E5E5E;">+</span><span class="dv" style="color: #AD0000;">1</span>), curr_time<span class="op" style="color: #5E5E5E;">=</span>curr_time), label<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"Conditional Survival Curve"</span>)</span>
<span id="cb23-10">plt.legend()</span>
<span id="cb23-11">plt.ylabel(<span class="st" style="color: #20794D;">"Conditional Survival Rate"</span>)</span>
<span id="cb23-12">plt.title(<span class="ss" style="color: #20794D;">f"Conditional Survival Given a Unit Reached Time=</span><span class="sc" style="color: #5E5E5E;">{</span>curr_time<span class="sc" style="color: #5E5E5E;">}</span><span class="ss" style="color: #20794D;">"</span>)</span>
<span id="cb23-13">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/posts/weibull_survival_analysis/weibull_survival_tools_files/figure-html/cell-14-output-1.png" width="662" height="478"></p>
</div>
</div>
<p>A convenient feature about survival curves is that if we want to know the probability of an event occurring in the next 1 time period, or 2 time periods, all we have to do is take the complement of the survival probability. So that means we can also use this method to calculate <em>“probability of an event occurring in the next 1 time period”</em></p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>That’s it for this post on working with Weibull Survival Analysis. There are also other re-parameterizations such as the Gumbel parameterization that I’d like to explore more later, but I found this collection of functionality really helpful in my work building out custom bayesian survival models.</p>


</section>

 ]]></description>
  <category>survival analysis</category>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/weibull_survival_analysis/weibull_survival_tools.html</guid>
  <pubDate>Mon, 31 Oct 2022 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Why do we need A/B tests? The Potential Outcomes Model</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/experiment_design.html</link>
  <description><![CDATA[ 




<section id="overview" class="level1">
<h1>Overview</h1>
<p>This blog post introduces the Potential Outcomes Model and introduces why experiments are often necessary to measure what we want. This topic is already covered extensively in other more rigorous resources. This post provides just another example.</p>
</section>
<section id="the-potential-outcomes-model" class="level1">
<h1>The Potential Outcomes Model</h1>
<p>Let’s say we want to know the effect of of a customer support product on a customer outcome such as customer lifetime value LTV. Customers who might seem particularly upset when on the phone with customer support will be more likely to receive a promo code from the customer support staff, which we label as <img src="https://latex.codecogs.com/png.latex?T=1"> (or treatment = True). We represent the outcome, their customer lifetime value (assuming we can observe their full LTV), as <img src="https://latex.codecogs.com/png.latex?Y(1)">, which really just means <em>“what is the outcome Y for customers who had the treatment”</em>.</p>
<section id="a-hypothetical-world" class="level2">
<h2 class="anchored" data-anchor-id="a-hypothetical-world">A Hypothetical world</h2>
<p>What if we envision some hypothetical world we can observe the outcome for each customer who reached out to customer support, with and without having the treatment of receiving a promo?</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">import</span> pandas <span class="im" style="color: #00769E;">as</span> pd</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">import</span> numpy <span class="im" style="color: #00769E;">as</span> np</span>
<span id="cb1-3"><span class="im" style="color: #00769E;">import</span> scipy.special <span class="im" style="color: #00769E;">as</span> sp</span>
<span id="cb1-4"><span class="im" style="color: #00769E;">import</span> matplotlib.pyplot <span class="im" style="color: #00769E;">as</span> plt</span>
<span id="cb1-5"><span class="im" style="color: #00769E;">import</span> seaborn <span class="im" style="color: #00769E;">as</span> sns</span>
<span id="cb1-6"><span class="im" style="color: #00769E;">import</span> statsmodels.api <span class="im" style="color: #00769E;">as</span> sm</span>
<span id="cb1-7"></span>
<span id="cb1-8">rng <span class="op" style="color: #5E5E5E;">=</span> np.random.default_rng(<span class="dv" style="color: #AD0000;">100</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10">N<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">100_000</span></span>
<span id="cb1-11">upset <span class="op" style="color: #5E5E5E;">=</span> rng.normal(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>, N)</span>
<span id="cb1-12"></span>
<span id="cb1-13"><span class="kw" style="color: #003B4F;">def</span> sim_treatment(upset, rng, return_prob<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">False</span>):</span>
<span id="cb1-14">    beta <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">1.5</span></span>
<span id="cb1-15">    p_treatment <span class="op" style="color: #5E5E5E;">=</span> sp.expit( <span class="op" style="color: #5E5E5E;">-</span><span class="fl" style="color: #AD0000;">2.5</span> <span class="op" style="color: #5E5E5E;">+</span> upset <span class="op" style="color: #5E5E5E;">*</span> beta)</span>
<span id="cb1-16">    <span class="cf" style="color: #003B4F;">if</span> return_prob:</span>
<span id="cb1-17">        <span class="cf" style="color: #003B4F;">return</span> p_treatment</span>
<span id="cb1-18">    <span class="cf" style="color: #003B4F;">return</span> rng.binomial(<span class="dv" style="color: #AD0000;">1</span>, p_treatment)</span>
<span id="cb1-19"></span>
<span id="cb1-20"><span class="kw" style="color: #003B4F;">def</span> sim_outcome(upset, treatment, rng):</span>
<span id="cb1-21">    eps <span class="op" style="color: #5E5E5E;">=</span> rng.normal(<span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">150</span>, size<span class="op" style="color: #5E5E5E;">=</span><span class="bu" style="color: null;">len</span>(upset))</span>
<span id="cb1-22">    ltv <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">2500</span> <span class="op" style="color: #5E5E5E;">+</span> <span class="dv" style="color: #AD0000;">500</span><span class="op" style="color: #5E5E5E;">*</span>treatment <span class="op" style="color: #5E5E5E;">+</span> <span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">500</span><span class="op" style="color: #5E5E5E;">*</span>upset <span class="op" style="color: #5E5E5E;">+</span> eps </span>
<span id="cb1-23">    <span class="cf" style="color: #003B4F;">return</span> ltv.<span class="bu" style="color: null;">round</span>(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb1-24"></span>
<span id="cb1-25">data <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame({</span>
<span id="cb1-26">    <span class="st" style="color: #20794D;">"Person"</span>: np.arange(N),</span>
<span id="cb1-27">    <span class="st" style="color: #20794D;">"upset"</span>: upset,</span>
<span id="cb1-28">    <span class="st" style="color: #20794D;">"T"</span>: sim_treatment(upset, rng),</span>
<span id="cb1-29">    <span class="st" style="color: #20794D;">"Y(0)"</span>: sim_outcome(upset, np.zeros(N), rng),</span>
<span id="cb1-30">    <span class="st" style="color: #20794D;">"Y(1)"</span>: sim_outcome(upset, np.ones(N), rng)</span>
<span id="cb1-31">}).set_index(<span class="st" style="color: #20794D;">"Person"</span>)<span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb1-32">  .assign(ITE <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> d: d[<span class="st" style="color: #20794D;">"Y(1)"</span>] <span class="op" style="color: #5E5E5E;">-</span> d[<span class="st" style="color: #20794D;">"Y(0)"</span>])<span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb1-33">  .assign(Y <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> d: np.where(d[<span class="st" style="color: #20794D;">"T"</span>] <span class="op" style="color: #5E5E5E;">==</span> <span class="dv" style="color: #AD0000;">1</span>, d[<span class="st" style="color: #20794D;">"Y(1)"</span>], d[<span class="st" style="color: #20794D;">"Y(0)"</span>]) )</span>
<span id="cb1-34"></span>
<span id="cb1-35">data.head()[[<span class="st" style="color: #20794D;">"T"</span>, <span class="st" style="color: #20794D;">"Y(0)"</span>, <span class="st" style="color: #20794D;">"Y(1)"</span>, <span class="st" style="color: #20794D;">"ITE"</span>]]</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>T</th>
      <th>Y(0)</th>
      <th>Y(1)</th>
      <th>ITE</th>
    </tr>
    <tr>
      <th>Person</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3108.62</td>
      <td>3583.87</td>
      <td>475.25</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>2347.01</td>
      <td>2878.23</td>
      <td>531.22</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>2176.28</td>
      <td>2379.30</td>
      <td>203.02</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>2146.09</td>
      <td>2559.96</td>
      <td>413.87</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2806.50</td>
      <td>3623.16</td>
      <td>816.66</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>As shown above, in this hypothetical world we can see the exact <strong>individual treatment effect (ITE)</strong> for every customer.</p>
<pre><code>- Person 0 would have spent $475.25 more over their lifetime  if they received the promo
- Person 2 would have spend $203.02 more over their lifetime if they received the promo</code></pre>
<p>If we want to know the <strong>Average Treatment Effect (ATE, often denoted <img src="https://latex.codecogs.com/png.latex?%5Ctau">)</strong>, all we have to do is take the mean of all of the individual treatment effects. As we can see, the ATE is about $500</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctau%20=%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum%5E%7BN%7D_%7Bi=0%7D%20Y_i(1)%20-%20Y_i(0)%0A"></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">data.ITE.mean()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>500.09949529999994</code></pre>
</div>
</div>
<p>We can also represent this in hypothetical terms that will be useful later - the <strong>average treatment effect of the treated (ATT)</strong>, and the <strong>average treatment effect of the untreated (ATU)</strong>. The true ATE ends up being the weighted average of these terms, weighted by the proportion of individuals seeing the treatment, <img src="https://latex.codecogs.com/png.latex?%5Cpi"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctau%20&amp;%20=%20%5Cpi%20%5Ccdot%20E%5B%5Ctau%20%7C%20T=1%5D%20+%20(1-%5Cpi)%20%5Ccdot%20E%5B%5Ctau%20%7C%20T=%200%5D%20%5C%5C%0A%20%20%20%20%20&amp;%20=%20%5Cpi%20%5Ccdot%20%5Ctext%7BATT%7D%20+%20(1-%5Cpi)%20%5Ccdot%20%5Ctext%7BATU%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>We can confirm that this is equivalent to the ATE from above with code</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1">pi <span class="op" style="color: #5E5E5E;">=</span> data[<span class="st" style="color: #20794D;">"T"</span>].value_counts(normalize<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb5-2">(pi <span class="op" style="color: #5E5E5E;">*</span> data.groupby(<span class="st" style="color: #20794D;">"T"</span>).mean()[<span class="st" style="color: #20794D;">"ITE"</span>]).<span class="bu" style="color: null;">sum</span>()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>500.0994953</code></pre>
</div>
</div>
</section>
<section id="getting-hit-with-the-real-world" class="level2">
<h2 class="anchored" data-anchor-id="getting-hit-with-the-real-world">Getting hit with the real world</h2>
<p>So how can we create a scenario where we can observe each person with and without having received the promo? Sadly, we can’t. But is there a way to make use of data we already have? Here’s the actual data we might have access to. Notice that now the hypothetical potential outcomes are no longer visible (just like in the real world).</p>
<div class="cell" data-execution_count="4">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><span class="co" style="color: #5E5E5E;"># Real world data</span></span>
<span id="cb7-2">df <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb7-3">    data[[<span class="st" style="color: #20794D;">"upset"</span>, <span class="st" style="color: #20794D;">"T"</span>, <span class="st" style="color: #20794D;">"Y(0)"</span>, <span class="st" style="color: #20794D;">"Y(1)"</span>, <span class="st" style="color: #20794D;">"ITE"</span>, <span class="st" style="color: #20794D;">"Y"</span>]]</span>
<span id="cb7-4">    .assign(<span class="op" style="color: #5E5E5E;">**</span>{</span>
<span id="cb7-5">        <span class="st" style="color: #20794D;">"Y(0)"</span>:<span class="kw" style="color: #003B4F;">lambda</span> d: np.where(d[<span class="st" style="color: #20794D;">"T"</span>]<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">1</span>, np.NaN, d[<span class="st" style="color: #20794D;">"Y(0)"</span>]),</span>
<span id="cb7-6">        <span class="st" style="color: #20794D;">"Y(1)"</span>:<span class="kw" style="color: #003B4F;">lambda</span> d: np.where(d[<span class="st" style="color: #20794D;">"T"</span>]<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>, np.NaN, d[<span class="st" style="color: #20794D;">"Y(1)"</span>]),</span>
<span id="cb7-7">        <span class="st" style="color: #20794D;">"ITE"</span>: np.NAN</span>
<span id="cb7-8">        })</span>
<span id="cb7-9">)</span>
<span id="cb7-10"></span>
<span id="cb7-11">df.iloc[:,<span class="dv" style="color: #AD0000;">1</span>:].head()</span></code></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="15">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>T</th>
      <th>Y(0)</th>
      <th>Y(1)</th>
      <th>ITE</th>
      <th>Y</th>
    </tr>
    <tr>
      <th>Person</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>3108.62</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3108.62</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>2347.01</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2347.01</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>NaN</td>
      <td>2379.3</td>
      <td>NaN</td>
      <td>2379.30</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>2146.09</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2146.09</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>2806.50</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2806.50</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>One (unfortunately incorrect) idea might be take the average of Y(1) and subtract the average of Y(0), also known as the <strong>simple difference in outcomes (SDO)</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BSDO%7D%20=%20E%5B%20Y(1)%20%7C%20T%20=%201%20%5D%20-%20E%5B%20Y(0)%20%7C%20T%20=%200%20%5D%0A"></p>
<blockquote class="blockquote">
<p>Notice that I use the terms <img src="https://latex.codecogs.com/png.latex?E%5B%20Y(0)%20%7C%20T%20=%200%20%5D"> and <img src="https://latex.codecogs.com/png.latex?E%5B%20Y(1)%20%7C%20T%20=%201%20%5D">. Reading these as plain english “the expected value (aka mean) of Y(0) given no treatment” and “the expected value (aka mean) of Y(1) given a treatment”</p>
</blockquote>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb8" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1">(</span>
<span id="cb8-2">    df.groupby(<span class="st" style="color: #20794D;">"T"</span>)</span>
<span id="cb8-3">    .mean()[[<span class="st" style="color: #20794D;">"Y"</span>]].T</span>
<span id="cb8-4">    .assign(tau <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> d: d[<span class="dv" style="color: #AD0000;">1</span>] <span class="op" style="color: #5E5E5E;">-</span> d[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb8-5">    .rename(columns<span class="op" style="color: #5E5E5E;">=</span>{<span class="dv" style="color: #AD0000;">0</span>:<span class="st" style="color: #20794D;">"E[ Y(0) | T = 0 ]"</span>, <span class="dv" style="color: #AD0000;">1</span>:<span class="st" style="color: #20794D;">"E[ Y(1) | T = 1 ]"</span>})</span>
<span id="cb8-6">    .rename_axis(<span class="va" style="color: #111111;">None</span>, axis<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb8-7">    .<span class="bu" style="color: null;">round</span>(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb8-8">    .reset_index(drop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb8-9">)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>E[ Y(0) | T = 0 ]</th>
      <th>E[ Y(1) | T = 1 ]</th>
      <th>tau</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2579.46</td>
      <td>2491.48</td>
      <td>-87.98</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p><strong>Under the SDO it looks like the treatment has a negative effect</strong> - this is saying that giving customers a promo makes their LTV $88 worse? That seems seriously wrong, and is a huge problem. It should be $500 like we saw in our hypothetical world. So what went wrong?</p>
</section>
<section id="selection-bias" class="level2">
<h2 class="anchored" data-anchor-id="selection-bias">Selection Bias</h2>
<p>We can illustrate the problem by bringing another variable into the mix - customer unhappiness (we’re pretending we can measure it directly for examples sake).</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">2</span>, figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">8</span>,<span class="dv" style="color: #AD0000;">3</span>))</span>
<span id="cb9-2">ax[<span class="dv" style="color: #AD0000;">0</span>].set_title(<span class="st" style="color: #20794D;">"Histogram of</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">Customer unhappiness"</span>)</span>
<span id="cb9-3">df.upset.hist(ax<span class="op" style="color: #5E5E5E;">=</span>ax[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb9-4"></span>
<span id="cb9-5">ax[<span class="dv" style="color: #AD0000;">1</span>].set_title(<span class="st" style="color: #20794D;">"More upset customers are</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">more likely to receive a promo"</span>)</span>
<span id="cb9-6">ax[<span class="dv" style="color: #AD0000;">1</span>].set_ylabel(<span class="st" style="color: #20794D;">"Proportion Receiving Promo"</span>)</span>
<span id="cb9-7">df.groupby(df.upset<span class="op" style="color: #5E5E5E;">//</span><span class="fl" style="color: #AD0000;">0.25</span><span class="op" style="color: #5E5E5E;">*</span><span class="fl" style="color: #AD0000;">0.25</span>).mean()[<span class="st" style="color: #20794D;">"T"</span>].plot(ax<span class="op" style="color: #5E5E5E;">=</span>ax[<span class="dv" style="color: #AD0000;">1</span>])</span>
<span id="cb9-8">plt.tight_layout()</span>
<span id="cb9-9"></span>
<span id="cb9-10">df.head()</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>upset</th>
      <th>T</th>
      <th>Y(0)</th>
      <th>Y(1)</th>
      <th>ITE</th>
      <th>Y</th>
    </tr>
    <tr>
      <th>Person</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1.157550</td>
      <td>0</td>
      <td>3108.62</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>3108.62</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.289756</td>
      <td>0</td>
      <td>2347.01</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2347.01</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.780854</td>
      <td>1</td>
      <td>NaN</td>
      <td>2379.3</td>
      <td>NaN</td>
      <td>2379.30</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.543974</td>
      <td>0</td>
      <td>2146.09</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2146.09</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.961383</td>
      <td>0</td>
      <td>2806.50</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>2806.50</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-display">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/posts/experiment_design_files/figure-html/cell-7-output-2.png" width="758" height="279"></p>
</div>
</div>
<p>It looks like the most unhappy customers are the most likely to receive a treatment as shown in the DAG below.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<p>
<svg width="672" height="480" viewbox="0.00 0.00 220.71 188.00" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 184)">
<title>
G
</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-184 216.71,-184 216.71,4 -4,4"></polygon> <!-- a --> <g id="node1" class="node">
<title>
a
</title>
<ellipse fill="none" stroke="black" cx="122.22" cy="-162" rx="90.47" ry="18"></ellipse> <text text-anchor="middle" x="122.22" y="-157.8" font-family="Arial" font-size="14.00">unhappy customer</text> </g> <!-- b --> <g id="node2" class="node">
<title>
b
</title>
<ellipse fill="none" stroke="black" cx="72.22" cy="-90" rx="72.45" ry="18"></ellipse> <text text-anchor="middle" x="72.22" y="-85.8" font-family="Arial" font-size="14.00">receive promo</text> </g> <!-- a&#45;&gt;b --> <g id="edge1" class="edge">
<title>
a-&gt;b
</title>
<path fill="none" stroke="black" d="M110.12,-144.05C104.1,-135.63 96.71,-125.28 90.06,-115.97"></path> <polygon fill="black" stroke="black" points="92.87,-113.89 84.21,-107.79 87.18,-117.96 92.87,-113.89"></polygon> </g> <!-- c --> <g id="node3" class="node">
<title>
c
</title>
<ellipse fill="none" stroke="black" cx="122.22" cy="-18" rx="67.29" ry="18"></ellipse> <text text-anchor="middle" x="122.22" y="-13.8" font-family="Arial" font-size="14.00">lifetime value</text> </g> <!-- a&#45;&gt;c --> <g id="edge2" class="edge">
<title>
a-&gt;c
</title>
<path fill="none" stroke="black" d="M135.37,-143.91C142.14,-134.01 149.71,-120.96 153.22,-108 157.41,-92.56 157.41,-87.44 153.22,-72 150.64,-62.48 145.88,-52.92 140.84,-44.59"></path> <polygon fill="black" stroke="black" points="143.73,-42.6 135.37,-36.09 137.84,-46.39 143.73,-42.6"></polygon> </g> <!-- b&#45;&gt;c --> <g id="edge3" class="edge">
<title>
b-&gt;c
</title>
<path fill="none" stroke="black" d="M84.33,-72.05C90.34,-63.63 97.74,-53.28 104.39,-43.97"></path> <polygon fill="black" stroke="black" points="107.27,-45.96 110.23,-35.79 101.57,-41.89 107.27,-45.96"></polygon> </g> </g>
</svg>
</p>
</div>
</div>
</div>
<p>This is an example of <strong>selection bias</strong> (more specifically, its <strong>collider bias</strong>, a common confound). When comparing customers who had the treatment vs. didnt have the treatment, we accidentally also end up comparing unhappy customers vs.&nbsp;happier customers, and obviously unhappier customers tend to have worse lifetime value. <strong>We need to find a way to compare the impact of the treatment while controlling for the happiness of customers so that we are making a more fair comparison.</strong> For example, if we had 2 equally unhappy customers and 1 received the treatment while the other didnt, we’d get a more reasonable comparison for evaluating the treatment effect.</p>
</section>
<section id="identification-under-selection-bias" class="level2">
<h2 class="anchored" data-anchor-id="identification-under-selection-bias">Identification Under Selection bias</h2>
<p>How can we represent the scenario above with math? This is where the Potential Outcomes model starts coming into play. <em>Note I’m borrowing this directly from Scott Cunningham. For the full proof, see his book, <a href="https://mixtape.scunning.com/">Causal Inference the Mixtape</a></em>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctext%7BSimple%20Difference%20in%20Outcomes%7D%0A&amp;=%20%5Cunderbrace%7BE%5BY(1)%5D%20-%20E%5BY(0)%5D%7D_%7B%20%5Ctext%7BAverage%20Treatment%20Effect%7D%7D%5C%5C%0A&amp;+%20%5Cunderbrace%7BE%5Cbig%5BY(0)%5Cmid%20T=1%5Cbig%5D%20-%20E%5Cbig%5BY(0)%5Cmid%20T=0%5Cbig%5D%7D_%7B%20%5Ctext%7BSelection%20bias%7D%7D%5C%5C%0A&amp;%20+%20%5Cunderbrace%7B(1-%5Cpi)(ATT%20-%20ATU)%7D_%7B%20%5Ctext%7BHeterogeneous%20treatment%20effect%20bias%7D%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>This equation for the Potential Outcomes model basically says that anytime you make a comparison on observational data, it ends up being the sum of the true average treatment effect, selection bias, and Heterogeneous Treatment effect (HTE) bias. HTEs are just a fancy way of saying the personalized effect, aka promos might be more impactful for some users than others.</p>
<p>So how does this relate to what we did before? Well when we tried to compare users who saw the treatment vs.&nbsp;those that didnt</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7BSDO%7D%20=%20E%5B%20Y(1)%20%7C%20T%20=%201%20%5D%20-%20E%5B%20Y(0)%20%7C%20T%20=%200%20%5D%0A"></p>
<p>we didnt take into account the fact that users who saw the treatment tend to be different than those who didn’t. Users who saw the treatment tend to be more unhappy by design.</p>
<p>So if we subtract out the <strong>selection bias</strong> from the SDO (I got this via simple algebra), aka we control for the unhappiness between customers, we can get closer to identifying the true ATE.</p>
<p>Note that selection bias was <img src="https://latex.codecogs.com/png.latex?%0AE%5Cbig%5BY(0)%5Cmid%20T=1%5Cbig%5D%20-%20E%5Cbig%5BY(0)%5Cmid%20T=0%5Cbig%5D%0A"></p>
<p>This is just saying selection bias is the fundamental difference between users who get picked for treatment vs.&nbsp;those who dont.</p>
<p>In our case, the fundamental difference between whether users are selected for treatment is based upon their unhappiness. So if we can subtract out the effect of unhappiness, we can subtract out the selection bias</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1">df.groupby(<span class="st" style="color: #20794D;">"T"</span>).mean()[[<span class="st" style="color: #20794D;">"upset"</span>]].T</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>T</th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>upset</th>
      <td>-0.159046</td>
      <td>1.018004</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>We can do this with OLS. The most obvious way is to fit a model relating unhappiness to LTV, and then subtract out that effect.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1">model1 <span class="op" style="color: #5E5E5E;">=</span> sm.OLS.from_formula(<span class="st" style="color: #20794D;">"Y ~ upset"</span>, data<span class="op" style="color: #5E5E5E;">=</span>df.loc[<span class="kw" style="color: #003B4F;">lambda</span> d: d[<span class="st" style="color: #20794D;">"T"</span>]<span class="op" style="color: #5E5E5E;">==</span><span class="dv" style="color: #AD0000;">0</span>]).fit()</span>
<span id="cb11-2">Y0_hat <span class="op" style="color: #5E5E5E;">=</span> model1.predict(df)</span>
<span id="cb11-3"></span>
<span id="cb11-4">selection_bias <span class="op" style="color: #5E5E5E;">=</span> (</span>
<span id="cb11-5">    df.assign(selection_bias <span class="op" style="color: #5E5E5E;">=</span> Y0_hat)</span>
<span id="cb11-6">    .groupby(<span class="st" style="color: #20794D;">"T"</span>).mean()</span>
<span id="cb11-7">    [[<span class="st" style="color: #20794D;">"selection_bias"</span>]]</span>
<span id="cb11-8">)</span>
<span id="cb11-9">selection_bias.T.<span class="bu" style="color: null;">round</span>(<span class="dv" style="color: #AD0000;">2</span>)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th>T</th>
      <th>0</th>
      <th>1</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>selection_bias</th>
      <td>2579.46</td>
      <td>1990.96</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>And finally we can subtract out the effect, ending up with an estimate very close to the true ATE of 500</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb12" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1">(</span>
<span id="cb12-2">    df.assign(selection_bias <span class="op" style="color: #5E5E5E;">=</span> Y0_hat)</span>
<span id="cb12-3">    .groupby(<span class="st" style="color: #20794D;">"T"</span>).mean()[[<span class="st" style="color: #20794D;">"Y"</span>, <span class="st" style="color: #20794D;">"selection_bias"</span>]].T</span>
<span id="cb12-4">    .assign(difference <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> d: d[<span class="dv" style="color: #AD0000;">1</span>] <span class="op" style="color: #5E5E5E;">-</span> d[<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb12-5">    [[<span class="st" style="color: #20794D;">"difference"</span>]].T</span>
<span id="cb12-6">    .reset_index(drop<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>)</span>
<span id="cb12-7">    .rename(columns<span class="op" style="color: #5E5E5E;">=</span>{<span class="st" style="color: #20794D;">"Y"</span>:<span class="st" style="color: #20794D;">"SDO"</span>})</span>
<span id="cb12-8">    .assign(tau <span class="op" style="color: #5E5E5E;">=</span> <span class="kw" style="color: #003B4F;">lambda</span> d: d.SDO <span class="op" style="color: #5E5E5E;">-</span> d.selection_bias)</span>
<span id="cb12-9">    .<span class="bu" style="color: null;">round</span>(<span class="dv" style="color: #AD0000;">2</span>)</span>
<span id="cb12-10">)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>SDO</th>
      <th>selection_bias</th>
      <th>tau</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-87.98</td>
      <td>-588.5</td>
      <td>500.52</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>There’s actually an even more simple way to control for selection bias - it can just be included as a term in an OLS regression model.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb13" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><span class="kw" style="color: #003B4F;">def</span> statsmodels_to_df(model):</span>
<span id="cb13-2">    table <span class="op" style="color: #5E5E5E;">=</span> np.array(model.summary().tables[<span class="dv" style="color: #AD0000;">1</span>].data)</span>
<span id="cb13-3">    <span class="cf" style="color: #003B4F;">return</span> pd.DataFrame(table[<span class="dv" style="color: #AD0000;">1</span>:, <span class="dv" style="color: #AD0000;">1</span>:], columns<span class="op" style="color: #5E5E5E;">=</span>table[<span class="dv" style="color: #AD0000;">0</span>,<span class="dv" style="color: #AD0000;">1</span>:], index<span class="op" style="color: #5E5E5E;">=</span>table[<span class="dv" style="color: #AD0000;">1</span>:,<span class="dv" style="color: #AD0000;">0</span>])</span>
<span id="cb13-4"></span>
<span id="cb13-5">model2 <span class="op" style="color: #5E5E5E;">=</span> sm.OLS.from_formula(<span class="st" style="color: #20794D;">" Y ~ T + upset"</span>, data<span class="op" style="color: #5E5E5E;">=</span>df).fit()</span>
<span id="cb13-6">statsmodels_to_df(model2)</span></code></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>coef</th>
      <th>std err</th>
      <th>t</th>
      <th>P&gt;|t|</th>
      <th>[0.025</th>
      <th>0.975]</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Intercept</th>
      <td>2499.9363</td>
      <td>0.516</td>
      <td>4847.317</td>
      <td>0.000</td>
      <td>2498.925</td>
      <td>2500.947</td>
    </tr>
    <tr>
      <th>T</th>
      <td>500.5529</td>
      <td>1.502</td>
      <td>333.191</td>
      <td>0.000</td>
      <td>497.608</td>
      <td>503.497</td>
    </tr>
    <tr>
      <th>upset</th>
      <td>-500.0068</td>
      <td>0.518</td>
      <td>-965.940</td>
      <td>0.000</td>
      <td>-501.021</td>
      <td>-498.992</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>As we can see above the estimate of the treatment effect is the beta coefficient for <code>T</code> and it closely matches our manual estimate above.</p>
</section>
<section id="a-quick-note-on-heterogeneous-treatment-effects" class="level2">
<h2 class="anchored" data-anchor-id="a-quick-note-on-heterogeneous-treatment-effects">A quick note on Heterogeneous Treatment Effects</h2>
<p>We’ve controlled for selection bias, what about Heterogeneous Treatment Effect bias? We actually don’t need to control for these once we’ve controlled for selection bias. These average treatment effect ends up being the average of all of the HTEs of individuals, which is fine because as long as we’ve accounted for selection bias, the HTEs tend to cancel out. They’re essentially captured by the error term, <img src="https://latex.codecogs.com/png.latex?%5Cepsilon"> in OLS <img src="https://latex.codecogs.com/png.latex?%0Ay%20=%20%5Calpha%20+%20%5Cbeta%20X%20+%20%5Cepsilon%0A"></p>
<p>We can also see that in our code, where the distribution of true HTE bias from our hypothetical dataset is centered at zero. Any time we’ve accounted for all selection bias, the HTE should be zero centered and cancel itself out as N increases.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1">ATE <span class="op" style="color: #5E5E5E;">=</span> data[<span class="st" style="color: #20794D;">"ITE"</span>].mean()</span>
<span id="cb14-2">HTE <span class="op" style="color: #5E5E5E;">=</span> data.ITE.values <span class="op" style="color: #5E5E5E;">-</span> ATE</span>
<span id="cb14-3">sns.histplot(HTE)</span>
<span id="cb14-4">plt.xlabel(<span class="st" style="color: #20794D;">"HTE"</span>)</span>
<span id="cb14-5">plt.title(<span class="st" style="color: #20794D;">"Distribution of HTEs (each customers difference from the ATE)"</span>)</span>
<span id="cb14-6">plt.show()</span></code></pre></div>
<div class="cell-output cell-output-display">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/posts/experiment_design_files/figure-html/cell-12-output-1.png" width="601" height="442"></p>
</div>
</div>
<p>The bias of HTEs for each person is just the distance their treatment effect is from the average treatment effect. Again, this follows the same property as the error term in OLS regression, which is why it can be such a powerful tool for causal inference <em>when used correctly</em>.</p>
</section>
</section>
<section id="why-are-ab-tests-needed" class="level1">
<h1>Why are A/B tests needed?</h1>
<p>We saw that when taking a simple difference in outcomes that we can end up with biased inference. Controlling for selection bias can help fix this, but we may not always be able to do so.</p>
<p>For instance, consider if we didn’t have data on customer unhappiness (which is more likely true than not in the real world) - how would we control for it?</p>
<p>In many cases we can’t, or even if we can (such as with Instrumental Variable Analysis), it’s very difficult. This is where randomization and A/B testing come into play.</p>
<p>Remember that the whole reason we had issues with measuring the ATE was because users treated ended up being fundamentally different from those that weren’t. But what if we made it so that its purely random who receives the treatment and who doesn’t? Then we’d expect the same level of unhappiness in each group, cancelling out any selection bias. HTEs would cancel out as well like before, and <strong>by randomizing, we find that the simple difference in outcomes equals the true average treatment effect</strong>.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Balign%7D%0A%5Ctext%7BSimple%20Difference%20in%20Outcomes%7D%0A&amp;=%20%5Cunderbrace%7BE%5BY(1)%5D%20-%20E%5BY(0)%5D%7D_%7B%20%5Ctext%7BAverage%20Treatment%20Effect%7D%7D%5C%5C%0A&amp;+%20%5Cunderbrace%7B%20%5Ccancel%7BE%5Cbig%5BY(0)%5Cmid%20T=1%5Cbig%5D%20-%20E%5Cbig%5BY(0)%5Cmid%20T=0%5Cbig%5D%7D%7D_%7B%20%5Ctext%7BSelection%20bias%7D%7D%5C%5C%0A&amp;%20+%20%5Cunderbrace%7B%5Ccancel%7B(1-%5Cpi)(ATT%20-%20ATU)%7D%7D_%7B%20%5Ctext%7BHeterogeneous%20treatment%20effect%20bias%7D%7D%0A%5Cend%7Balign%7D%0A"></p>
<p>This is why randomization is so powerful, and why many people say A/B tests are the gold standard.</p>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>In this post we walked through the Potential Outcomes Model, showed how it applies to a fake data scenario, and then used it to tie back to why randomization works for A/B testing.</p>
</section>
<section id="additional-reading" class="level1">
<h1>Additional Reading</h1>
<p>This is just one example of many that exist out there. Here are some other examples I’ve come across:</p>
<ul>
<li>Scott Cunningham’s <a href="https://mixtape.scunning.com/04-potential_outcomes">“The Perfect Doctor” example</a> from <a href="https://mixtape.scunning.com/">Causal Inference: the Mixtape</a></li>
<li>Matheus Facure’s <a href="https://matheusfacure.github.io/python-causality-handbook/landing-page.html">Causal Inference for the Brave and True</a></li>
</ul>


</section>

 ]]></description>
  <category>experimentation</category>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/experiment_design.html</guid>
  <pubDate>Sat, 17 Sep 2022 04:00:00 GMT</pubDate>
</item>
<item>
  <title>Making out of sample predictions with PyMC</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-17-out-of-sample-pymc.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>A cool thing about hierarchical models is that its easy to predict out of sample - i.e.&nbsp;if you want to make a prediction on a new zipcode, just sample from the state’s distribution (composed of the state average and variance across zip codes in that state).</p>
<p>In pymc3, it’s somewhat easy to accomplish this, but not as straightforward as we’d hope. This blog post will show a trick that lets you easily predict out of sample, and will reduce some of the overhead that comes from writing alot of custom prediction functions</p>
</section>
<section id="simulating-data" class="level1">
<h1>Simulating data</h1>
<p>I simulated a 2 level hierarchical model - for interpretability, I set it up as a state &gt; zipcode model. You can following along with the notebook <a href="https://github.com/kylejcaron/case_studies/blob/main/PyMC%20out%20of%20sample%20predictions.ipynb">here</a>. The data is as follows</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="using-categorical-variables" class="level1">
<h1>Using categorical variables</h1>
<p>Categorical variables are a somewhat new feature of pandas - they can store categories that aren’t in the observed data, and are an easy replacement for <code>pd.factorize()</code> (a common tool for those familiar with the bayesian workflow).</p>
<p>We can use these to trick pymc into thinking there’s a category with no observed data, and pymc ends up assigning the global distribution to that unobserved category, which we can simply reference in the future for any time we want to make a prediction on out of sample data.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Convert to categorical and add an `out_of_sample` category</span></span>
<span id="cb1-2">df <span class="op" style="color: #5E5E5E;">=</span> df.assign(state <span class="op" style="color: #5E5E5E;">=</span> pd.Categorical(df.state).add_categories(<span class="st" style="color: #20794D;">"out_of_sample"</span>))<span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb1-3">    .assign(zipcode <span class="op" style="color: #5E5E5E;">=</span> pd.Categorical(df.zipcode).add_categories(<span class="st" style="color: #20794D;">"out_of_sample"</span>))</span></code></pre></div>
</section>
<section id="fitting-the-model" class="level1">
<h1>Fitting the model</h1>
<p>We’ll use the codes from the categorical columns to index our model coefficients, and we’ll use the categories as coordinates for the model to map names to.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">coords<span class="op" style="color: #5E5E5E;">=</span>{</span>
<span id="cb2-2">    <span class="st" style="color: #20794D;">"state"</span>:df.state.cat.categories,</span>
<span id="cb2-3">    <span class="st" style="color: #20794D;">"zipcode"</span>:df.zipcode.cat.categories</span>
<span id="cb2-4">}</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;">def</span> hierarchical_normal(name, μ, dims):</span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;">'''Adapted from Austin Rochford'''</span></span>
<span id="cb2-8">    Δ <span class="op" style="color: #5E5E5E;">=</span> pm.Normal(<span class="st" style="color: #20794D;">'Δ_</span><span class="sc" style="color: #5E5E5E;">{}</span><span class="st" style="color: #20794D;">'</span>.<span class="bu" style="color: null;">format</span>(name), <span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, dims<span class="op" style="color: #5E5E5E;">=</span>dims)</span>
<span id="cb2-9">    σ <span class="op" style="color: #5E5E5E;">=</span> pm.Exponential(<span class="st" style="color: #20794D;">'σ_</span><span class="sc" style="color: #5E5E5E;">{}</span><span class="st" style="color: #20794D;">'</span>.<span class="bu" style="color: null;">format</span>(name), <span class="fl" style="color: #AD0000;">2.5</span>)</span>
<span id="cb2-10">    <span class="cf" style="color: #003B4F;">return</span> pm.Deterministic(name, μ <span class="op" style="color: #5E5E5E;">+</span> Δ <span class="op" style="color: #5E5E5E;">*</span> σ, dims<span class="op" style="color: #5E5E5E;">=</span>dims)</span>
<span id="cb2-11"></span>
<span id="cb2-12"></span>
<span id="cb2-13"><span class="cf" style="color: #003B4F;">with</span> pm.Model(coords<span class="op" style="color: #5E5E5E;">=</span>coords) <span class="im" style="color: #00769E;">as</span> model_nc:</span>
<span id="cb2-14">    </span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;"># Observed Data tracking</span></span>
<span id="cb2-16">    state_ <span class="op" style="color: #5E5E5E;">=</span> pm.Data(<span class="st" style="color: #20794D;">"state_"</span>, df.state.cat.codes)</span>
<span id="cb2-17">    zip_ <span class="op" style="color: #5E5E5E;">=</span> pm.Data(<span class="st" style="color: #20794D;">"zip_"</span>, df.zipcode.cat.codes)</span>
<span id="cb2-18">    obs <span class="op" style="color: #5E5E5E;">=</span> pm.Data(<span class="st" style="color: #20794D;">"obs"</span>, df.y)</span>
<span id="cb2-19"></span>
<span id="cb2-20">    <span class="co" style="color: #5E5E5E;"># Hyperprior</span></span>
<span id="cb2-21">    mu_country <span class="op" style="color: #5E5E5E;">=</span> pm.Normal(<span class="st" style="color: #20794D;">"mu_country"</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-22">    </span>
<span id="cb2-23">    <span class="co" style="color: #5E5E5E;"># Prior</span></span>
<span id="cb2-24">    sig <span class="op" style="color: #5E5E5E;">=</span> pm.Exponential(<span class="st" style="color: #20794D;">"sig"</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-25">    </span>
<span id="cb2-26">    <span class="co" style="color: #5E5E5E;"># Hierarchical coefficients</span></span>
<span id="cb2-27">    mu_state <span class="op" style="color: #5E5E5E;">=</span> hierarchical_normal(<span class="st" style="color: #20794D;">"mu_state"</span>, μ<span class="op" style="color: #5E5E5E;">=</span>mu_country, dims<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"state"</span>)</span>
<span id="cb2-28">    mu_zipcode <span class="op" style="color: #5E5E5E;">=</span> hierarchical_normal(<span class="st" style="color: #20794D;">"mu_zipcode"</span>, μ<span class="op" style="color: #5E5E5E;">=</span>mu_state, dims<span class="op" style="color: #5E5E5E;">=</span>(<span class="st" style="color: #20794D;">"zipcode"</span>, <span class="st" style="color: #20794D;">"state"</span>) )</span>
<span id="cb2-29">    </span>
<span id="cb2-30">    <span class="co" style="color: #5E5E5E;"># Observational model</span></span>
<span id="cb2-31">    y <span class="op" style="color: #5E5E5E;">=</span> pm.Normal(<span class="st" style="color: #20794D;">"y"</span>, mu_zipcode[zip_, state_], sig, observed<span class="op" style="color: #5E5E5E;">=</span>obs)</span>
<span id="cb2-32">    </span>
<span id="cb2-33">    <span class="co" style="color: #5E5E5E;"># Fit </span></span>
<span id="cb2-34">    trace_nc <span class="op" style="color: #5E5E5E;">=</span> pm.sample(target_accept<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>, return_inferencedata<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, random_seed<span class="op" style="color: #5E5E5E;">=</span>SEED)</span></code></pre></div>
<p>There are a few key point that make out of sample prediction possible * Having the <code>out_of_sample</code> category for each indexed variable with no observed data * Passing the <code>coords</code> in the model statement * Using dims to reference which model coefficients have which coordinate labels * Having all of our input data wrapped in a <code>pm.Data()</code> statement</p>
<p>That last point is particularly important. For PyMC, if you want to make predictions on new data, you have to replace the data that the model references and the only way to do that (that I know of atleast) is to using a Theano shared variable. <code>pm.Data()</code> handles all of that fo you.</p>
<p>So we fit our model, lets take a quick look at the state level coefficients</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">pm.plot_forest(trace_nc, var_names<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"mu_state"</span>])</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Great, that out of sample variable seems to represent the <code>global</code> distribution across states - i.e.&nbsp;if we were to make a prediction for a new state we’d potentially use that distribtion (we’ll confirm further down).</p>
<p>We’ll check the zip code level below as well, looking at Maine specifically</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As we can see, the <code>out_of_sample</code> variable has a sampled value despite there being no observed data for it. Now the question is, does this align with how we’d predict new data?</p>
<p>Let’s try calculating coefficients out of sample by hand and see if it aligns with the out_of_sample values</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">post <span class="op" style="color: #5E5E5E;">=</span> trace_nc.posterior</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;"># Pull the true data from our simulation</span></span>
<span id="cb4-4">state_true <span class="op" style="color: #5E5E5E;">=</span> mu_state_true.random(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4000</span>)</span>
<span id="cb4-5"></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;"># Calculate out of sample state means by drawing from global distribution</span></span>
<span id="cb4-8">mu_country <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"mu_country"</span>].values.reshape(<span class="dv" style="color: #AD0000;">4000</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-9">σ_state <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"σ_mu_state"</span>].values.reshape(<span class="dv" style="color: #AD0000;">4000</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-10">mu_state <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(mu_country, σ_state)</span>
<span id="cb4-11"></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;"># Using the indexing trick</span></span>
<span id="cb4-13">state_idx_trick <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"mu_state"</span>].sel({<span class="st" style="color: #20794D;">"state"</span>:[<span class="st" style="color: #20794D;">"out_of_sample"</span>]}).values.ravel()</span>
<span id="cb4-14"></span>
<span id="cb4-15"><span class="co" style="color: #5E5E5E;"># Pull the true data from simulation</span></span>
<span id="cb4-16">zip_true <span class="op" style="color: #5E5E5E;">=</span> pm.Normal.dist(mu_state_true.random(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4000</span>), sig_zip_true).random(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4000</span>)</span>
<span id="cb4-17"></span>
<span id="cb4-18"><span class="co" style="color: #5E5E5E;"># calculate out of sample mu by hand by drawing from out of sample state prediction above</span></span>
<span id="cb4-19">σ_zipcode <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"σ_mu_zipcode"</span>].values.reshape(<span class="dv" style="color: #AD0000;">4000</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-20">mu_zipcode <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(mu_state, σ_zipcode)</span>
<span id="cb4-21"></span>
<span id="cb4-22"><span class="co" style="color: #5E5E5E;"># Use the indexing trick</span></span>
<span id="cb4-23">zip_idx_trick <span class="op" style="color: #5E5E5E;">=</span> (post[<span class="st" style="color: #20794D;">"mu_zipcode"</span>]</span>
<span id="cb4-24">                .sel({<span class="st" style="color: #20794D;">"state"</span>:[<span class="st" style="color: #20794D;">"out_of_sample"</span>], <span class="st" style="color: #20794D;">"zipcode"</span>:[<span class="st" style="color: #20794D;">"out_of_sample"</span>]})</span>
<span id="cb4-25">                .values.ravel())</span></code></pre></div>
<p>We can compare these results by plotting their distributions below</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Notice that the manual prediction and the indexing trick are basically identical. There’s a slight difference from the ground truth, but thats to be expected since we’re fitting a model on limited data (and anyway, it’s still quite close).</p>
</section>
<section id="predicting-out-of-sample" class="level1">
<h1>Predicting out of sample</h1>
<p>Let’s go ahead and actually make prediction now - we’ll make predictions for the following data below</p>
<ul>
<li>The first example is in sample</li>
<li>The second example is in sample for state, out of sample for zipcode</li>
<li>The third example is out of sample entirely</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>And finally we’ll use the model to make predictions on this new data. Notice the <code>pm.set_data()</code> function - remember our <code>pm.Data()</code> calls from before? This tells PyMC to override that with new data, so when we sample from the posterior predictive it makes predictions on the new data instead of the data used to fit the model.</p>
<details>
<summary>
Click here for helper function code
</summary>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># We're making some quick convenience functions to map this new data </span></span>
<span id="cb5-2"><span class="co" style="color: #5E5E5E;"># to the proper indexes from the fitted model</span></span>
<span id="cb5-3">zip_lookup <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(<span class="bu" style="color: null;">zip</span>(df.zipcode.cat.categories, <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(df.zipcode.cat.categories))))</span>
<span id="cb5-4">state_lookup <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(<span class="bu" style="color: null;">zip</span>(df.state.cat.categories, <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(df.state.cat.categories))))</span>
<span id="cb5-5"></span>
<span id="cb5-6"><span class="kw" style="color: #003B4F;">def</span> labels_to_index(series, lookup):</span>
<span id="cb5-7">    <span class="co" style="color: #5E5E5E;">'''Converts categories to their proper codes'''</span></span>
<span id="cb5-8">    series <span class="op" style="color: #5E5E5E;">=</span> series.copy()</span>
<span id="cb5-9">    in_sample <span class="op" style="color: #5E5E5E;">=</span> series.isin(lookup.keys())</span>
<span id="cb5-10">    series.loc[<span class="op" style="color: #5E5E5E;">~</span>in_sample] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"out_of_sample"</span></span>
<span id="cb5-11">    <span class="cf" style="color: #003B4F;">return</span> series.<span class="bu" style="color: null;">map</span>(lookup).values.astype(<span class="st" style="color: #20794D;">"int8"</span>)</span></code></pre></div>
</details>
<p><br></p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">with</span> model_nc:</span>
<span id="cb6-2">    <span class="co" style="color: #5E5E5E;"># Set new data for the model to make predictions on</span></span>
<span id="cb6-3">    pm.set_data({</span>
<span id="cb6-4">        <span class="st" style="color: #20794D;">"state_"</span>: X.state.pipe(labels_to_index, state_lookup),</span>
<span id="cb6-5">        <span class="st" style="color: #20794D;">"zip_"</span>: X.zipcode.pipe(labels_to_index, zip_lookup)</span>
<span id="cb6-6">    })</span>
<span id="cb6-7">    </span>
<span id="cb6-8">    <span class="co" style="color: #5E5E5E;"># make predictions</span></span>
<span id="cb6-9">    preds <span class="op" style="color: #5E5E5E;">=</span> pm.sample_posterior_predictive(trace_nc)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>This is exactly what we were looking for - and prediction is easy, just map any out of sample states or zipcodes to the <code>out_of_sample</code> category. Notice how in sample predictions have smaller uncertainty intervals and out of sample data is more uncertain - this is exactly what we’d expect. This trick makes it much easier to make predictions compared to having to write out a custom prediction function that follows the same logic as the model.</p>
<p>If you have any other easy tricks for out of sample prediction let me know!</p>


</section>

 ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-17-out-of-sample-pymc.html</guid>
  <pubDate>Sun, 17 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig1.png" medium="image" type="image/png" height="118" width="144"/>
</item>
<item>
  <title>How long should you run an A/B test for?</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-05-ab-test-duration.html</link>
  <description><![CDATA[ 




<p>For some people in industry new to A/B testing, they might wonder “Why cant we just run an A/B test for 2 days and be done with it?”. Even those familiar with it might wonder why their team’s Data Scientist is insisting on so much. And even Data Scientists may be looking for easier ways to explain the need to them. The goal of this article is to cover just that, from a naive and explainable point of view.</p>
<p>So, <strong>How long should you run an A/B test for?</strong> Well let’s say you step into a casino with $5000 and you walk away with $6000. You just made a 20% return. Is it fair to say that a night out in the casino leads to a 20% return? Is it fair to say that our A/B test we ran for 2 days leads to a 20% lift in conversion? How do we know for sure?</p>
<p><strong>We should run an A/B test for as long as it takes to rule out random chance.</strong></p>
<p>While vague, and technically not the full picture, your friendly neightborhood data scientist should be able to answer this for you. The code for this blogpost can be found <a href="https://github.com/kylejcaron/case_studies/blob/master/How%20long%20should%20you%20run%20an%20AB%20Test%20for%3F.ipynb">here</a>.</p>
<p><br></p>
<section id="simulating-a-fake-scenario" class="level3">
<h3 class="anchored" data-anchor-id="simulating-a-fake-scenario">Simulating a fake scenario</h3>
<p>Let’s play out the casino example from above. I’m going to simulate out an entirely fake, but entirely possible scenario.</p>
<p>You go to the casino one night with $5000 and decide roulette is your game of choice. You get a little into it and play 500 rounds (in one night?? for examples sake, yes). Little do you know the real probability of winning is 48.5%</p>
<p>The plot below shows the total money you had at the start of each round of roulette</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>This is great - after 500 rounds of this you’ve made 122% return on your initial investment of $5000 and you’re winning 51% of the time.</p>
<p>Playing roulette must lead to a 20% return right? Commited to your strategy you decide to come back over the next few weeks and play another 3000 rounds, shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Alright you’ve played 3500 rounds now and you have $5400 total. You’ve definitely had some runs of bad luck, but you’re still seeing a win percentage above 50% (50.1% in fact) and right now you’re heating up. You stay determined and play until you reach 15000 rounds.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="what-happened" class="level1">
<h1>What happened?</h1>
<p>We started off on a hot streak winning 51% of our rounds, but as we played more and more rounds, it became more obvious we were losing money. This is a demonstration of the law of large numbers - as we play more and more rounds, the truth comes out</p>
<p>We can visualize this process via the beta distribution below. These plots visualize all of the possible values that the true win percentage could be (the x axis), and their relative plausibilities (the y axis). The first plot can be read as follows:</p>
<blockquote class="blockquote">
<p>The win percentage is likely to be somewhere between 42.5% and 60%, with the most likely value being around 51%</p>
</blockquote>
<p>As we move from left to right, our estimated distribution converges closer and closer to the true probability of winning a round</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>We can also visualize this as a time series, which really makes it clear how the uncertainty becomes smaller over time and the estimated value converges to the true value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><br></p>
<section id="how-does-this-tie-back-to-ab-testing" class="level3">
<h3 class="anchored" data-anchor-id="how-does-this-tie-back-to-ab-testing">How does this tie back to A/B testing?</h3>
<p>If we don’t choose our sample size for an experiment properly, we can end up making the wrong decisions!<sup>1</sup> The larger the sample size we choose, the more likely we’ll make the right choice.</p>
<p>We can use <strong>power analyses</strong> (sometimes referred to as simulation studies) to estimate what sample size is needed for an experiment given the desired outcome.</p>
<hr>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Unless you’re using bayesian inference, which can really mitigate this risk.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-05-ab-test-duration.html</guid>
  <pubDate>Fri, 15 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig1.png" medium="image" type="image/png" height="80" width="144"/>
</item>
<item>
  <title>Uncertainty Intervals or p-values?</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-11-uncertainty_intervals_over_pvals.html</link>
  <description><![CDATA[ 




<p>Uncertainty Intervals are better than p-values. Sure, its better to use both, but p-values are just a point estimate and they bring no concept of uncertainty in our estimate - this can lead to situations where we expose ourselves to high downside risk.</p>
<p>Take the following example for instance. Let’s say we’re running a “Do no harm” A/B test where we want to roll out an experiment as long as it doesnt harm conversion rate.</p>
<p>If you want to follow along with the code, <a href="https://github.com/kylejcaron/case_studies/blob/main/Uncertainty%20intervals%20over%20p%20values.ipynb">see here</a>.</p>
<section id="the-experiment-design" class="level2">
<h2 class="anchored" data-anchor-id="the-experiment-design">The experiment design</h2>
<p>Given the stakeholders want to rule out a drop in conversion, and ruling out small differences requires large sample sizes, we decide to design an experiment with good power to detect the presence of a 1/2% absolute drop (if one were to truly exist)</p>
<p>We ran a power analysis and found that in order to have a 90% probability of detecting (power=0.9) a 1/2% absolute drop in conversion rate with 80 percent confidence ( 𝛼=0.2 ), we need N=32500 per group</p>
<blockquote class="blockquote">
<p>Statisticians might not love this interpretation of a power analysis, but its a useful and interpretable translation and tends to coincide with what we’re aiming for anyway. In reality, frequentist power analyses assume that the null hypothesis is correct, which isn’t quite what we want, not to mention, frequentist power analyses use backwards probabilities which are just plain confusing - <a href="https://www.fharrell.com/post/pvalprobs/">see here to for more</a></p>
</blockquote>
<p>Note that we’re prioritizing power here for a reason. If 𝛼 is false positive rate, and power is probability of detection, then don’t we want to prioritize our probability of detecting a drop if one truly exists? A false negative here would be more expensive then a false positive</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">pA <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.1</span> <span class="co" style="color: #5E5E5E;"># historical conversion rate</span></span>
<span id="cb1-2">abs_delta <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.005</span> <span class="co" style="color: #5E5E5E;"># minimum detectable effect to test for</span></span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Statsmodels requires an effect size </span></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;"># (aka an effect normalized by its standard deviation)</span></span>
<span id="cb1-6">stdev <span class="op" style="color: #5E5E5E;">=</span> np.sqrt( pA<span class="op" style="color: #5E5E5E;">*</span>(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>pA) ) <span class="co" style="color: #5E5E5E;"># bernoulli stdev, sigma = sqrt(p(1-p))</span></span>
<span id="cb1-7">ES <span class="op" style="color: #5E5E5E;">=</span> abs_delta <span class="op" style="color: #5E5E5E;">/</span> stdev </span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;"># estimate required sample size</span></span>
<span id="cb1-10">sm.stats.tt_ind_solve_power(</span>
<span id="cb1-11">    <span class="op" style="color: #5E5E5E;">-</span>ES, </span>
<span id="cb1-12">    alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>,</span>
<span id="cb1-13">    power<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>,</span>
<span id="cb1-14">    alternative<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"smaller"</span></span>
<span id="cb1-15">)</span></code></pre></div>
<p>Running the code above leads us to conclude are sample size should be roughly 32,500 users per group.</p>
<section id="the-experiment" class="level3">
<h3 class="anchored" data-anchor-id="the-experiment">The experiment</h3>
<p>I’m going to simulate fake data for this experiment where * The <strong>control</strong> has a <strong><em>true</em></strong> conversion rate of 10% * the <strong>variant</strong> has a <strong><em>true</em></strong> conversion rate of 9.25%</p>
<p>For examples sake we’ll pretend we don’t know that the variant is worse</p>
<details>
<summary>
Click here for code
</summary>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Settings</span></span>
<span id="cb2-2">np.random.seed(<span class="dv" style="color: #AD0000;">1325</span>)</span>
<span id="cb2-3">N <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">32500</span></span>
<span id="cb2-4">pA <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.1</span></span>
<span id="cb2-5">pB <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.0925</span></span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;"># Simulation</span></span>
<span id="cb2-8"><span class="kw" style="color: #003B4F;">def</span> simulate_experiment(pA, pB, N_per_group):</span>
<span id="cb2-9">    </span>
<span id="cb2-10">    df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame({</span>
<span id="cb2-11">        <span class="st" style="color: #20794D;">"group"</span>:[<span class="st" style="color: #20794D;">"A"</span>]<span class="op" style="color: #5E5E5E;">*</span>N <span class="op" style="color: #5E5E5E;">+</span> [<span class="st" style="color: #20794D;">"B"</span>]<span class="op" style="color: #5E5E5E;">*</span>N,</span>
<span id="cb2-12">        <span class="st" style="color: #20794D;">"convert"</span>:np.r_[</span>
<span id="cb2-13">             np.random.binomial(<span class="dv" style="color: #AD0000;">1</span>, p<span class="op" style="color: #5E5E5E;">=</span>pA, size<span class="op" style="color: #5E5E5E;">=</span>N),</span>
<span id="cb2-14">             np.random.binomial(<span class="dv" style="color: #AD0000;">1</span>, p<span class="op" style="color: #5E5E5E;">=</span>pB, size<span class="op" style="color: #5E5E5E;">=</span>N)</span>
<span id="cb2-15">        ]</span>
<span id="cb2-16">    })</span>
<span id="cb2-17">    </span>
<span id="cb2-18">    <span class="cf" style="color: #003B4F;">return</span> df</span>
<span id="cb2-19"></span>
<span id="cb2-20">df <span class="op" style="color: #5E5E5E;">=</span> simulate_experiment(pA, pB, N)</span></code></pre></div>
</details>
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Looking at the data above, we’re seeing a better conversion rate in group B. We run a two-proportions z-test and we find that there’s a non-significant p-value, meaning we found insufficient evidence of the variant having lower conversion than the control.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">def</span> pval_from_summary(tab):</span>
<span id="cb3-2">    </span>
<span id="cb3-3">    _, pval <span class="op" style="color: #5E5E5E;">=</span> sm.stats.proportions_ztest(</span>
<span id="cb3-4">        count<span class="op" style="color: #5E5E5E;">=</span>tab[<span class="st" style="color: #20794D;">"converts"</span>][::<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>], </span>
<span id="cb3-5">        nobs<span class="op" style="color: #5E5E5E;">=</span>tab[<span class="st" style="color: #20794D;">"N"</span>][::<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb3-6">        alternative<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"smaller"</span></span>
<span id="cb3-7">    )</span>
<span id="cb3-8">    <span class="cf" style="color: #003B4F;">return</span> pval</span>
<span id="cb3-9"></span>
<span id="cb3-10">(df.pipe(summarize_experiment)</span>
<span id="cb3-11">   .pipe(pval_from_summary))</span></code></pre></div>
<p><img src="https://latex.codecogs.com/png.latex?%20p%20=%200.38%20"></p>
<p>We recommend to our stakeholders to roll out the variant since it “does no harm”</p>
<p><strong>There are some serious red flags here</strong></p>
<ul>
<li>First of all, p-values are all about the null hypothesis. So just because we don’t find a significant drop in conversion rate, that doesnt mean one doesnt exist. It just means we didnt find evidence for it in this test</li>
<li>There was no visualization of the uncertainty in the result</li>
</ul>
</section>
</section>
<section id="understanding-uncertainty-with-the-beta-distribution" class="level2">
<h2 class="anchored" data-anchor-id="understanding-uncertainty-with-the-beta-distribution">Understanding Uncertainty with the Beta Distribution</h2>
<p>For binary outcomes, the beta distribution is highly effective for understanding uncertainty.</p>
<p>It has 2 parameters * <strong>alpha</strong>, the number of successes * <strong>beta</strong>, the number of failures</p>
<p>It’s output is easy to interpret: Its a distribution of plausible probabilities that lead to the outcome.</p>
<p>So we can simply count our successes and failures from out observed data, plug it into a beta distribution to simulate outcomes, and visualize it as a density plot to understand uncertainty</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>It’s also easy to work with - if we want to understand the plausible differences between groups, we can just take the differences in our estimates</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdelta%20=%20%5Chat%7Bp_B%7D%20-%20%5Chat%7Bp_A%7D%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>With visualization, we get a very different picture than our non-significant p-value. We see that there’s plenty of plausibility that the control could be worse.</p>
<p>We can further calculate the probability of a drop, <img src="https://latex.codecogs.com/png.latex?P(B%20%3C%20A)">, and find that theres a greater than 60% probability that the variant is worse than the control</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># P(B &lt; A)</span></span>
<span id="cb4-2">(pB_hat <span class="op" style="color: #5E5E5E;">&lt;</span> pA_hat).mean()</span></code></pre></div>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(B%20%3C%20A)%20=%200.62%0A"></p>
<p>Remember when we designed the experiment? Considering our main goal was to do no harm, we might not feel so confident in that now, and rightly so, we know the variants worse since we simulated it.</p>
<p>Unless we feel very confident in our choice of testing for a 1/2% drop and know that we can afford anything up to that, then we we really shouldnt roll out this variant without further evaluation</p>
<p>This is particularly important with higher uncertainty As we can see in the example below, where the observed conversion rate is better in the variant, but the downside risk is as high as a 4% drop in conversion rate</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%200.59%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="another-example-which-metric" class="level1">
<h1>Another Example: Which Metric?</h1>
<p>This is a fun problem from <a href="https://twitter.com/seanjtaylor"><span class="citation" data-cites="seanjtaylor">@seanjtaylor</span></a> &gt; “You run a product test and measure a strong positive effect on your first metric.</p>
<blockquote class="blockquote">
<p>Metric 1: +1% (p&lt;.01)</p>
</blockquote>
<blockquote class="blockquote">
<p>You also see a negative, but not significant result on equally important Metric 2. You only care about these two metrics. Which of these estimates would you prefer to ship?”</p>
</blockquote>
<blockquote class="blockquote">
<ol type="1">
<li><input type="checkbox" unchecked=""> Metric 2: -0.5% (p = 0.10)</li>
<li><input type="checkbox" unchecked=""> Metric 2: -0.5% (p=0.45)</li>
<li><input type="checkbox" unchecked=""> Neither is shippable</li>
</ol>
</blockquote>
<p><br></p>
<p><em>Try to think it through on your own first, then scroll down for the answer</em></p>
<p><br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br></p>
<p>If you chose option 2, you weren’t alone. Option 1 makes it seem like there’s a more likely negative effect due to the lower p-value, so thats worse, right?</p>
<p>Not quite. Check out the uncertainties. The downside risk option 2 is much worse than option 1.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>We can take this one step further and add our effects to compare (remember we assumed the metrics are equally important), and see if it’s overall net positive</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig7.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As shown above, the non significant p value option has a higher probability of being negative, AND it gives more plausibility to more negative possible effects</p>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>Always report uncertainty intervals - p-values definitely dont tell the whole story, even with well designed experiments. As we saw, ignoring uncertainty can expose ourselves to high downside, especially when our choice in experiment design has even the slightest bit of arbitrary choices involved (such as an arbitrary minimum detectable effecs)</p>
<p>Reporting uncertainty intervals or beta distributions (or even bootstrapping) can be a great way to avoid falling for this mistake</p>


</section>

 ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-11-uncertainty_intervals_over_pvals.html</guid>
  <pubDate>Mon, 11 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig1.png" medium="image" type="image/png" height="60" width="144"/>
</item>
<item>
  <title>Explainable AI is not Causal Inference</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-03-explainable-ai-is-not-causal.html</link>
  <description><![CDATA[ 




<p>Explainable AI is all the rage these days. Black box ML models come along with some fun tools such as LIME, SHAP, or Partial Depence Plots that try to give visibility into how the model is interpreting data and making predictions. It’s a common misconception that these are causal inference techniques - sadly we’ve all been mislead.</p>
<p>We’re going to walk through an example that shows these tools fall victim to the same rules of causal inference as everything else. A confound is still a confound, and if you want to measure some causal effect there’s still no way around that without careful deliberation of which variables to include in your models.</p>
<p>The code for this blogpost can be found <a href="https://github.com/kylejcaron/case_studies/blob/master/Explainable%20AI%20vs%20Causal%20Inference.ipynb">here</a></p>
<section id="starting-simple-simulating-some-fake-data" class="level1">
<h1>Starting simple: simulating some fake data</h1>
<p>Let’s start with a simple scenario. Our goal is to estimate some causal effects. We’re going to simulate out data ourself so we know the true causal effects. We can see how good some popular “explainable AI” algorithms actually are at causal inference. We’ll simulate data from the following DAG:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_dag1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<hr>
<p><strong>What’s a DAG?</strong> A dag is a directed acyclic graph, or fancy talk for a flowchart that goes in 1 direction. It’s really just a diagram of a true data generating process. <strong>They’re typically assumed based on domain knowledge</strong> (like all models), although ocassionally there are some validation checks you can perform.</p>
<p>Edges in the graph are assumed to be true causal effects. So for example,</p>
<ul>
<li><code>X3</code> influences <code>Y</code></li>
<li><code>X5</code> influences <code>X1</code> which influences <code>Y</code></li>
<li>Some unobserved variable <code>U</code> influences both <code>X1</code> and <code>Y</code>. By unobserved, what I mean is that its some variable we don’t have data for.</li>
</ul>
<p>For those familiar with causal inference, this DAG in particular is also riddled with confounds.</p>
<hr>
<p>Ok back on track. We’ll get out one of the more popular Explainable AI tools nowadays, <code>XGBoost</code>. I’m going to start in the most dangerous way possible - I’m going to toss everything in the model.</p>
</section>
<section id="test-1-whats-the-impact-of-x1-on-y" class="level1">
<h1>Test 1: What’s the impact of X1 on Y?</h1>
<p>We know for a fact that X1 influences Y. Let’s see how well Partial Dependence Plots and SHAP values do at identifying the true causal effect</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>These SHAP values arent just wrong, but the effect is in the wrong direction. The reason for this: there’s a <strong>Fork Confound.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Some variable <code>Z</code> confounds <code>X</code>s true effect on <code>Y</code>.</p>
<blockquote class="blockquote">
<p>A very common example of a fork confound is <code>warm weather (Z)</code> on the relationship between <code>ice cream sales (X)</code> and <code>crime (Y)</code>. Ice cream sales obviously have no influence on crime, but ice cream sales are higher during warmer weather, and crime is higher during warmer weather.</p>
</blockquote>
<p>So back to our main point - Explainable AI can’t get around a fork confound. This is our first lesson on why SHAP / explainable AI is different from causal inference.</p>
<p>Luckily in this case, statistics can solve this problem.</p>
<p>Using some domain knowledge about the generating process, we notice an instrument, <code>X5</code>, that can be used to estimate the causal effect of <code>X1</code> on <code>Y</code></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>I won’t go into the details of instrumental variable analysis since the goal of this article is to highlight that Explainable AI can’t replace causal inference. To learn more about it, see <a href="https://mixtape.scunning.com/instrumental-variables.html?panelset=python-code&amp;panelset1=python-code2">Scott Cunningham’s Causal Inference the Mixtape</a>.</p>
<p>But for now, I will show that a classic causal inference method succeeds where XGBoost and SHAP values fail</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> linearmodels <span class="im" style="color: #00769E;">import</span> IV2SLS</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> src.dagtools <span class="im" style="color: #00769E;">import</span> get_effect</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Instrumental variable analysis</span></span>
<span id="cb1-5">iv_model <span class="op" style="color: #5E5E5E;">=</span> IV2SLS.from_formula(<span class="st" style="color: #20794D;">"Y ~ 1 + [X1 ~ X5]"</span>, data<span class="op" style="color: #5E5E5E;">=</span>df).fit()</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;"># pull true effect</span></span>
<span id="cb1-8">true_effect <span class="op" style="color: #5E5E5E;">=</span> get_effect(DAG, <span class="st" style="color: #20794D;">"X1"</span>, <span class="st" style="color: #20794D;">"Y"</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;"># Plot</span></span>
<span id="cb1-11">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb1-12">ax.set_title(<span class="st" style="color: #20794D;">"Instrumental Variable Analysis</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">Recovers True effect"</span>)</span>
<span id="cb1-13">plot_model_estimate(iv_model, true_effect<span class="op" style="color: #5E5E5E;">=</span>true_effect, feat<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"X1"</span>, ax<span class="op" style="color: #5E5E5E;">=</span>ax)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As we can see, a simple statistics technique succeeds where explainable AI fails.</p>
</section>
<section id="what-about-estimating-the-effect-of-x4-on-y" class="level1">
<h1>What about estimating the effect of X4 on Y?</h1>
<p>This relationship is slightly more complicated, but certainly measurable. <code>X4</code> influences <code>X2</code> which influences <code>Y</code>. Here’s the DAG again for reference</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_dag1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The plots below show how well explainable AI does at estimating the causal effect of this relationship.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Unfortunately, they don’t pick up an effect at all! And if our goal was to increase <code>Y</code> we’d end up missing a pretty good lever for it. There’s another simple explanation here for why explainable AI: there’s a <strong>Pipe Confound</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig7.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When trying to measure the effect of <code>X -&gt; Y</code>, conditioning on <code>Z</code> (aka including it in a model as a covariate with X) ends up blocking inference.</p>
<p>For more details on how a Pipe confound works, I recommend chapters 5 and 6 of <a href="https://xcelab.net/rm/statistical-rethinking/">Richard McElreath’s Statistical Rethinking v2</a> (where I borrowed the example from as well).</p>
<p>The main things to note here are that pipes are common and Explainable AI doesn’t get around them.</p>
<p>We can recover an unbiased estimate of the true effect simply with OLS</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Fit simple OLS model</span></span>
<span id="cb2-2">model <span class="op" style="color: #5E5E5E;">=</span> sm.OLS.from_formula(<span class="st" style="color: #20794D;">"Y ~ X4"</span>, data<span class="op" style="color: #5E5E5E;">=</span>df).fit()</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;"># pull true effect</span></span>
<span id="cb2-5">true_effect <span class="op" style="color: #5E5E5E;">=</span> get_effect(DAG, <span class="st" style="color: #20794D;">"X4"</span>, <span class="st" style="color: #20794D;">"X2"</span>) <span class="op" style="color: #5E5E5E;">*</span> get_effect(DAG, <span class="st" style="color: #20794D;">"X2"</span>, <span class="st" style="color: #20794D;">"Y"</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;"># Plot (see notebok for plot_model_estimate function)</span></span>
<span id="cb2-8">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb2-9">ax.set_title(<span class="st" style="color: #20794D;">"Instrumental Variable Analysis</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">Recovers True effect"</span>)</span>
<span id="cb2-10">plot_model_estimate(model, true_effect<span class="op" style="color: #5E5E5E;">=</span>true_effect, feat<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"X4"</span>, ax<span class="op" style="color: #5E5E5E;">=</span>ax)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="can-we-use-explainable-ai-for-causal-inference-at-all" class="level1">
<h1>Can we use Explainable AI for causal inference at all?</h1>
<p>We can! We just need to be deliberate in which variables we include in our models, and the only way to do that right is to use DAGs! The example below looks at an XGBoost model that doesnt condition on <code>X2</code> (allowing us to estimate the causal effect of <code>X4 -&gt; Y</code>).</p>
<p>[](/assets/img/explainable_ai_fig9.png]{fig-align=“center”}</p>
</section>
<section id="take-aways" class="level1">
<h1>Take Aways</h1>
<p>Explainable AI is not some magic tool for causal inference. What these tools are good at is explaining why complicated models make the decisions they do. Explainable AI tools suffer from the same limitations for causal inference as all other statistical estimators.</p>
<p>At the end of the day when causal inference is your goal, nothing beats using DAGs to inform deliberate variable selection.</p>
<p>If you’re new to the subject, I highly recommend the following resources that will teach you how to use causal inference properly: * Chapter’s 5 and 6 of Statistical Rethinking v2, by Richard McElreath * Causal Inference for the Brave and True by Matheus Facure * Causal Inference the Mixtape, by Scott Cunningham</p>


</section>

 ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-03-explainable-ai-is-not-causal.html</guid>
  <pubDate>Sun, 03 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_dag1.png" medium="image" type="image/png" height="101" width="144"/>
</item>
</channel>
</rss>
