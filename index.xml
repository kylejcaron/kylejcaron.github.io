<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Kyle Caron</title>
<link>https://github.com/kylejcaron/kylejcaron.github.io/index.html</link>
<atom:link href="https://github.com/kylejcaron/kylejcaron.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>Welcome to my blog! I'm a self-taught data scientist interested in causal inference and bayesian methods. I mainly use this blog to formalize what I learn, but hopefully others find it helpful as well.</description>
<generator>quarto-1.1.163</generator>
<lastBuildDate>Sun, 17 Apr 2022 04:00:00 GMT</lastBuildDate>
<item>
  <title>Making out of sample predictions with PyMC</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-17-out-of-sample-pymc.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>A cool thing about hierarchical models is that its easy to predict out of sample - i.e.&nbsp;if you want to make a prediction on a new zipcode, just sample from the state‚Äôs distribution (composed of the state average and variance across zip codes in that state).</p>
<p>In pymc3, it‚Äôs somewhat easy to accomplish this, but not as straightforward as we‚Äôd hope. This blog post will show a trick that lets you easily predict out of sample, and will reduce some of the overhead that comes from writing alot of custom prediction functions</p>
</section>
<section id="simulating-data" class="level1">
<h1>Simulating data</h1>
<p>I simulated a 2 level hierarchical model - for interpretability, I set it up as a state &gt; zipcode model. You can following along with the notebook <a href="https://github.com/kylejcaron/case_studies/blob/main/PyMC%20out%20of%20sample%20predictions.ipynb">here</a>. The data is as follows</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="using-categorical-variables" class="level1">
<h1>Using categorical variables</h1>
<p>Categorical variables are a somewhat new feature of pandas - they can store categories that aren‚Äôt in the observed data, and are an easy replacement for <code>pd.factorize()</code> (a common tool for those familiar with the bayesian workflow).</p>
<p>We can use these to trick pymc into thinking there‚Äôs a category with no observed data, and pymc ends up assigning the global distribution to that unobserved category, which we can simply reference in the future for any time we want to make a prediction on out of sample data.</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="co" style="color: #5E5E5E;"># Convert to categorical and add an `out_of_sample` category</span></span>
<span id="cb1-2">df <span class="op" style="color: #5E5E5E;">=</span> df.assign(state <span class="op" style="color: #5E5E5E;">=</span> pd.Categorical(df.state).add_categories(<span class="st" style="color: #20794D;">"out_of_sample"</span>))<span class="op" style="color: #5E5E5E;">\</span></span>
<span id="cb1-3">    .assign(zipcode <span class="op" style="color: #5E5E5E;">=</span> pd.Categorical(df.zipcode).add_categories(<span class="st" style="color: #20794D;">"out_of_sample"</span>))</span></code></pre></div>
</section>
<section id="fitting-the-model" class="level1">
<h1>Fitting the model</h1>
<p>We‚Äôll use the codes from the categorical columns to index our model coefficients, and we‚Äôll use the categories as coordinates for the model to map names to.</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1">coords<span class="op" style="color: #5E5E5E;">=</span>{</span>
<span id="cb2-2">    <span class="st" style="color: #20794D;">"state"</span>:df.state.cat.categories,</span>
<span id="cb2-3">    <span class="st" style="color: #20794D;">"zipcode"</span>:df.zipcode.cat.categories</span>
<span id="cb2-4">}</span>
<span id="cb2-5"></span>
<span id="cb2-6"><span class="kw" style="color: #003B4F;">def</span> hierarchical_normal(name, Œº, dims):</span>
<span id="cb2-7">    <span class="co" style="color: #5E5E5E;">'''Adapted from Austin Rochford'''</span></span>
<span id="cb2-8">    Œî <span class="op" style="color: #5E5E5E;">=</span> pm.Normal(<span class="st" style="color: #20794D;">'Œî_</span><span class="sc" style="color: #5E5E5E;">{}</span><span class="st" style="color: #20794D;">'</span>.<span class="bu" style="color: null;">format</span>(name), <span class="fl" style="color: #AD0000;">0.</span>, <span class="fl" style="color: #AD0000;">1.</span>, dims<span class="op" style="color: #5E5E5E;">=</span>dims)</span>
<span id="cb2-9">    œÉ <span class="op" style="color: #5E5E5E;">=</span> pm.Exponential(<span class="st" style="color: #20794D;">'œÉ_</span><span class="sc" style="color: #5E5E5E;">{}</span><span class="st" style="color: #20794D;">'</span>.<span class="bu" style="color: null;">format</span>(name), <span class="fl" style="color: #AD0000;">2.5</span>)</span>
<span id="cb2-10">    <span class="cf" style="color: #003B4F;">return</span> pm.Deterministic(name, Œº <span class="op" style="color: #5E5E5E;">+</span> Œî <span class="op" style="color: #5E5E5E;">*</span> œÉ, dims<span class="op" style="color: #5E5E5E;">=</span>dims)</span>
<span id="cb2-11"></span>
<span id="cb2-12"></span>
<span id="cb2-13"><span class="cf" style="color: #003B4F;">with</span> pm.Model(coords<span class="op" style="color: #5E5E5E;">=</span>coords) <span class="im" style="color: #00769E;">as</span> model_nc:</span>
<span id="cb2-14">    </span>
<span id="cb2-15">    <span class="co" style="color: #5E5E5E;"># Observed Data tracking</span></span>
<span id="cb2-16">    state_ <span class="op" style="color: #5E5E5E;">=</span> pm.Data(<span class="st" style="color: #20794D;">"state_"</span>, df.state.cat.codes)</span>
<span id="cb2-17">    zip_ <span class="op" style="color: #5E5E5E;">=</span> pm.Data(<span class="st" style="color: #20794D;">"zip_"</span>, df.zipcode.cat.codes)</span>
<span id="cb2-18">    obs <span class="op" style="color: #5E5E5E;">=</span> pm.Data(<span class="st" style="color: #20794D;">"obs"</span>, df.y)</span>
<span id="cb2-19"></span>
<span id="cb2-20">    <span class="co" style="color: #5E5E5E;"># Hyperprior</span></span>
<span id="cb2-21">    mu_country <span class="op" style="color: #5E5E5E;">=</span> pm.Normal(<span class="st" style="color: #20794D;">"mu_country"</span>, <span class="dv" style="color: #AD0000;">0</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-22">    </span>
<span id="cb2-23">    <span class="co" style="color: #5E5E5E;"># Prior</span></span>
<span id="cb2-24">    sig <span class="op" style="color: #5E5E5E;">=</span> pm.Exponential(<span class="st" style="color: #20794D;">"sig"</span>, <span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb2-25">    </span>
<span id="cb2-26">    <span class="co" style="color: #5E5E5E;"># Hierarchical coefficients</span></span>
<span id="cb2-27">    mu_state <span class="op" style="color: #5E5E5E;">=</span> hierarchical_normal(<span class="st" style="color: #20794D;">"mu_state"</span>, Œº<span class="op" style="color: #5E5E5E;">=</span>mu_country, dims<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"state"</span>)</span>
<span id="cb2-28">    mu_zipcode <span class="op" style="color: #5E5E5E;">=</span> hierarchical_normal(<span class="st" style="color: #20794D;">"mu_zipcode"</span>, Œº<span class="op" style="color: #5E5E5E;">=</span>mu_state, dims<span class="op" style="color: #5E5E5E;">=</span>(<span class="st" style="color: #20794D;">"zipcode"</span>, <span class="st" style="color: #20794D;">"state"</span>) )</span>
<span id="cb2-29">    </span>
<span id="cb2-30">    <span class="co" style="color: #5E5E5E;"># Observational model</span></span>
<span id="cb2-31">    y <span class="op" style="color: #5E5E5E;">=</span> pm.Normal(<span class="st" style="color: #20794D;">"y"</span>, mu_zipcode[zip_, state_], sig, observed<span class="op" style="color: #5E5E5E;">=</span>obs)</span>
<span id="cb2-32">    </span>
<span id="cb2-33">    <span class="co" style="color: #5E5E5E;"># Fit </span></span>
<span id="cb2-34">    trace_nc <span class="op" style="color: #5E5E5E;">=</span> pm.sample(target_accept<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>, return_inferencedata<span class="op" style="color: #5E5E5E;">=</span><span class="va" style="color: #111111;">True</span>, random_seed<span class="op" style="color: #5E5E5E;">=</span>SEED)</span></code></pre></div>
<p>There are a few key point that make out of sample prediction possible * Having the <code>out_of_sample</code> category for each indexed variable with no observed data * Passing the <code>coords</code> in the model statement * Using dims to reference which model coefficients have which coordinate labels * Having all of our input data wrapped in a <code>pm.Data()</code> statement</p>
<p>That last point is particularly important. For PyMC, if you want to make predictions on new data, you have to replace the data that the model references and the only way to do that (that I know of atleast) is to using a Theano shared variable. <code>pm.Data()</code> handles all of that fo you.</p>
<p>So we fit our model, lets take a quick look at the state level coefficients</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1">pm.plot_forest(trace_nc, var_names<span class="op" style="color: #5E5E5E;">=</span>[<span class="st" style="color: #20794D;">"mu_state"</span>])</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Great, that out of sample variable seems to represent the <code>global</code> distribution across states - i.e.&nbsp;if we were to make a prediction for a new state we‚Äôd potentially use that distribtion (we‚Äôll confirm further down).</p>
<p>We‚Äôll check the zip code level below as well, looking at Maine specifically</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As we can see, the <code>out_of_sample</code> variable has a sampled value despite there being no observed data for it. Now the question is, does this align with how we‚Äôd predict new data?</p>
<p>Let‚Äôs try calculating coefficients out of sample by hand and see if it aligns with the out_of_sample values</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1">post <span class="op" style="color: #5E5E5E;">=</span> trace_nc.posterior</span>
<span id="cb4-2"></span>
<span id="cb4-3"><span class="co" style="color: #5E5E5E;"># Pull the true data from our simulation</span></span>
<span id="cb4-4">state_true <span class="op" style="color: #5E5E5E;">=</span> mu_state_true.random(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4000</span>)</span>
<span id="cb4-5"></span>
<span id="cb4-6"></span>
<span id="cb4-7"><span class="co" style="color: #5E5E5E;"># Calculate out of sample state means by drawing from global distribution</span></span>
<span id="cb4-8">mu_country <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"mu_country"</span>].values.reshape(<span class="dv" style="color: #AD0000;">4000</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-9">œÉ_state <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"œÉ_mu_state"</span>].values.reshape(<span class="dv" style="color: #AD0000;">4000</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-10">mu_state <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(mu_country, œÉ_state)</span>
<span id="cb4-11"></span>
<span id="cb4-12"><span class="co" style="color: #5E5E5E;"># Using the indexing trick</span></span>
<span id="cb4-13">state_idx_trick <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"mu_state"</span>].sel({<span class="st" style="color: #20794D;">"state"</span>:[<span class="st" style="color: #20794D;">"out_of_sample"</span>]}).values.ravel()</span>
<span id="cb4-14"></span>
<span id="cb4-15"><span class="co" style="color: #5E5E5E;"># Pull the true data from simulation</span></span>
<span id="cb4-16">zip_true <span class="op" style="color: #5E5E5E;">=</span> pm.Normal.dist(mu_state_true.random(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4000</span>), sig_zip_true).random(size<span class="op" style="color: #5E5E5E;">=</span><span class="dv" style="color: #AD0000;">4000</span>)</span>
<span id="cb4-17"></span>
<span id="cb4-18"><span class="co" style="color: #5E5E5E;"># calculate out of sample mu by hand by drawing from out of sample state prediction above</span></span>
<span id="cb4-19">œÉ_zipcode <span class="op" style="color: #5E5E5E;">=</span> post[<span class="st" style="color: #20794D;">"œÉ_mu_zipcode"</span>].values.reshape(<span class="dv" style="color: #AD0000;">4000</span>,<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>)</span>
<span id="cb4-20">mu_zipcode <span class="op" style="color: #5E5E5E;">=</span> np.random.normal(mu_state, œÉ_zipcode)</span>
<span id="cb4-21"></span>
<span id="cb4-22"><span class="co" style="color: #5E5E5E;"># Use the indexing trick</span></span>
<span id="cb4-23">zip_idx_trick <span class="op" style="color: #5E5E5E;">=</span> (post[<span class="st" style="color: #20794D;">"mu_zipcode"</span>]</span>
<span id="cb4-24">                .sel({<span class="st" style="color: #20794D;">"state"</span>:[<span class="st" style="color: #20794D;">"out_of_sample"</span>], <span class="st" style="color: #20794D;">"zipcode"</span>:[<span class="st" style="color: #20794D;">"out_of_sample"</span>]})</span>
<span id="cb4-25">                .values.ravel())</span></code></pre></div>
<p>We can compare these results by plotting their distributions below</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Notice that the manual prediction and the indexing trick are basically identical. There‚Äôs a slight difference from the ground truth, but thats to be expected since we‚Äôre fitting a model on limited data (and anyway, it‚Äôs still quite close).</p>
</section>
<section id="predicting-out-of-sample" class="level1">
<h1>Predicting out of sample</h1>
<p>Let‚Äôs go ahead and actually make prediction now - we‚Äôll make predictions for the following data below</p>
<ul>
<li>The first example is in sample</li>
<li>The second example is in sample for state, out of sample for zipcode</li>
<li>The third example is out of sample entirely</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>And finally we‚Äôll use the model to make predictions on this new data. Notice the <code>pm.set_data()</code> function - remember our <code>pm.Data()</code> calls from before? This tells PyMC to override that with new data, so when we sample from the posterior predictive it makes predictions on the new data instead of the data used to fit the model.</p>
<details>
<summary>
Click here for helper function code
</summary>
<div class="sourceCode" id="cb5" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><span class="co" style="color: #5E5E5E;"># We're making some quick convenience functions to map this new data </span></span>
<span id="cb5-2"><span class="co" style="color: #5E5E5E;"># to the proper indexes from the fitted model</span></span>
<span id="cb5-3">zip_lookup <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(<span class="bu" style="color: null;">zip</span>(df.zipcode.cat.categories, <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(df.zipcode.cat.categories))))</span>
<span id="cb5-4">state_lookup <span class="op" style="color: #5E5E5E;">=</span> <span class="bu" style="color: null;">dict</span>(<span class="bu" style="color: null;">zip</span>(df.state.cat.categories, <span class="bu" style="color: null;">range</span>(<span class="bu" style="color: null;">len</span>(df.state.cat.categories))))</span>
<span id="cb5-5"></span>
<span id="cb5-6"><span class="kw" style="color: #003B4F;">def</span> labels_to_index(series, lookup):</span>
<span id="cb5-7">    <span class="co" style="color: #5E5E5E;">'''Converts categories to their proper codes'''</span></span>
<span id="cb5-8">    series <span class="op" style="color: #5E5E5E;">=</span> series.copy()</span>
<span id="cb5-9">    in_sample <span class="op" style="color: #5E5E5E;">=</span> series.isin(lookup.keys())</span>
<span id="cb5-10">    series.loc[<span class="op" style="color: #5E5E5E;">~</span>in_sample] <span class="op" style="color: #5E5E5E;">=</span> <span class="st" style="color: #20794D;">"out_of_sample"</span></span>
<span id="cb5-11">    <span class="cf" style="color: #003B4F;">return</span> series.<span class="bu" style="color: null;">map</span>(lookup).values.astype(<span class="st" style="color: #20794D;">"int8"</span>)</span></code></pre></div>
</details>
<p><br></p>
<div class="sourceCode" id="cb6" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><span class="cf" style="color: #003B4F;">with</span> model_nc:</span>
<span id="cb6-2">    <span class="co" style="color: #5E5E5E;"># Set new data for the model to make predictions on</span></span>
<span id="cb6-3">    pm.set_data({</span>
<span id="cb6-4">        <span class="st" style="color: #20794D;">"state_"</span>: X.state.pipe(labels_to_index, state_lookup),</span>
<span id="cb6-5">        <span class="st" style="color: #20794D;">"zip_"</span>: X.zipcode.pipe(labels_to_index, zip_lookup)</span>
<span id="cb6-6">    })</span>
<span id="cb6-7">    </span>
<span id="cb6-8">    <span class="co" style="color: #5E5E5E;"># make predictions</span></span>
<span id="cb6-9">    preds <span class="op" style="color: #5E5E5E;">=</span> pm.sample_posterior_predictive(trace_nc)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>This is exactly what we were looking for - and prediction is easy, just map any out of sample states or zipcodes to the <code>out_of_sample</code> category. Notice how in sample predictions have smaller uncertainty intervals and out of sample data is more uncertain - this is exactly what we‚Äôd expect. This trick makes it much easier to make predictions compared to having to write out a custom prediction function that follows the same logic as the model.</p>
<p>If you have any other easy tricks for out of sample prediction let me know!</p>


</section>

 ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-17-out-of-sample-pymc.html</guid>
  <pubDate>Sun, 17 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/out_of_sample/fig1.png" medium="image" type="image/png" height="118" width="144"/>
</item>
<item>
  <title>How long should you run an A/B test for?</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-05-ab-test-duration.html</link>
  <description><![CDATA[ 




<p>For some people in industry new to A/B testing, they might wonder ‚ÄúWhy cant we just run an A/B test for 2 days and be done with it?‚Äù. Even those familiar with it might wonder why their team‚Äôs Data Scientist is insisting on so much. And even Data Scientists may be looking for easier ways to explain the need to them. The goal of this article is to cover just that, from a naive and explainable point of view.</p>
<p>So, <strong>How long should you run an A/B test for?</strong> Well let‚Äôs say you step into a casino with $5000 and you walk away with $6000. You just made a 20% return. Is it fair to say that a night out in the casino leads to a 20% return? Is it fair to say that our A/B test we ran for 2 days leads to a 20% lift in conversion? How do we know for sure?</p>
<p><strong>We should run an A/B test for as long as it takes to rule out random chance.</strong></p>
<p>While vague, and technically not the full picture, your friendly neightborhood data scientist should be able to answer this for you. The code for this blogpost can be found <a href="https://github.com/kylejcaron/case_studies/blob/master/How%20long%20should%20you%20run%20an%20AB%20Test%20for%3F.ipynb">here</a>.</p>
<p><br></p>
<section id="simulating-a-fake-scenario" class="level3">
<h3 class="anchored" data-anchor-id="simulating-a-fake-scenario">Simulating a fake scenario</h3>
<p>Let‚Äôs play out the casino example from above. I‚Äôm going to simulate out an entirely fake, but entirely possible scenario.</p>
<p>You go to the casino one night with $5000 and decide roulette is your game of choice. You get a little into it and play 500 rounds (in one night?? for examples sake, yes). Little do you know the real probability of winning is 48.5%</p>
<p>The plot below shows the total money you had at the start of each round of roulette</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>This is great - after 500 rounds of this you‚Äôve made 122% return on your initial investment of $5000 and you‚Äôre winning 51% of the time.</p>
<p>Playing roulette must lead to a 20% return right? Commited to your strategy you decide to come back over the next few weeks and play another 3000 rounds, shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Alright you‚Äôve played 3500 rounds now and you have $5400 total. You‚Äôve definitely had some runs of bad luck, but you‚Äôre still seeing a win percentage above 50% (50.1% in fact) and right now you‚Äôre heating up. You stay determined and play until you reach 15000 rounds.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="what-happened" class="level1">
<h1>What happened?</h1>
<p>We started off on a hot streak winning 51% of our rounds, but as we played more and more rounds, it became more obvious we were losing money. This is a demonstration of the law of large numbers - as we play more and more rounds, the truth comes out</p>
<p>We can visualize this process via the beta distribution below. These plots visualize all of the possible values that the true win percentage could be (the x axis), and their relative plausibilities (the y axis). The first plot can be read as follows:</p>
<blockquote class="blockquote">
<p>The win percentage is likely to be somewhere between 42.5% and 60%, with the most likely value being around 51%</p>
</blockquote>
<p>As we move from left to right, our estimated distribution converges closer and closer to the true probability of winning a round</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>We can also visualize this as a time series, which really makes it clear how the uncertainty becomes smaller over time and the estimated value converges to the true value.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><br></p>
<section id="how-does-this-tie-back-to-ab-testing" class="level3">
<h3 class="anchored" data-anchor-id="how-does-this-tie-back-to-ab-testing">How does this tie back to A/B testing?</h3>
<p>If we don‚Äôt choose our sample size for an experiment properly, we can end up making the wrong decisions!<sup>1</sup> The larger the sample size we choose, the more likely we‚Äôll make the right choice.</p>
<p>We can use <strong>power analyses</strong> (sometimes referred to as simulation studies) to estimate what sample size is needed for an experiment given the desired outcome.</p>
<hr>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Unless you‚Äôre using bayesian inference, which can really mitigate this risk.‚Ü©Ô∏é</p></li>
</ol>
</section></div> ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-05-ab-test-duration.html</guid>
  <pubDate>Fri, 15 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/how_long_ab_test/fig1.png" medium="image" type="image/png" height="80" width="144"/>
</item>
<item>
  <title>Uncertainty Intervals or p-values?</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-11-uncertainty_intervals_over_pvals.html</link>
  <description><![CDATA[ 




<p>Uncertainty Intervals are better than p-values. Sure, its better to use both, but p-values are just a point estimate and they bring no concept of uncertainty in our estimate - this can lead to situations where we expose ourselves to high downside risk.</p>
<p>Take the following example for instance. Let‚Äôs say we‚Äôre running a ‚ÄúDo no harm‚Äù A/B test where we want to roll out an experiment as long as it doesnt harm conversion rate.</p>
<p>If you want to follow along with the code, <a href="https://github.com/kylejcaron/case_studies/blob/main/Uncertainty%20intervals%20over%20p%20values.ipynb">see here</a>.</p>
<section id="the-experiment-design" class="level2">
<h2 class="anchored" data-anchor-id="the-experiment-design">The experiment design</h2>
<p>Given the stakeholders want to rule out a drop in conversion, and ruling out small differences requires large sample sizes, we decide to design an experiment with good power to detect the presence of a 1/2% absolute drop (if one were to truly exist)</p>
<p>We ran a power analysis and found that in order to have a 90% probability of detecting (power=0.9) a 1/2% absolute drop in conversion rate with 80 percent confidence ( ùõº=0.2 ), we need N=32500 per group</p>
<blockquote class="blockquote">
<p>Statisticians might not love this interpretation of a power analysis, but its a useful and interpretable translation and tends to coincide with what we‚Äôre aiming for anyway. In reality, frequentist power analyses assume that the null hypothesis is correct, which isn‚Äôt quite what we want, not to mention, frequentist power analyses use backwards probabilities which are just plain confusing - <a href="https://www.fharrell.com/post/pvalprobs/">see here to for more</a></p>
</blockquote>
<p>Note that we‚Äôre prioritizing power here for a reason. If ùõº is false positive rate, and power is probability of detection, then don‚Äôt we want to prioritize our probability of detecting a drop if one truly exists? A false negative here would be more expensive then a false positive</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1">pA <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.1</span> <span class="co" style="color: #5E5E5E;"># historical conversion rate</span></span>
<span id="cb1-2">abs_delta <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.005</span> <span class="co" style="color: #5E5E5E;"># minimum detectable effect to test for</span></span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Statsmodels requires an effect size </span></span>
<span id="cb1-5"><span class="co" style="color: #5E5E5E;"># (aka an effect normalized by its standard deviation)</span></span>
<span id="cb1-6">stdev <span class="op" style="color: #5E5E5E;">=</span> np.sqrt( pA<span class="op" style="color: #5E5E5E;">*</span>(<span class="dv" style="color: #AD0000;">1</span><span class="op" style="color: #5E5E5E;">-</span>pA) ) <span class="co" style="color: #5E5E5E;"># bernoulli stdev, sigma = sqrt(p(1-p))</span></span>
<span id="cb1-7">ES <span class="op" style="color: #5E5E5E;">=</span> abs_delta <span class="op" style="color: #5E5E5E;">/</span> stdev </span>
<span id="cb1-8"></span>
<span id="cb1-9"><span class="co" style="color: #5E5E5E;"># estimate required sample size</span></span>
<span id="cb1-10">sm.stats.tt_ind_solve_power(</span>
<span id="cb1-11">    <span class="op" style="color: #5E5E5E;">-</span>ES, </span>
<span id="cb1-12">    alpha<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.2</span>,</span>
<span id="cb1-13">    power<span class="op" style="color: #5E5E5E;">=</span><span class="fl" style="color: #AD0000;">0.9</span>,</span>
<span id="cb1-14">    alternative<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"smaller"</span></span>
<span id="cb1-15">)</span></code></pre></div>
<p>Running the code above leads us to conclude are sample size should be roughly 32,500 users per group.</p>
<section id="the-experiment" class="level3">
<h3 class="anchored" data-anchor-id="the-experiment">The experiment</h3>
<p>I‚Äôm going to simulate fake data for this experiment where * The <strong>control</strong> has a <strong><em>true</em></strong> conversion rate of 10% * the <strong>variant</strong> has a <strong><em>true</em></strong> conversion rate of 9.25%</p>
<p>For examples sake we‚Äôll pretend we don‚Äôt know that the variant is worse</p>
<details>
<summary>
Click here for code
</summary>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Settings</span></span>
<span id="cb2-2">np.random.seed(<span class="dv" style="color: #AD0000;">1325</span>)</span>
<span id="cb2-3">N <span class="op" style="color: #5E5E5E;">=</span> <span class="dv" style="color: #AD0000;">32500</span></span>
<span id="cb2-4">pA <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.1</span></span>
<span id="cb2-5">pB <span class="op" style="color: #5E5E5E;">=</span> <span class="fl" style="color: #AD0000;">0.0925</span></span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;"># Simulation</span></span>
<span id="cb2-8"><span class="kw" style="color: #003B4F;">def</span> simulate_experiment(pA, pB, N_per_group):</span>
<span id="cb2-9">    </span>
<span id="cb2-10">    df <span class="op" style="color: #5E5E5E;">=</span> pd.DataFrame({</span>
<span id="cb2-11">        <span class="st" style="color: #20794D;">"group"</span>:[<span class="st" style="color: #20794D;">"A"</span>]<span class="op" style="color: #5E5E5E;">*</span>N <span class="op" style="color: #5E5E5E;">+</span> [<span class="st" style="color: #20794D;">"B"</span>]<span class="op" style="color: #5E5E5E;">*</span>N,</span>
<span id="cb2-12">        <span class="st" style="color: #20794D;">"convert"</span>:np.r_[</span>
<span id="cb2-13">             np.random.binomial(<span class="dv" style="color: #AD0000;">1</span>, p<span class="op" style="color: #5E5E5E;">=</span>pA, size<span class="op" style="color: #5E5E5E;">=</span>N),</span>
<span id="cb2-14">             np.random.binomial(<span class="dv" style="color: #AD0000;">1</span>, p<span class="op" style="color: #5E5E5E;">=</span>pB, size<span class="op" style="color: #5E5E5E;">=</span>N)</span>
<span id="cb2-15">        ]</span>
<span id="cb2-16">    })</span>
<span id="cb2-17">    </span>
<span id="cb2-18">    <span class="cf" style="color: #003B4F;">return</span> df</span>
<span id="cb2-19"></span>
<span id="cb2-20">df <span class="op" style="color: #5E5E5E;">=</span> simulate_experiment(pA, pB, N)</span></code></pre></div>
</details>
<p><br></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Looking at the data above, we‚Äôre seeing a better conversion rate in group B. We run a two-proportions z-test and we find that there‚Äôs a non-significant p-value, meaning we found insufficient evidence of the variant having lower conversion than the control.</p>
<div class="sourceCode" id="cb3" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><span class="kw" style="color: #003B4F;">def</span> pval_from_summary(tab):</span>
<span id="cb3-2">    </span>
<span id="cb3-3">    _, pval <span class="op" style="color: #5E5E5E;">=</span> sm.stats.proportions_ztest(</span>
<span id="cb3-4">        count<span class="op" style="color: #5E5E5E;">=</span>tab[<span class="st" style="color: #20794D;">"converts"</span>][::<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>], </span>
<span id="cb3-5">        nobs<span class="op" style="color: #5E5E5E;">=</span>tab[<span class="st" style="color: #20794D;">"N"</span>][::<span class="op" style="color: #5E5E5E;">-</span><span class="dv" style="color: #AD0000;">1</span>],</span>
<span id="cb3-6">        alternative<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"smaller"</span></span>
<span id="cb3-7">    )</span>
<span id="cb3-8">    <span class="cf" style="color: #003B4F;">return</span> pval</span>
<span id="cb3-9"></span>
<span id="cb3-10">(df.pipe(summarize_experiment)</span>
<span id="cb3-11">   .pipe(pval_from_summary))</span></code></pre></div>
<p><img src="https://latex.codecogs.com/png.latex?%20p%20=%200.38%20"></p>
<p>We recommend to our stakeholders to roll out the variant since it ‚Äúdoes no harm‚Äù</p>
<p><strong>There are some serious red flags here</strong></p>
<ul>
<li>First of all, p-values are all about the null hypothesis. So just because we don‚Äôt find a significant drop in conversion rate, that doesnt mean one doesnt exist. It just means we didnt find evidence for it in this test</li>
<li>There was no visualization of the uncertainty in the result</li>
</ul>
</section>
</section>
<section id="understanding-uncertainty-with-the-beta-distribution" class="level2">
<h2 class="anchored" data-anchor-id="understanding-uncertainty-with-the-beta-distribution">Understanding Uncertainty with the Beta Distribution</h2>
<p>For binary outcomes, the beta distribution is highly effective for understanding uncertainty.</p>
<p>It has 2 parameters * <strong>alpha</strong>, the number of successes * <strong>beta</strong>, the number of failures</p>
<p>It‚Äôs output is easy to interpret: Its a distribution of plausible probabilities that lead to the outcome.</p>
<p>So we can simply count our successes and failures from out observed data, plug it into a beta distribution to simulate outcomes, and visualize it as a density plot to understand uncertainty</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>It‚Äôs also easy to work with - if we want to understand the plausible differences between groups, we can just take the differences in our estimates</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdelta%20=%20%5Chat%7Bp_B%7D%20-%20%5Chat%7Bp_A%7D%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>With visualization, we get a very different picture than our non-significant p-value. We see that there‚Äôs plenty of plausibility that the control could be worse.</p>
<p>We can further calculate the probability of a drop, <img src="https://latex.codecogs.com/png.latex?P(B%20%3C%20A)">, and find that theres a greater than 60% probability that the variant is worse than the control</p>
<div class="sourceCode" id="cb4" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><span class="co" style="color: #5E5E5E;"># P(B &lt; A)</span></span>
<span id="cb4-2">(pB_hat <span class="op" style="color: #5E5E5E;">&lt;</span> pA_hat).mean()</span></code></pre></div>
<p><img src="https://latex.codecogs.com/png.latex?%0AP(B%20%3C%20A)%20=%200.62%0A"></p>
<p>Remember when we designed the experiment? Considering our main goal was to do no harm, we might not feel so confident in that now, and rightly so, we know the variants worse since we simulated it.</p>
<p>Unless we feel very confident in our choice of testing for a 1/2% drop and know that we can afford anything up to that, then we we really shouldnt roll out this variant without further evaluation</p>
<p>This is particularly important with higher uncertainty As we can see in the example below, where the observed conversion rate is better in the variant, but the downside risk is as high as a 4% drop in conversion rate</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p><img src="https://latex.codecogs.com/png.latex?%0Ap%20=%200.59%0A"></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="another-example-which-metric" class="level1">
<h1>Another Example: Which Metric?</h1>
<p>This is a fun problem from <a href="https://twitter.com/seanjtaylor"><span class="citation" data-cites="seanjtaylor">@seanjtaylor</span></a> &gt; ‚ÄúYou run a product test and measure a strong positive effect on your first metric.</p>
<blockquote class="blockquote">
<p>Metric 1: +1% (p&lt;.01)</p>
</blockquote>
<blockquote class="blockquote">
<p>You also see a negative, but not significant result on equally important Metric 2. You only care about these two metrics. Which of these estimates would you prefer to ship?‚Äù</p>
</blockquote>
<blockquote class="blockquote">
<ol type="1">
<li><input type="checkbox" unchecked=""> Metric 2: -0.5% (p = 0.10)</li>
<li><input type="checkbox" unchecked=""> Metric 2: -0.5% (p=0.45)</li>
<li><input type="checkbox" unchecked=""> Neither is shippable</li>
</ol>
</blockquote>
<p><br></p>
<p><em>Try to think it through on your own first, then scroll down for the answer</em></p>
<p><br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br></p>
<p>If you chose option 2, you weren‚Äôt alone. Option 1 makes it seem like there‚Äôs a more likely negative effect due to the lower p-value, so thats worse, right?</p>
<p>Not quite. Check out the uncertainties. The downside risk option 2 is much worse than option 1.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>We can take this one step further and add our effects to compare (remember we assumed the metrics are equally important), and see if it‚Äôs overall net positive</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig7.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As shown above, the non significant p value option has a higher probability of being negative, AND it gives more plausibility to more negative possible effects</p>
</section>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>Always report uncertainty intervals - p-values definitely dont tell the whole story, even with well designed experiments. As we saw, ignoring uncertainty can expose ourselves to high downside, especially when our choice in experiment design has even the slightest bit of arbitrary choices involved (such as an arbitrary minimum detectable effecs)</p>
<p>Reporting uncertainty intervals or beta distributions (or even bootstrapping) can be a great way to avoid falling for this mistake</p>


</section>

 ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-11-uncertainty_intervals_over_pvals.html</guid>
  <pubDate>Mon, 11 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/uncertainty_fig1.png" medium="image" type="image/png" height="60" width="144"/>
</item>
<item>
  <title>Explainable AI is not Causal Inference</title>
  <link>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-03-explainable-ai-is-not-causal.html</link>
  <description><![CDATA[ 




<p>Explainable AI is all the rage these days. Black box ML models come along with some fun tools such as LIME, SHAP, or Partial Depence Plots that try to give visibility into how the model is interpreting data and making predictions. It‚Äôs a common misconception that these are causal inference techniques - sadly we‚Äôve all been mislead.</p>
<p>We‚Äôre going to walk through an example that shows these tools fall victim to the same rules of causal inference as everything else. A confound is still a confound, and if you want to measure some causal effect there‚Äôs still no way around that without careful deliberation of which variables to include in your models.</p>
<p>The code for this blogpost can be found <a href="https://github.com/kylejcaron/case_studies/blob/master/Explainable%20AI%20vs%20Causal%20Inference.ipynb">here</a></p>
<section id="starting-simple-simulating-some-fake-data" class="level1">
<h1>Starting simple: simulating some fake data</h1>
<p>Let‚Äôs start with a simple scenario. Our goal is to estimate some causal effects. We‚Äôre going to simulate out data ourself so we know the true causal effects. We can see how good some popular ‚Äúexplainable AI‚Äù algorithms actually are at causal inference. We‚Äôll simulate data from the following DAG:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_dag1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<hr>
<p><strong>What‚Äôs a DAG?</strong> A dag is a directed acyclic graph, or fancy talk for a flowchart that goes in 1 direction. It‚Äôs really just a diagram of a true data generating process. <strong>They‚Äôre typically assumed based on domain knowledge</strong> (like all models), although ocassionally there are some validation checks you can perform.</p>
<p>Edges in the graph are assumed to be true causal effects. So for example,</p>
<ul>
<li><code>X3</code> influences <code>Y</code></li>
<li><code>X5</code> influences <code>X1</code> which influences <code>Y</code></li>
<li>Some unobserved variable <code>U</code> influences both <code>X1</code> and <code>Y</code>. By unobserved, what I mean is that its some variable we don‚Äôt have data for.</li>
</ul>
<p>For those familiar with causal inference, this DAG in particular is also riddled with confounds.</p>
<hr>
<p>Ok back on track. We‚Äôll get out one of the more popular Explainable AI tools nowadays, <code>XGBoost</code>. I‚Äôm going to start in the most dangerous way possible - I‚Äôm going to toss everything in the model.</p>
</section>
<section id="test-1-whats-the-impact-of-x1-on-y" class="level1">
<h1>Test 1: What‚Äôs the impact of X1 on Y?</h1>
<p>We know for a fact that X1 influences Y. Let‚Äôs see how well Partial Dependence Plots and SHAP values do at identifying the true causal effect</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>These SHAP values arent just wrong, but the effect is in the wrong direction. The reason for this: there‚Äôs a <strong>Fork Confound.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig3.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Some variable <code>Z</code> confounds <code>X</code>s true effect on <code>Y</code>.</p>
<blockquote class="blockquote">
<p>A very common example of a fork confound is <code>warm weather (Z)</code> on the relationship between <code>ice cream sales (X)</code> and <code>crime (Y)</code>. Ice cream sales obviously have no influence on crime, but ice cream sales are higher during warmer weather, and crime is higher during warmer weather.</p>
</blockquote>
<p>So back to our main point - Explainable AI can‚Äôt get around a fork confound. This is our first lesson on why SHAP / explainable AI is different from causal inference.</p>
<p>Luckily in this case, statistics can solve this problem.</p>
<p>Using some domain knowledge about the generating process, we notice an instrument, <code>X5</code>, that can be used to estimate the causal effect of <code>X1</code> on <code>Y</code></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig4.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>I won‚Äôt go into the details of instrumental variable analysis since the goal of this article is to highlight that Explainable AI can‚Äôt replace causal inference. To learn more about it, see <a href="https://mixtape.scunning.com/instrumental-variables.html?panelset=python-code&amp;panelset1=python-code2">Scott Cunningham‚Äôs Causal Inference the Mixtape</a>.</p>
<p>But for now, I will show that a classic causal inference method succeeds where XGBoost and SHAP values fail</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><span class="im" style="color: #00769E;">from</span> linearmodels <span class="im" style="color: #00769E;">import</span> IV2SLS</span>
<span id="cb1-2"><span class="im" style="color: #00769E;">from</span> src.dagtools <span class="im" style="color: #00769E;">import</span> get_effect</span>
<span id="cb1-3"></span>
<span id="cb1-4"><span class="co" style="color: #5E5E5E;"># Instrumental variable analysis</span></span>
<span id="cb1-5">iv_model <span class="op" style="color: #5E5E5E;">=</span> IV2SLS.from_formula(<span class="st" style="color: #20794D;">"Y ~ 1 + [X1 ~ X5]"</span>, data<span class="op" style="color: #5E5E5E;">=</span>df).fit()</span>
<span id="cb1-6"></span>
<span id="cb1-7"><span class="co" style="color: #5E5E5E;"># pull true effect</span></span>
<span id="cb1-8">true_effect <span class="op" style="color: #5E5E5E;">=</span> get_effect(DAG, <span class="st" style="color: #20794D;">"X1"</span>, <span class="st" style="color: #20794D;">"Y"</span>)</span>
<span id="cb1-9"></span>
<span id="cb1-10"><span class="co" style="color: #5E5E5E;"># Plot</span></span>
<span id="cb1-11">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb1-12">ax.set_title(<span class="st" style="color: #20794D;">"Instrumental Variable Analysis</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">Recovers True effect"</span>)</span>
<span id="cb1-13">plot_model_estimate(iv_model, true_effect<span class="op" style="color: #5E5E5E;">=</span>true_effect, feat<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"X1"</span>, ax<span class="op" style="color: #5E5E5E;">=</span>ax)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig5.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>As we can see, a simple statistics technique succeeds where explainable AI fails.</p>
</section>
<section id="what-about-estimating-the-effect-of-x4-on-y" class="level1">
<h1>What about estimating the effect of X4 on Y?</h1>
<p>This relationship is slightly more complicated, but certainly measurable. <code>X4</code> influences <code>X2</code> which influences <code>Y</code>. Here‚Äôs the DAG again for reference</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_dag1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>The plots below show how well explainable AI does at estimating the causal effect of this relationship.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig6.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>Unfortunately, they don‚Äôt pick up an effect at all! And if our goal was to increase <code>Y</code> we‚Äôd end up missing a pretty good lever for it. There‚Äôs another simple explanation here for why explainable AI: there‚Äôs a <strong>Pipe Confound</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig7.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>When trying to measure the effect of <code>X -&gt; Y</code>, conditioning on <code>Z</code> (aka including it in a model as a covariate with X) ends up blocking inference.</p>
<p>For more details on how a Pipe confound works, I recommend chapters 5 and 6 of <a href="https://xcelab.net/rm/statistical-rethinking/">Richard McElreath‚Äôs Statistical Rethinking v2</a> (where I borrowed the example from as well).</p>
<p>The main things to note here are that pipes are common and Explainable AI doesn‚Äôt get around them.</p>
<p>We can recover an unbiased estimate of the true effect simply with OLS</p>
<div class="sourceCode" id="cb2" style="background: #f1f3f5;"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><span class="co" style="color: #5E5E5E;"># Fit simple OLS model</span></span>
<span id="cb2-2">model <span class="op" style="color: #5E5E5E;">=</span> sm.OLS.from_formula(<span class="st" style="color: #20794D;">"Y ~ X4"</span>, data<span class="op" style="color: #5E5E5E;">=</span>df).fit()</span>
<span id="cb2-3"></span>
<span id="cb2-4"><span class="co" style="color: #5E5E5E;"># pull true effect</span></span>
<span id="cb2-5">true_effect <span class="op" style="color: #5E5E5E;">=</span> get_effect(DAG, <span class="st" style="color: #20794D;">"X4"</span>, <span class="st" style="color: #20794D;">"X2"</span>) <span class="op" style="color: #5E5E5E;">*</span> get_effect(DAG, <span class="st" style="color: #20794D;">"X2"</span>, <span class="st" style="color: #20794D;">"Y"</span>)</span>
<span id="cb2-6"></span>
<span id="cb2-7"><span class="co" style="color: #5E5E5E;"># Plot (see notebok for plot_model_estimate function)</span></span>
<span id="cb2-8">fig, ax <span class="op" style="color: #5E5E5E;">=</span> plt.subplots(<span class="dv" style="color: #AD0000;">1</span>,<span class="dv" style="color: #AD0000;">1</span>,figsize<span class="op" style="color: #5E5E5E;">=</span>(<span class="dv" style="color: #AD0000;">6</span>,<span class="dv" style="color: #AD0000;">4</span>))</span>
<span id="cb2-9">ax.set_title(<span class="st" style="color: #20794D;">"Instrumental Variable Analysis</span><span class="ch" style="color: #20794D;">\n</span><span class="st" style="color: #20794D;">Recovers True effect"</span>)</span>
<span id="cb2-10">plot_model_estimate(model, true_effect<span class="op" style="color: #5E5E5E;">=</span>true_effect, feat<span class="op" style="color: #5E5E5E;">=</span><span class="st" style="color: #20794D;">"X4"</span>, ax<span class="op" style="color: #5E5E5E;">=</span>ax)</span></code></pre></div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_fig8.png" class="img-fluid figure-img"></p>
</figure>
</div>
</section>
<section id="can-we-use-explainable-ai-for-causal-inference-at-all" class="level1">
<h1>Can we use Explainable AI for causal inference at all?</h1>
<p>We can! We just need to be deliberate in which variables we include in our models, and the only way to do that right is to use DAGs! The example below looks at an XGBoost model that doesnt condition on <code>X2</code> (allowing us to estimate the causal effect of <code>X4 -&gt; Y</code>).</p>
<p>[](/assets/img/explainable_ai_fig9.png]{fig-align=‚Äúcenter‚Äù}</p>
</section>
<section id="take-aways" class="level1">
<h1>Take Aways</h1>
<p>Explainable AI is not some magic tool for causal inference. What these tools are good at is explaining why complicated models make the decisions they do. Explainable AI tools suffer from the same limitations for causal inference as all other statistical estimators.</p>
<p>At the end of the day when causal inference is your goal, nothing beats using DAGs to inform deliberate variable selection.</p>
<p>If you‚Äôre new to the subject, I highly recommend the following resources that will teach you how to use causal inference properly: * Chapter‚Äôs 5 and 6 of Statistical Rethinking v2, by Richard McElreath * Causal Inference for the Brave and True by Matheus Facure * Causal Inference the Mixtape, by Scott Cunningham</p>


</section>

 ]]></description>
  <guid>https://github.com/kylejcaron/kylejcaron.github.io/posts/2022-04-03-explainable-ai-is-not-causal.html</guid>
  <pubDate>Sun, 03 Apr 2022 04:00:00 GMT</pubDate>
  <media:content url="https://github.com/kylejcaron/kylejcaron.github.io/assets/img/explainable_ai_dag1.png" medium="image" type="image/png" height="101" width="144"/>
</item>
</channel>
</rss>
