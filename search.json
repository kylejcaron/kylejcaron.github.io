[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kyle Caron",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDouble ML in Numpyro using scope\n\n\n\n\n\n\ncausal inference\n\n\nnumpyro\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n9 min\n\n\n\n\n\n\n\nAn easy way to choose evaluation metrics\n\n\n\n\n\n\nmetrics\n\n\nmodel evaluation\n\n\ntime series\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n4 min\n\n\n\n\n\n\n\nCommon Misconceptions with Multicollinearity\n\n\n\n\n\n\nmulticollinearity\n\n\ncausal inference\n\n\nregression\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n4 min\n\n\n\n\n\n\n\nAutomatic Dim Labelling with Numpyro?\n\n\n\n\n\n\nnumpyro\n\n\ntensors\n\n\nArviZ\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\n33 min\n\n\n\n\n\n\n\nPandera and Object Oriented Data Validation\n\n\n\n\n\n\nobject oriented programming\n\n\ndata processing\n\n\n\n\n\n\n\n\n\nFeb 21, 2025\n\n\n7 min\n\n\n\n\n\n\n\nModeling Anything With First Principles: Demand under extreme stockouts\n\n\n\n\n\n\nTime Series\n\n\nDemand Modeling\n\n\nCausal Inference\n\n\nSupply Chain\n\n\nDiscrete Choice\n\n\nSurvival Analysis\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n32 min\n\n\n\n\n\n\n\nIntroduction to Surrogate Indexes\n\n\n\n\n\n\nexperimentation\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n11 min\n\n\n\n\n\n\n\nDesiging an Experimentation Strategy\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n16 min\n\n\n\n\n\n\n\nUseful Tools for Weibull Survival Analysis\n\n\n\n\n\n\nsurvival analysis\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n13 min\n\n\n\n\n\n\n\nWhy do we need A/B tests? The Potential Outcomes Model\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n10 min\n\n\n\n\n\n\n\nMaking out of sample predictions with PyMC\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n6 min\n\n\n\n\n\n\n\nHow long should you run an A/B test for?\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n4 min\n\n\n\n\n\n\n\nUncertainty Intervals or p-values?\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n7 min\n\n\n\n\n\n\n\nExplainable AI is not Causal Inference\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Double ML in Numpyro using scope\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n\n\n\n\n\nAn easy way to choose evaluation metrics\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n\n\n\n\n\nCommon Misconceptions with Multicollinearity\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n\n\n\n\n\nAutomatic Dim Labelling with Numpyro?\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\n\n\n\n\n\nPandera and Object Oriented Data Validation\n\n\n\n\n\n\n\n\nFeb 21, 2025\n\n\n\n\n\n\n\nModeling Anything With First Principles: Demand under extreme stockouts\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\nIntroduction to Surrogate Indexes\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n\n\n\n\n\nDesiging an Experimentation Strategy\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n\n\n\n\n\nUseful Tools for Weibull Survival Analysis\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n\n\n\n\n\nWhy do we need A/B tests? The Potential Outcomes Model\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\nMaking out of sample predictions with PyMC\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n\n\n\n\n\nHow long should you run an A/B test for?\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n\n\n\n\n\nUncertainty Intervals or p-values?\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n\nExplainable AI is not Causal Inference\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-04-01-scope-double-ml.html",
    "href": "posts/2025-04-01-scope-double-ml.html",
    "title": "Double ML in Numpyro using scope",
    "section": "",
    "text": "Background\nThis is a more of a tutorial for using numpyro’s scope handler. It’s fairly straightforward and allows one to use a composable model framework in numpyro - ie calling multiple models within a model.\nThe code fold below just has some imports and helper functions\n\n\nCode\nfrom abc import abstractmethod, ABC\nfrom typing_extensions import Self\nfrom typing import Any, Optional, Dict\n\nimport numpyro\nnumpyro.set_host_device_count(4)\n\nimport numpy as np\nimport scipy.special as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport jax.numpy as jnp\nimport jax.scipy.special as jsp\nfrom jax import random\nfrom numpyro import distributions as dist\nfrom numpyro.handlers import scope\nfrom numpyro.infer import MCMC, NUTS\n\nSEED = 99\n\nclass NumpyroModel(ABC):\n\n    def __init__(self):\n        self._fitted = None\n        self.samples = None\n        self.inference_obj = None\n\n\n    @abstractmethod\n    def __call__(self, *args, **kwargs):\n        raise NotImplemented\n\n    @property\n    def params(self):\n        assert self._fitted\n        return self.samples\n\n    def fit(\n        self,\n        *args,\n        num_chains: int = 2,\n        num_samples: int = 2000,\n        num_warmup: int = 1000,\n        seed: int = None,\n        inference_kwargs: Optional[Dict[str, Any]] = None,\n        **kwargs,\n    ) -&gt; Self:\n\n        inference_kwargs = {} if inference_kwargs is None else inference_kwargs\n        rng_key = random.PRNGKey(seed or np.random.choice(10000))\n        kernel = NUTS(self, **inference_kwargs)\n        mcmc = MCMC(\n            sampler=kernel,\n            num_chains=num_chains,\n            num_warmup=num_warmup,\n            num_samples=num_samples,\n        )\n        mcmc.run(rng_key, *args, **kwargs)\n\n        # extract posterior and save results internally\n        self.samples = mcmc.get_samples()\n        self.inference_obj = mcmc\n        self._fitted = True\n        return self\n\n\n\n\nHow does scope work?\nWe’ll simulate a dataset and fit it with scope to show how it works\n\nrng = np.random.default_rng(SEED)\nN = 1000\nX = rng.normal(0, 3, size=(N,1))\nALPHA, BETA = np.array([1.2]), np.array([0.4])\ny = ALPHA + np.dot(X, BETA) + rng.normal(size=N)\nplt.scatter(X[:,0], y)\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()\n\n\n\n\n\n\n\n\nNow lets define a simple linear model\n\nclass Linear(NumpyroModel):\n\n    def __call__(self, X, y=None):\n        alpha = numpyro.sample(\"alpha\", dist.Normal(0, 1))\n        sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n        with numpyro.plate(\"features\", X.shape[-1]):\n            beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n\n        with numpyro.plate(\"obs_id\", X.shape[0]):\n            mu = numpyro.deterministic(\"mu\", alpha + jnp.dot(X, beta))\n            y = numpyro.sample(\"obs\", dist.Normal(mu, sigma), obs=y)\n            return mu, y\n\n\nFitting a model using scope\nWe could just fit a model with the linear model above, but to illustrate how scope works, we’ll use the linear model as a sub-module in another model\nNote, scope has no use in this example, this is just to illustrate how it works\n\nclass ScopeExampleModel(NumpyroModel):\n\n    def __call__(self, X, y=None):\n        model = scope(Linear(), prefix=\"M\", divider='.')\n        return model(X, y=y)\n\n\nm1 = Linear().fit(X=X, y=y, seed=SEED)\nm2 = ScopeExampleModel().fit(X=X, y=y, seed=SEED)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow lets look at the fitted parameters (theyre the same but scope has a prefix)\nIf we peek at both posteriors below, we see that the results are identical, but the site names are different - Model 2 (that used scope), has a prefix M.* for each site. Thats all scope does\nThis is an important feature that lets you call multiple models within your numpyro model. Remember, that models with duplicate site names will fail. This allows us to call the same model multiple times in a different model\n\n# looks at first 3 samples for each parameter\ndef peek(dct, n=3):\n    return {k:v[:n].ravel() for k,v in dct.items() if k[-2:]!='mu'}\n\npeek( m1.params )\n\n{'alpha': Array([1.2196213, 1.1488996, 1.1982138], dtype=float32),\n 'beta': Array([0.41865802, 0.4032275 , 0.40824327], dtype=float32),\n 'sigma': Array([0.9782854, 0.9691421, 1.0134417], dtype=float32)}\n\n\n\n\n\n\nDouble ML example\nThis is the fun part. We’ll use Double Machine Learning as an example of why scope is so useful.\n\nDouble ML background\nFor some quick background, Double ML is a procedure for estimating unbiased Average Treatment Effects (ATE) where \\(A\\) is the treatment, \\(y\\) is the outcome, \\(X\\) are covariates, and * represents residuals, ie \\(A^* = (A-\\hat{A})\\)\n\nModel 1: Propensity Model, \\(E[A|X]\\): Debiases the treatment with propensity scores\nModel 2: Outcome Model \\(E[y|X]\\): Denoises the Outcome\nModel 3: Final Model \\(E[y^*|A^*]\\): Estimates the ATE as \\(\\beta\\) in a Linear Regression on the residuals of the outcome and treatment\n\n\n\nSimulate some data with a biased treatment\n\n\nCode\n# simulate a biased treatment\n# X -&gt; A -&gt; y, X -&gt; y\n\n# treatment effect\ntau = 0.4 \n\nX = rng.normal(0,1.5, size=(N,3))\nb = rng.normal(0, 0.5, size=3)\npA = sp.expit(0.25 + np.dot(X, b))\nA = rng.binomial(1, pA)\n\nb_y = rng.normal(0, 0.5,size=3)\n\nmu_y = -1.2 + tau*A + np.dot(X, b_y)\ny = rng.normal(mu_y, 0.2)\n\n\n# Show the data as a dataframe\ndf = (\n    pd.DataFrame(X, columns=[f\"X{i}\" for i in range(X.shape[1])])\n    .assign(A=A)\n    .assign(y=y)\n)\ndf.head()\n\n\n\n\n\n\n\n\n\nX0\nX1\nX2\nA\ny\n\n\n\n\n0\n0.301738\n0.587291\n0.837701\n1\n-0.381980\n\n\n1\n2.883451\n-0.783693\n1.644344\n0\n-0.544268\n\n\n2\n-2.214088\n1.222192\n0.332317\n1\n-0.983728\n\n\n3\n0.909440\n1.710911\n3.296030\n0\n0.685601\n\n\n4\n1.826930\n-1.149363\n-1.683725\n1\n-0.996186\n\n\n\n\n\n\n\n\n\nNext lets make a Logit model\n\nclass Logit(NumpyroModel):\n\n    def __call__(self, X, y=None):\n        alpha = numpyro.sample(\"alpha\", dist.Normal(0, 2.5))\n        with numpyro.plate(\"features\", X.shape[-1]):\n            beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n\n        with numpyro.plate(\"obs_id\", X.shape[0]):\n            eta =  numpyro.deterministic(\"eta\", alpha + jnp.dot(X, beta))\n            mu = numpyro.deterministic(\"mu\", jsp.expit(eta))\n            y = numpyro.sample(\"obs\", dist.Bernoulli(mu), obs=y)\n            return mu, y\n\n\n\nFinally, use scope to fit multiple models within 1 numpyro model\nNotice how simple Scope allows the __call__ function to be. We can also plug and play any models we want including non-parametric models for the debias/denoise models of Double ML\n\nclass DoubleML(NumpyroModel):\n    def __init__(\n        self,\n        propensity_model: NumpyroModel = None,\n        outcome_model: NumpyroModel = None,\n        **kwargs,\n    ):\n        self.debias_model = scope(propensity_model, prefix='Mt', divider='.')\n        self.denoise_model = scope(outcome_model, prefix='My', divider='.')\n        self.ate_model = scope(Linear(), prefix=\"M\", divider='.')\n        super().__init__(**kwargs)\n\n    # Look how simple our model is\n    def __call__(self, X, A, y=None):\n\n        mu_A, A = self.debias_model(X=X, y=A) # estimate treatment E[A|X]\n        mu_y, y = self.denoise_model(X=X, y=y) # denoise outcome E[y|X]\n\n        # calculate residuals to debias and denoise\n        A_star = (A - mu_A)\n        y_star = (y - mu_y)\n\n        # run linear regression on residuals to estimate ATE\n        return self.ate_model(X=A_star[:,None], y=y_star)\n    \n    @property\n    def ate(self):\n        return self.params['M.beta'].ravel()\n\n\ndml = DoubleML(\n    propensity_model=Logit(), \n    outcome_model=Linear()\n).fit(X, A, y, seed=SEED)\n\n\n\n\n\n\n\n\n\nPulling the average treatment effect\nFrom the Double ML literature, its clear that the \\(\\beta\\) estimate of the final model is the unbiased average treatment effect, so all we have to do is pull that from the model posterior. We just have to make sure to reference the prefix, M. first\n\n# Plot the parameter recovery\nsns.histplot(dml.params['M.beta'], element='step', label='Estimate')\nplt.axvline(tau, color='r', ls='--', label='Ground Truth')\nplt.legend()\nplt.xlabel(\"ATE\")\nplt.title(\"Estimated ATE vs. Actual\")\n\nText(0.5, 1.0, 'Estimated ATE vs. Actual')\n\n\n\n\n\n\n\n\n\n\n\nWhat happens without scope?\nNote, without scope this would have failed because there would be multipled sites with the same param names, since both Linear() and Logit() models have sites named alpha, beta, etc\n\nimport traceback\n\n# We'll just change the __init__ to not use scope\n# but we'll keep everything else the same\nclass IncorrectDoubleML(DoubleML):\n    \n    def __init__(\n        self,\n        propensity_model: NumpyroModel = None,\n        outcome_model: NumpyroModel = None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.debias_model = propensity_model\n        self.denoise_model = outcome_model\n        self.ate_model = Linear()\n\n\ntry:\n    dml = IncorrectDoubleML(\n        propensity_model=Logit(), \n        outcome_model=Linear()\n    ).fit(X, A, y)\nexcept Exception as e:\n    traceback.print_exc()\n\nTraceback (most recent call last):\n  File \"/var/folders/1p/v01fvg3j1cz1hzv988ygj8m00000gn/T/ipykernel_23623/1941668504.py\", line 23, in &lt;module&gt;\n    ).fit(X, A, y)\n  File \"/var/folders/1p/v01fvg3j1cz1hzv988ygj8m00000gn/T/ipykernel_23623/3488910688.py\", line 60, in fit\n    mcmc.run(rng_key, *args, **kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/infer/mcmc.py\", line 640, in run\n    states, last_state = pmap(partial_map_fn)(map_args)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 179, in reraise_with_filtered_traceback\n    return fun(*args, **kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/api.py\", line 1804, in cache_miss\n    execute = pxla.xla_pmap_impl_lazy(fun_, *tracers, **params)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py\", line 285, in xla_pmap_impl_lazy\n    compiled_fun, fingerprint = parallel_callable(\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 349, in memoized_fun\n    ans = call(fun, *args)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py\", line 570, in parallel_callable\n    pmap_computation = lower_parallel_callable(\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/profiler.py\", line 336, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py\", line 727, in lower_parallel_callable\n    jaxpr, consts, replicas, shards = stage_parallel_callable(pci, fun)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/interpreters/pxla.py\", line 659, in stage_parallel_callable\n    jaxpr, out_sharded_avals, consts = pe.trace_to_jaxpr_final(\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/profiler.py\", line 336, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2392, in trace_to_jaxpr_final\n    jaxpr, out_avals, consts, () = trace_to_subjaxpr_dynamic(\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2336, in trace_to_subjaxpr_dynamic\n    ans = fun.call_wrapped(*in_tracers_)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 191, in call_wrapped\n    ans = self.f(*args, **dict(self.params, **kwargs))\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/infer/mcmc.py\", line 416, in _single_chain_mcmc\n    new_init_state = self.sampler.init(\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/infer/hmc.py\", line 713, in init\n    init_params = self._init_state(\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/infer/hmc.py\", line 657, in _init_state\n    ) = initialize_model(\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/infer/util.py\", line 656, in initialize_model\n    ) = _get_model_transforms(substituted_model, model_args, model_kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/infer/util.py\", line 450, in _get_model_transforms\n    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/handlers.py\", line 171, in get_trace\n    self(*args, **kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/primitives.py\", line 105, in __call__\n    return self.fn(*args, **kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/primitives.py\", line 105, in __call__\n    return self.fn(*args, **kwargs)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/primitives.py\", line 105, in __call__\n    return self.fn(*args, **kwargs)\n  File \"/var/folders/1p/v01fvg3j1cz1hzv988ygj8m00000gn/T/ipykernel_23623/1207791297.py\", line 17, in __call__\n    mu_y, y = self.denoise_model(X=X, y=y) # denoise outcome E[y|X]\n  File \"/var/folders/1p/v01fvg3j1cz1hzv988ygj8m00000gn/T/ipykernel_23623/3665807378.py\", line 4, in __call__\n    alpha = numpyro.sample(\"alpha\", dist.Normal(0, 1))\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/primitives.py\", line 222, in sample\n    msg = apply_stack(initial_msg)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/primitives.py\", line 59, in apply_stack\n    handler.postprocess_message(msg)\n  File \"/Users/kylecaron/Desktop/kylejcaron.github.io/.venv/lib/python3.10/site-packages/numpyro/handlers.py\", line 156, in postprocess_message\n    assert not (\nAssertionError: all sites must have unique names but got `alpha` duplicated\n\n\n\n\n\nConclusion\nThats about it. Scope is a great tool for extending numpyro and lets us start to stack and compose models modularly. This makes Double ML and other causal model implementations incredible easy to read and extend."
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "",
    "text": "The Problem: You work at a rental service that is suffering from high periods of churn. You’ve found that stockouts are one of the biggest reasons for churn - despite having plenty of products, they tend to only be available 60-80% of the time. How can we determine how much stock to reorder?\nThe idea of this project is to borrow ideas from first principles modeling - think about the data generating process and model each step of it. This example borrows ideas from discrete choice literature, survival analysis, demand forecasting, and simulation.\nProducts with extreme stockouts have misleading demand estimates under classic demand models.\n\nThe basis of these ideas is really simple. If we only have 5 units of a product in stock, and we saw 5 sales (or rentals in this case) that day, then we know that the observed demand would’ve been atleast 5 rentals if stock levels didn’t constrain it.\n\nTypical demand modeling might only look at the count of rentals each day, while censored demand modeling would look at both rental counts and stock levels, and incoporate both of these pieces of information.\nThe idea is to start simple(-ish) and add complexity. We’ll first model a single product and test different ordering policies, and then begin modeling multiple products.\nThe original code including the full simulation of the dataset can be found here."
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html#plotting-product-rentals",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html#plotting-product-rentals",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "Plotting Product Rentals",
    "text": "Plotting Product Rentals\nLets convert the rental data to time series data and see what it looks like\n\n\nCode\ndef rental_events_to_ts(rentals, freq='D'):\n    return (rentals\n            .groupby(\"product_id\")\n            .resample(freq,on='date')[['rental_id']].count()\n            .set_axis(['rentals'],axis=1)\n           )\n\ndaily_rentals = rentals.pipe(rental_events_to_ts, freq='D')\n\nfig, ax = plt.subplots(2,1,sharex=True)\n\nax[0].set(ylabel='Total Daily Rentals', xlabel='Date', title='Total Daily Rentals')\ndaily_rentals.groupby(\"date\").sum().rentals.plot(ax=ax[0])\n\nax[1].set(ylabel='Daily Rentals', xlabel='Date', title='Daily Rentals Across Products')\nsample_idxs = np.random.choice(J_PRODUCTS, size=25, replace=False)\ndaily_rentals.unstack(0).iloc[:,sample_idxs].plot(ax=ax[1], alpha=0.5,legend=None)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nIt looks like there are typically 0-15 rentals per day for a given product, and there are typically 5000 rentals total each day.\nLooking at the distribution of total rentals across products, we see that it is mildly long-tailed.\n\n\nCode\nfig, ax = plt.subplots(1,1,figsize=(5,3),sharex=True)\nax.set_xlabel(\"Rentals\",fontsize=10)\nax.set_ylabel(\"Count\",fontsize=10)\nax.set_title('Distribution of total rentals\\nper product in observation period', fontsize=10)\nsns.histplot( daily_rentals.groupby(\"product_id\").sum(),ax=ax )\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can also see that stockouts are extremely prevalent - on a given day, up to 85% of products could be stocked out. A recent reorder in June helped a little but not enough. How can we reduce the stockout problem and reorder accordingly?\n\n\nCode\nfig, ax = plt.subplots(1,1)\n(stock\n .assign(stockout_rate=lambda d: d.ending_units==0)\n .groupby(\"date\").stockout_rate.mean()\n .plot(ylabel='Stocked Out Products',title='Percent of products with a Stockout each day'))\n\nplt.show()"
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html#looking-at-a-single-product",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html#looking-at-a-single-product",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "Looking at a single product",
    "text": "Looking at a single product\nWe’re going to start with a single product and see if we can come up with a system that informs us how many units we should reorder.\n\n\nCode\nj=165\nreorder_date = (\n    purchases\n    .query(f\"product_id=='product_{j}'\")\n    .query(\"order_type=='reorder'\")\n    .date.min()\n)\n\nfig, ax = plt.subplot_mosaic(\n    \"\"\"\n    AAA\n    AAA\n    BBB\n    CCC\n    \"\"\",sharex=True)\n\nax['A'].set_title(f\"Daily Rentals for Product ID {j}\")\ndaily_rentals.loc[f'product_{j}'].plot(ax=ax['A'], color='k',ylabel='Rentals')\ndaily_rentals.groupby(\"date\").sum().rentals.rename(\"total_rentals\").plot(ax=ax['B'],ylabel='Total Rentals')\nax['B'].set_ylabel(\"Total Rentals\", fontsize=12)\n\n(stock\n .assign(stockout_rate=lambda d: d.ending_units==0)\n .groupby(\"date\").stockout_rate.mean()\n .plot(ax=ax['C']))\nax['C'].set_ylabel(\"Stockout Rate\", fontsize=12)\nax['C'].set_ylim(0,1)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAs shown above, just looking at a time series of rentals each day is a bit confusing - there are seemingly random spikes in demand for just this product that dont seem to line up with spikes in total demand across products.\nWhen we look at the stockout rate across all products over time, it looks like the stockout rate drops at the same time demand increases for Product ID 165.\nLet’s join stock data to rental data and see whats happening for this product.\n\n\nCode\ndaily_rentals = (rentals.pipe(rental_events_to_ts, freq='D')\n                 .merge(stock, on=['product_id', 'date'])\n                 .assign(stockout=lambda d: 1*(d.ending_units==0))\n                 .assign(total_stockout_rate=lambda d: d.groupby(\"date\").stockout.transform(\"mean\")) \n                 .assign(total_rentals=lambda d: d.groupby(\"date\").rentals.transform(\"sum\"))\n                 .set_index(['product_id', \"date\"])\n                )\n\ndaily_rentals.loc[[f'product_{j}']].head()\n\n\n\n\n\n\n\n\n\n\nrentals\nstarting_units\nending_units\nstockout\ntotal_stockout_rate\ntotal_rentals\n\n\nproduct_id\ndate\n\n\n\n\n\n\n\n\n\n\nproduct_165\n2022-04-01\n3\n80.0\n77.0\n0\n0.001122\n5014\n\n\n2022-04-02\n2\n77.0\n75.0\n0\n0.003125\n4887\n\n\n2022-04-03\n5\n75.0\n70.0\n0\n0.011190\n5058\n\n\n2022-04-04\n6\n70.0\n64.0\n0\n0.023209\n5040\n\n\n2022-04-05\n4\n64.0\n60.0\n0\n0.032193\n4890\n\n\n\n\n\n\n\nNow that we have this data joined, we can plot the rental data with corresponding stock data.\n\n\nCode\nax = plot_rentals(\n    daily_rentals = daily_rentals.loc[f'product_{j}'].rentals, \n    daily_stock = daily_rentals.loc[f'product_{j}'].starting_units,\n)\n\nax.axvline(reorder_date, color='k', ls='--', label='Reorder: new units added to inventory')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nFor most of the days in the product’s lifetime, there are as many rentals as there are units available. This indicates we’re probably understocked - if we had more stock available, we’d likely observe more rentals.\nThis is also apparent in the overstocked periods - at the start of the product’s lifetime there tends to be more rentals. There’s also a reorder that happens in June, where the business procured new units of that product to rent out to customers. We can see that there’s a jump in observed rentals\nIt’s time to introduce a key concept - the difference between rental demand and observed rentals:\n\nRental demand is the true demand to rent the product each day.\nObserved rentals are the number of rentals we actually observe in the data, after random noise is added to demand and it gets constrained by stock levels."
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html#rental-duration-model",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html#rental-duration-model",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "Rental Duration Model",
    "text": "Rental Duration Model\n\nThe plan for now is to model the unconstrained demand for this single product - whats the actual rental demand for the product? How many rentals would it get if there weren’t stockouts? Lets start off by looking at our data.\n\nPreparing the data\nIt’s easy to see that there are plenty of rentals that are still out with customers and haven’t been returned yet. For these unreturned rentals, we can’t calculate an accurate rental duration.\n\n\nCode\nrentals.sample(10, random_state=SEED)\n\n\n\n\n\n\n\n\n\nrental_id\ndate\nreturn_date\nproduct_id\n\n\n\n\n336567\n9a0c22af-d234-4967-af33-904dc2063f07\n2022-05-05\n2022-05-30\nproduct_661\n\n\n372791\n781e6d0a-c94c-4a34-8447-5dee6002248e\n2022-05-28\nNaN\nproduct_739\n\n\n316499\n9c1cce54-62df-45e2-9a33-3cf1ee3056e7\n2022-06-15\nNaN\nproduct_622\n\n\n235191\n4ce955dd-773c-443c-ae28-d4ce018686e2\n2022-06-19\n2022-06-29\nproduct_459\n\n\n299770\n842f7574-3784-41f8-a627-29cf7d622b9e\n2022-06-04\n2022-06-23\nproduct_591\n\n\n39846\n366afbe0-805b-46ac-a25b-018aa35367f7\n2022-05-30\nNaN\nproduct_78\n\n\n395599\ndd13bd67-3d1e-464a-8cb9-c2205e050815\n2022-04-09\n2022-04-19\nproduct_787\n\n\n70230\n0193f7c4-a229-41a4-8872-3277229980a9\n2022-04-16\n2022-05-01\nproduct_139\n\n\n449004\n4b7c426e-ac62-4467-8b95-0397f8aff0ce\n2022-07-06\nNaN\nproduct_895\n\n\n90329\n632f806f-c782-4680-8464-f3ffafe4a9f5\n2022-05-16\n2022-06-09\nproduct_178\n\n\n\n\n\n\n\nIt’s important to remember that even though these items aren’t returned, some have been rented out for 30, 40, 50 days, and it is important information to know that some items have been out for at least that long. A good example is that if it is possible for some items to be rented out for 200 days, but we’ve only observed 100 days of activity, then whatever we estimate for the rental duration would be underestimated if we just used basic averaging.\nWhat we can instead do is calculate the rental duration to date and then use a survival model to properly incorporate those unreturned items.\n\ncurr_date = rentals.date.max()\nrentals = (\n    rentals\n    .assign(return_date=lambda d: pd.to_datetime(d.return_date))\n    .assign(time_since=lambda d: (d.return_date.fillna(curr_date)-d.date).dt.days)\n    .assign(event=lambda d: d.return_date.notnull()*1)\n)\nrentals.sample(5)\n\n\n\n\n\n\n\n\nrental_id\ndate\nreturn_date\nproduct_id\ntime_since\nevent\n\n\n\n\n206163\nb2da4df7-066b-4a2d-940b-0117535b7c90\n2022-05-31\n2022-06-05\nproduct_395\n5\n1\n\n\n167082\n5ed20edf-fa98-4d6f-8b02-a558db5cb19f\n2022-04-29\n2022-05-12\nproduct_321\n13\n1\n\n\n159965\n765bdbb8-36df-46ac-ab0b-43b7a880e7d0\n2022-06-02\n2022-06-09\nproduct_306\n7\n1\n\n\n90327\n06bb1c49-d794-470b-ba84-8b5e01bd32cd\n2022-05-16\n2022-05-25\nproduct_178\n9\n1\n\n\n437958\ndfd13840-3011-46e0-9b4c-b88820366c0c\n2022-06-27\nNaT\nproduct_869\n12\n0\n\n\n\n\n\n\n\n\n\nDefining and fitting the model\nWe can write up a survival model in numpyro. The idea of survival analysis is that:\n\nif a return has already occurred, we fit the model as usual with the observed rental duration.\nIf a return hasn’t occurred yet, we tell the model that the rental duration is at least as long as has been observed so far. This is done with stats via a survival function, or the complementary CDF (ccdf)\n\nFor simplicity, we’re assuming the data is lognormally distributed (and we simulated the data that way). In reality, it is best to plot distributions of your data and decide for yourself.\n\\[\n\\small{\n\\displaylines{\n\\text{rental duration} \\sim \\text{Lognormal}(\\theta, \\sigma)\n}}\n\\]\n\ndef censored_lognormal(theta, sigma, cens, y=None):\n    # If observed, this is the likelihood contribution\n    numpyro.sample(\"obs\", dist.LogNormal(theta, sigma).mask(cens != 1), obs=y)\n\n    # If not observed, use the survival function as the likelihood constribution\n    ccdf = numpyro.deterministic(\"ccdf\", 1 - dist.LogNormal(theta, sigma).cdf(y))\n    numpyro.sample(\"censored_label\", dist.Bernoulli(ccdf).mask(cens == 1), obs=cens)\n\ndef survival_model(E, T=None):\n    theta = numpyro.sample(\"theta\", dist.Normal(2.9, 1))\n    sigma = numpyro.sample(\"sigma\", dist.Exponential(0.7))\n    with numpyro.plate(\"data\", len(E)):\n        censored_lognormal(theta, sigma, cens=(1-E), y=T)\n\nWe can now use numpyro to fit a model and estimate the typical rental duration.\n\nE = rentals.query(f\"product_id=='product_{j}'\").query(\"time_since&gt;0\").event.values\nT = rentals.query(f\"product_id=='product_{j}'\").query(\"time_since&gt;0\").time_since.values\n\nkernel = numpyro.infer.NUTS(survival_model)\nmcmc = numpyro.infer.MCMC(\n    kernel,\n    num_warmup=1000,\n    num_samples=1000,\n    num_chains=4,\n    progress_bar=False,\n)\nmcmc.run(random.PRNGKey(SEED),E=E,T=T)\nidata = mcmc.get_samples()\n\nRemember, this is simulated data so we know the truth and can use that as a way to test this is working - a correctly specified model should recover the the true parameter values.\n\n\nCode\nfig, ax = plt.subplots(1,2)\nsns.histplot( idata['theta'],ax=ax[0] )\nax[0].axvline(2.9,color='k', ls='--', label='Ground Truth')\nax[0].set(title=\"Theta Estimate\", xlabel='Value')\nax[0].legend()\n\nsns.histplot( idata['sigma'],ax=ax[1] )\nax[1].axvline(0.7,color='k', ls='--', label='Ground Truth')\nax[1].set(title=\"Sigma Estimate\", xlabel='Value')\nax[1].legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(1,1, figsize=(7,4))\n\nbins = np.arange(0,200,4)\nsns.histplot(T, stat='density', label='Observed Distribution', bins=bins,ax=ax)\n\nsns.histplot(\n    dist.LogNormal(idata['theta'], idata['sigma']).sample(random.PRNGKey(SEED)),\n    stat='density',\n    label='Estimated Distribution',\n    bins=bins,\n    ax=ax\n)\nax.set(title=\"Estimated Rental Duration\", xlabel='Rental Duration')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nGreat, now we have a rental duration model fit, we just need to estimate demand."
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html#rental-demand-model",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html#rental-demand-model",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "Rental Demand Model",
    "text": "Rental Demand Model\n\n\nLooking at the demand data\nThe rental demand model is going to start simple for this single-product case - we’ll just estimate a poisson distribuion.\nTaking a look at the data again, it is clear that demand is constrained, or censored, by stockouts here.\n\n\nCode\nax = plot_rentals(\n    daily_rentals = daily_rentals.loc[f'product_{j}'].rentals, \n    daily_stock = daily_rentals.loc[f'product_{j}'].starting_units\n)\n\nax.axvline(reorder_date, color='k', ls='--', label='Reorder: new units added to inventory')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nTo confirm this is the case, we can take a look at the typical number of rentals when there is a stockout vs. when there isn’t - observed rentals are lower on days when there are stockouts - the sample size of stocked out periods is also smaller.\n\n\nCode\n(\n    daily_rentals\n    .loc[f'product_{j}']\n    .groupby(\"stockout\")\n    .rentals.agg(['mean', 'count'])\n    .round(2)\n)\n\n\n\n\n\n\n\n\n\nmean\ncount\n\n\nstockout\n\n\n\n\n\n\n0\n6.65\n26\n\n\n1\n4.31\n74\n\n\n\n\n\n\n\n\n\nDefining and fitting the model\nWe can leverage survival analysis again to estimate demand. We define a censored poisson model below that assumes demand is constant over time and if there’s a stockout, the model thinks that rentals might’ve been higher than observed if there was more stock, otherwise if there’s no stockout it thinks that demand is the number of rentals that day (plus some noise of course)\n\\[\n\\small{\n\\displaylines{\n\\text{log}(\\lambda) \\sim \\alpha + \\beta X \\\\\n\\text{rentals} = \\text{min}(\\text{stock}, \\text{Poisson}(\\lambda))\n}}\n\\]\n\ndef censored_poisson(lambd, cens, y=None):\n    # If observed, this is the likelihood contribution\n    numpyro.sample(\"obs\", dist.Poisson(lambd).mask(cens != 1), obs=y)\n\n    # If not observed, use the survival function as the likelihood constribution\n    ccdf = 1 - dist.Poisson(lambd).cdf(y)\n    pmf = jnp.exp(dist.Poisson(lambd).log_prob(y)) # need to include the pmf for discrete distributions\n    numpyro.sample(\"censored_label\", dist.Bernoulli(ccdf+pmf).mask(cens == 1), obs=cens)\n\ndef demand_model(stockout, X, y=None):\n\n    # parameters\n    alpha = numpyro.sample(\"alpha\", dist.Normal(2, 2))\n    beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n\n    with numpyro.plate(\"data\", len(stockout)):\n\n        # regression\n        log_lambd = numpyro.deterministic(\n            \"log_lambd\", \n            # base demand\n            alpha \n            # covariate influence on demand - in this case cross product effect\n            # as more products go out of stock, demand for the remaining in-stock products increases\n            + jnp.dot( X, beta ) \n        )\n\n        # demand as a rental rate per day\n        lambd = numpyro.deterministic(\"lambd\", jnp.exp(log_lambd))\n\n        # Observational model\n        censored_poisson(lambd, cens=stockout, y=y)\n\nWe can then fit this model with numpyro below:\n\ny = daily_rentals.loc[f'product_{j}'].rentals.values\nstockout = daily_rentals.loc[f'product_{j}'].stockout.values\nX = daily_rentals.loc[f'product_{j}'].total_stockout_rate.values\n\nkernel = numpyro.infer.NUTS(demand_model)\nmcmc = numpyro.infer.MCMC(\n    kernel,\n    num_warmup=1000,\n    num_samples=1000,\n    num_chains=4,\n    progress_bar=False,\n)\nmcmc.run(random.PRNGKey(SEED),stockout=stockout, X=X, y=y)\nidata_demand = mcmc.get_samples()\n\nWe can check the estimated rental rate parameter, \\(\\lambda\\), against the actual value that we simulated and find that our model does have coverage over the ground truth.\n\n\nCode\nparams = (\n    pd.read_csv(f\"{BASE_URL}/data/true_params.csv\")\n    .assign(demand=lambda d: jax.nn.softmax(d.utility.values)*5000)\n    .query(f\"product_id=='product_{j}'\")\n)\ndemand_param = params.loc[j,'demand']\n\n# Plot\nfig, ax = plt.subplots(figsize=(5,3))\nsns.histplot( np.exp(idata_demand['alpha']),ax=ax )\nax.axvline(demand_param, ls='--', color='k', label='Ground Truth')\nax.legend()\nax.set_xlabel(\"Value\",fontsize=10)\nax.set_ylabel(\"Count\",fontsize=10)\nax.set_title(\"Lambda estimate\", fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\nNote that this is the base level of demand for the product - as competing products stock out, we expect \\(\\lambda\\) to increase. Currently, that is represented by \\(\\beta\\) in the model above, which is the relationship of the products demand to global stockout rate. As global stockout rate climbs, \\(\\lambda\\) increases. We can roughly estimate lambda at differen’t stockout levels.\n\n\nCode\n# Plot\nfig, ax = plt.subplots(figsize=(5,3))\nglobal_stockout_rate=0.6\nbX = idata_demand['beta']*global_stockout_rate\nsns.histplot( np.exp(idata_demand['alpha'] + bX),ax=ax )\nax.set_xlabel(\"Value\",fontsize=10)\nax.set_ylabel(\"Count\",fontsize=10)\nax.set_title(\"Lambda estimate\\nwhen 60% of products are stocked out\", fontsize=10)\nax.set_xlim(0,20)\nplt.show()\n\n\n\n\n\n\n\n\n\nNow that we have a rental duration model and a rental demand model, lets build a simulation."
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html#the-simulation-code",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html#the-simulation-code",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "The simulation code",
    "text": "The simulation code\nWe can leverage numpyro to simulate this. They have a scan operator that iterates day over day which fits well with this problem. Here’s the initial simulation model structure.\nWe’ll start by building a base class, RentalInventory that can be inherited by other classes. If you look at the model method, it takes in an initial state, and iterates over the model_single_day method each day we tell it to.\nThe model_single_day method simulates returns and rentals each day, and logs them.\nclass RentalInventory:\n    \"\"\"A model of rental inventory, modeling stock levels as returns and rentals occur each day.\n    Currently supports a single product\n    \"\"\"\n    def __init__(self, n_products: int = 1, policies: np.ndarray = None):\n        self.n_products = n_products\n        self.policies = policies if policies is not None else jnp.zeros((n_products, 10000))\n        # Rentals that are out with customers are stored as an array, where the index corresponds with time, \n        # and the value corresponds with the number of rentals from that time that are still out with customers\n        # max_periods is the total number of periods to log\n        self.max_periods = 10000\n\n    def model(self, init_state: Dict, start_time: int, end_time: int) -&gt; jnp.array:\n        \"\"\"The Rental Inventory model. Each day returns occur as represented by a lognormal time to event distribution,\n        and rentals occur as simulated by a poisson distribution and constrained physically by stock levels.\n        \"\"\"\n        _, ys = scan(\n            self.model_single_day, \n            init=init_state, \n            xs=jnp.arange(start_time, end_time)\n        )\n        return ys\n\n    def model_single_day(self, prev_state: Dict, time: int) -&gt; Tuple[Dict, jnp.array]:\n        \"\"\"Models a single day of inventory activity, including returns, rentals, and stock changes\n        \"\"\"\n        curr_state = dict()\n\n        # Simulate Returns\n        returns = self.returns_model(prev_state['existing_rentals'], time)\n        curr_state['starting_stock'] = numpyro.deterministic(\"starting_stock\", prev_state['ending_stock'] + returns.sum(1) + self.apply_policy(time))\n\n        # Simulate Rentals, incorporate them into the next state\n        rentals = self.demand_model(available_stock=curr_state['starting_stock'], time=time)\n        curr_state['ending_stock'] = numpyro.deterministic(\"ending_stock\", curr_state['starting_stock'] - rentals.sum(1))\n        curr_state['existing_rentals'] = numpyro.deterministic(\"existing_rentals\", prev_state['existing_rentals'] - returns + rentals)\n        return curr_state, rentals\n\n    ...\n    \nThe sub-models and methods referenced but not shown above are a little complicated and overwhelming with the array operations and logic however, so feel free to skim over this for now.\nclass RentalInventory\n    \n    ...\n    \n    ...\n\n    def demand_model(self, available_stock, time):\n        \"\"\"Models the true demand each day.\n        \"\"\"\n        raise NotImplementedError()\n    \n    def returns_model(self, existing_rentals: jnp.array, time: int) -&gt; jnp.array:\n        \"\"\"Models the number of returns each date\n        \"\"\"\n        # Distribution of possible rental durations\n        theta = numpyro.sample(\"theta\", dist.Normal(2.9, 0.01))\n        sigma = numpyro.sample(\"sigma\", dist.TruncatedNormal(0.7, 0.01, low=0))\n        return_dist = dist.LogNormal(theta, sigma)\n\n        # Calculate the discrete hazard of rented out inventory from previous time-points being returned\n        discrete_hazards = self.survival_convolution(dist=return_dist, time=time)\n\n        # Simulate returns from hazards\n        returns = numpyro.sample(\"returns\", dist.Binomial(existing_rentals.astype(\"int32\"), probs=discrete_hazards))\n        total_returns = numpyro.deterministic(\"total_returns\", returns.sum())\n        return returns\n        \n    def survival_convolution(self, dist, time: int) -&gt; jnp.array:\n        \"\"\"Calculates the probability of a return happening (discrete hazard rate) from all past time periods, returning an array where each index is a previous time period,\n        and the value is the probability of a rental from that time being returned at the current date.\n        \"\"\"\n        rental_durations = (time-jnp.arange(self.max_periods))\n        discrete_hazards = jnp.where(\n            # If rental duration is nonnegative,\n            rental_durations&gt;0,\n            # Use those rental durations to calculate a return rate, using a discrete interval hazard function\n            RentalInventory.hazard_func(jnp.clip(rental_durations, a_min=0), dist=dist ),\n            # Otherwise, return rate is 0\n            0\n        )\n        return discrete_hazards\n\n    @staticmethod\n    def hazard_func(t, dist):\n        \"\"\"Discrete interval hazard function - aka the probability of a return occurring on a single date\n        \"\"\"\n        return (dist.cdf(t+1)-dist.cdf(t))/(1-dist.cdf(t))\n\n    def apply_policy(self, time):\n        \"\"\"Adds in some number of units for the product at time T=t\n        \"\"\"\n        return self.policies[time]\nOne key thing to note is the demand_model method raises a NotImplementedError the idea is that this base class can be inherited by other classes that may have different demand models. Even the returns model could be overwritten. Here’s an example of a poisson demand model:\nclass PoissonDemandInventory(RentalInventory):\n    \"\"\"A model of rental inventory, modeling stock levels as returns and rentals occur each day.\n    Currently supports a single product\n    \"\"\"\n    def __init__(self, n_products: int = 1, policies: np.ndarray = None):\n        super().__init__(n_products, policies)\n        rng = np.random.default_rng(seed=99)\n\n        # Heterogeneity in true demand is lambd ~ Exp(5) distributed when using this class to simulate data from scratch\n        # When simulating demand based on an existing dataset, this can be overwritten\n        # i.e. `numpyro.do(inventory.demand_model, {\"lambd\": jnp.exp(U_hat)})`\n        self.U = jnp.log( 5 * rng.exponential( size=n_products) )\n\n    def demand_model(self, available_stock, time):\n        \"\"\"Models the true demand each day.\n        \"\"\"\n        with numpyro.plate(\"n_products\", self.n_products) as ind:\n            lambd = numpyro.sample(\"lambd\", dist.Normal(jnp.exp(self.U[ind]), 0.001))\n\n        unconstrained_rentals = numpyro.sample(\"unconstrained_rentals\", dist.Poisson(lambd))\n        rentals = numpyro.deterministic(\"rentals\", jnp.clip(unconstrained_rentals, a_min=0, a_max=available_stock ))\n        rentals_as_arr = ( time == jnp.arange(self.max_periods) )*rentals[:,None]\n        return rentals_as_arr\nRentals are simulated according to a poisson distribution for each product, and if there isn’t enough stock, then rentals are constrained. For example:\n\\[\n\\small{\n\\displaylines{\n\\text{demand}_{\\text{[j,t]}} \\sim \\text{Poisson}(\\lambda_{\\text{[j]}}) \\\\\n\\text{rentals}_{\\text{[j,t]}} = \\text{min}(\\text{demand}_{\\text{[j,t]}},\\ \\text{stock}_{\\text{[j,t]}}) \\\\\n\\text{for }j \\in J \\text{ products}\n}}\n\\]"
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html#simulating-the-impact-of-reordering-more-units",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html#simulating-the-impact-of-reordering-more-units",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "Simulating the impact of reordering more units",
    "text": "Simulating the impact of reordering more units\nThere are some helper functions in the code block below that we’ll use. Feel free to skip past these. They’re used to transform data from long form into a wide form where time periods are the column axis and each product is an row.\n\n\nCode\nfrom rental_model import RentalInventory, PoissonDemandInventory, MultinomialDemandInventory\n\n# Helper functions\ndef sort_products(df):\n    idx = list(df.index.names)\n    if idx[0] is not None:\n        return (\n            df.reset_index()\n            .assign(idx=lambda d: d.product_id.str.split(\"_\",expand=True)[1].astype(float))\n            .sort_values(by=\"idx\").drop('idx',axis='columns')\n            .set_index(idx)\n        )\n    else:\n        return (\n            df\n            .assign(idx=lambda d: d.product_id.str.split(\"_\",expand=True)[1].astype(float))\n            .sort_values(by=\"idx\").drop('idx',axis='columns')\n        )\n\ndef get_active_rentals_as_array(rentals: pd.DataFrame, max_periods: int = 10000) -&gt; np.array:\n    \"\"\"Take a dataframe of rental data and convert it to an array of currently active rentals and the days they were initially rented.\n    Each array element corresponds to a single day, and the value is the amount of active rentals that are still\n    out with customers that started on that date\n    \"\"\"\n    def _get_active_rentals_as_array(rentals, dates, max_periods=10000):\n        \"\"\"Gets currently active rentals for a single product\n        \"\"\"\n        active_rentals = np.zeros(max_periods)\n        active_rentals[:len(dates)] = (rentals.loc[lambda d: d.return_date.isnull()].date.values == dates[:,None]).sum(1)\n        return active_rentals\n    \n    start_date, end_date = rentals.date.agg(['min', 'max'])\n    dates = pd.date_range(start_date, end=end_date, freq='D').values\n    products = rentals.pipe(sort_products).product_id.unique()\n    active_rentals = np.zeros((len(products), max_periods))  # Create an empty matrix to store results\n\n    # Iterate through each product\n    for j, product_j in enumerate(products):\n        # Store their currently active rentals \n        active_rentals[j] = _get_active_rentals_as_array(rentals.query(f\"product_id==@product_j\"), dates=dates, max_periods=max_periods)\n   \n    return  active_rentals\n\ndef reorder_as_array(reorder_amount, reorder_time, max_periods=10000):\n    return (jnp.arange(max_periods) == reorder_time)*reorder_amount\n\n\nThe convenience function below combines all of the helper functions above to transform rental and stock data into a usable form for the model.\n\n# set initial state to last observed state in dataset\ndef get_current_state(rentals, stock):\n    active_rentals = get_active_rentals_as_array(rentals)\n    latest_starting_stock = stock.query(\"date==date.max()\").pipe(sort_products).starting_units.values\n    latest_ending_stock = stock.query(\"date==date.max()\").pipe(sort_products).ending_units.values\n    return dict(\n        starting_stock=latest_starting_stock,\n        ending_stock = latest_ending_stock,\n        existing_rentals=active_rentals\n    )\n\ninit_state = get_current_state(\n    rentals=rentals.query(f\"product_id=='product_{j}'\"), \n    stock=stock.query(f\"product_id=='product_{j}'\"))\n\nWe can now simulate what might happen if we re-stocked different amounts.\n\n# Try adding new units at time T=100\nreorder_amount = 150\nreorder_policy = reorder_as_array(reorder_amount, reorder_time=100)[None,:]\n\n# Define Simulation\nnsamples =250\nrental_inventory = PoissonDemandInventory(n_products = 1, policies=reorder_policy)\nsimulation = numpyro.infer.Predictive(\n    rental_inventory.model, \n    num_samples = nsamples,\n    # Input our learned parameters from the previous models\n    posterior_samples={\n        \"lambd\":idata_demand['lambd'][:nsamples,[-1]], \n        \"theta\":idata['theta'][:nsamples,None],\n        \"sigma\":idata['sigma'][:nsamples,None]\n    }\n\n)\n# Run Simulation\nresults = simulation(random.PRNGKey(SEED), init_state, start_time=100, end_time=200)\n\nThe code above runs a full series of 250 simulations. Ideally in the future, the lambda parameter would be a forecast as opposed to just using the latest rental rate, but that’s a problem for later.\nLet’s look at one potential outcome of the simulation. You can try re-running this multiple times to see a range of different outcomes that are all possible under this reorder policy. Most of these end up still being stocked out.\n\n\nCode\nsample_idx = np.random.choice(250)\ndates = pd.date_range(rentals.date.min(), periods=200, freq='D')\n\nhistorical_data = daily_rentals.loc[f'product_{j}']\nsim_data = pd.Series(results['rentals'][sample_idx,:].ravel(),  index = dates[100:] )\n \ndaily_rentals_j = pd.concat((historical_data.rentals, sim_data))\ndaily_stock_j = np.concatenate((historical_data.starting_units.values, results['starting_stock'][sample_idx,:].squeeze(-1)))\n\nax = plot_rentals(daily_rentals_j, daily_stock_j)\nax.set(title=f'Simulated Stock Levels\\nafter Reordering {reorder_amount} units')\nax.axvline(daily_rentals_j.index.values[100], color='k', ls='--', label='Reorder')\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can summarize all of these simulations with uncertainty intervals.\n\n\nCode\nfig, axes = plt.subplots(2,1,\n        gridspec_kw=dict(width_ratios=[15], height_ratios=[10,1]), \n        sharex=True)\n    \n\n# Plot simulations\ndaily_ending_stock_sim = results['ending_stock'].squeeze(-1)\naz.plot_hdi(dates[100:], daily_ending_stock_sim, smooth=False,ax=axes[0], color='r', fill_kwargs=dict(alpha=0.25),)\naxes[0].plot(dates[100:], daily_ending_stock_sim.mean(0), color='r', ls='--', label='Simulated Stock Levels')\n# axes[0].axvline(dates[60], color='C0',ls='--', label='Reorder (40 units)')\naxes[0].axvline(dates[100], color='k',ls='--', label='Reorder')\naxes[0].set(\n    title=f'Simulated Stock Levels:\\nwhat would happen after reordering {reorder_amount} units?',\n    ylabel='Stock Levels',\n    xlabel=''\n)\n\n# Plot actuals\nhistorical_data.ending_units.plot(color='k',ax=axes[0], label='Historical Stock Levels')\n\n# Overlay some simulations to confirm its lining up with the plot\naxes[0].plot(dates[100:], daily_ending_stock_sim[:20,:].T, alpha=0.1, color='k');\naxes[0].plot(dates[100:], daily_ending_stock_sim[0,:].T, alpha=0.1, color='k',label='Individual Simulations');\naxes[0].legend()\n\n# plot stockouts\nhistorical_stockouts = dates[:100][(historical_data.ending_units==0).values]\nsim_stockouts = np.unique(dates[100:][np.where((daily_ending_stock_sim.T==0))[0]])\naxes[-1].vlines(historical_stockouts,0,1, color='r', lw=5)\naxes[-1].vlines(sim_stockouts,0,1, color='r', lw=5)\naxes[-1].set_yticks([])\naxes[-1].annotate(\"Stockouts\", xy=(0.01, 0.2),xycoords='axes fraction', fontsize=14)\nplt.savefig(\"preview_image.png\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThis view makes it a little more obvious that there are probably stockouts happening, but it doesn’t tell us for sure. We can actually calculate the stockout rate over time by averaging over all of the existing simulations and counting the number of simulations that have 0 stock each day.\nWe could also go back and update the simulator to log things like “missed rentals due to stockouts” or other quantities that might be helpful.\n\n\nCode\nfig, ax = plt.subplots(1,1)\np_stockout = (daily_ending_stock_sim==0).mean(0)\nax.plot(dates[100:200], p_stockout)\nax.axvline(dates[100], color='k', ls='--', label='Reorder Time')\nax.set(title=f'Estimated Stockout Probability after reordering {reorder_amount} units', ylabel='Stockout Rate')\nax.legend()\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1,0))\nplt.show()\n\n\n\n\n\n\n\n\n\nIt appears that the current reorder guidance isn’t enough - so what should reorder? Lets try and estimate what would happen if we reordered more.\n\n\nCode\n# Try estimating the impact of 500 new units at time T=100\nreorder_amount = 500\nreorder_policy = reorder_as_array(reorder_amount, reorder_time=100)[None,:]\n\n# Define Simulation\nnsamples =250\nrental_inventory = PoissonDemandInventory(n_products = 1, policies=reorder_policy)\n\nsimulation = numpyro.infer.Predictive(\n    rental_inventory.model, \n    num_samples = nsamples,\n    # Input our learned parameters from the previous models\n    posterior_samples={\n        \"lambd\":idata_demand['lambd'][:nsamples,[-1]], \n        \"theta\":idata['theta'][:nsamples,None],\n        \"sigma\":idata['sigma'][:nsamples,None]\n    }\n\n)\n\n# Run Simulation\nresults = simulation(random.PRNGKey(SEED), init_state, start_time=100, end_time=200)\n\n# Plot\nfig, axes = plt.subplots(2,1,\n        gridspec_kw=dict(width_ratios=[15], height_ratios=[10,1]), \n        sharex=True)\n\n# Plot simulations\ndaily_ending_stock_sim = results['ending_stock'].squeeze(-1)\naz.plot_hdi(dates[100:], daily_ending_stock_sim, smooth=False,ax=axes[0], color='r', fill_kwargs=dict(alpha=0.25),)\naxes[0].plot(dates[100:], daily_ending_stock_sim.mean(0), color='r', ls='--', label='Simulated Stock Levels')\naxes[0].axvline(dates[100], color='k',ls='--', label='Reorder')\naxes[0].set(\n    title=f'Simulated Stock Levels:\\nwhat would happen after reordering {reorder_amount} units?',\n    ylabel='Stock Levels',\n    xlabel=''\n)\n\n# Plot actuals\nhistorical_data.ending_units.plot(color='k',ax=axes[0], label='Historical Stock Levels')\n\n# Overlay some simulations to confirm it is lining up with the plot\naxes[0].plot(dates[100:], daily_ending_stock_sim[:50,:].T, alpha=0.1, color='k');\naxes[0].plot(dates[100:], daily_ending_stock_sim[0,:].T, alpha=0.1, color='k',label='Individual Simulations');\naxes[0].legend()\n\n# plot stockouts\naxes[-1].vlines(dates[:100][(historical_data.ending_units==0).values],0,1, color='r', lw=5)\naxes[-1].vlines(np.unique(dates[100:][np.where((daily_ending_stock_sim.T==0))[0]]),0,1, color='r', lw=5)\naxes[-1].set_yticks([])\naxes[-1].annotate(\"Stockouts\", xy=(0.01, 0.2),xycoords='axes fraction', fontsize=14)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nClearly this is now way too much stock and we’d be over-ordering. So how can we identify the right amount to stock so that we don’t have stockouts but we don’t over-order?\nWe could just simulate repeatedly with different stock reorder inputs like below.\n\ndef sim_stockout_rate(stock_reorder, lambd, samples_per_iter=100):    \n    \n    init_state = get_current_state(\n        rentals=rentals.query(f\"product_id=='product_{j}'\"),\n        stock=stock.query(f\"product_id=='product_{j}'\")\n    )\n\n    # Try adding x new units at time T=100\n    reorder_policy = reorder_as_array(stock_reorder, reorder_time=100)[None,:]\n    rental_inventory = PoissonDemandInventory(n_products = 1, policies=reorder_policy)\n\n    simulation = numpyro.infer.Predictive(\n        rental_inventory.model, \n        num_samples = samples_per_iter,\n        # Input our learned parameters from the previous models\n        posterior_samples={\n            # Use the latest demand sample from the last date as a simple approach for now\n            \"lambd\": lambd[:samples_per_iter], \n            # Use rental duration parameter estimates\n             \"theta\": idata['theta'][:samples_per_iter,None],\n             \"sigma\": idata['sigma'][:samples_per_iter,None]\n        }\n    )\n    results = simulation(random.PRNGKey(SEED), init_state, start_time=100, end_time=200)\n    # Only going to care about instock rate for the second half of the simulation to be safe\n    burn_in = 50\n    stockout_rate = (results['ending_stock']==0).mean(0)[burn_in:].mean()\n    return stockout_rate\n\n# Run simulations\nstockout_results = np.empty((9,2))\nfor i, units in enumerate(range(0,450, 50)):\n    stockout_rate = sim_stockout_rate(units, lambd=idata_demand['lambd'][...,[-1]])\n    stockout_results[i,:] = [units, stockout_rate]\n\nfig, ax = plt.subplots()\nax.plot(stockout_results[:,0], stockout_results[:,1])\nax.set(ylabel='Est. Stockout Rate', xlabel='Units Reordered', title='Estimated Stockout Rate as we reorder more units')\nplt.show()\n\n\n\n\n\n\n\n\nIt’s clear that right around 300 units is when the probability of a stockout gets really low. We could speed this up if we wanted by building an optimizer. One easy way might be to build a binary search algorithm that gets you the minimum amount of stock without having any stockouts.\nThat’s an exercise for another time. Let’s reorder the recommended 300 units and see what happens.\n\ninit_state = get_current_state(rentals=rentals, stock=stock)\n\n# Try adding x new units at time T=100\nreorder_amount = 300\nreorder_policy = jnp.zeros((J_PRODUCTS, MAX_PERIODS)).at[j,100].set(reorder_amount)\nrental_inventory = MultinomialDemandInventory(n_products=J_PRODUCTS, policies=reorder_policy)\n\n# only simulate 1 sample - we're going to use the true data generating process \n# to pretend we actually reordered this amount of units and watched what happened afterward\nobserve_future = numpyro.infer.Predictive( rental_inventory.model, num_samples = 1)\nresults = observe_future(random.PRNGKey(SEED), init_state, start_time=100, end_time=200)\n\nNotice that this time we’re not loading in any parameters into the Predictive call. That’s because we’re using the true data-generating process to simulate what happens this time. We’re basically observing one possible future in the original fake world we created.\n\n\nCode\nfig, axes = plt.subplots(2,1,\n        gridspec_kw=dict(width_ratios=[15], height_ratios=[10,1]), \n        sharex=True)\n\ndaily_ending_stock = pd.concat((\n    daily_rentals.loc[f'product_{j}'].ending_units,\n    pd.Series(results['ending_stock'][..., j].squeeze(), index=dates[100:])\n))\n\ndaily_ending_stock.plot(color='k',ax=axes[0], label='Stock Levels')\naxes[0].axvline(dates[100], color='k',ls='--', label='Reorder')\naxes[0].set(\n    title=f'Observed Stock Levels after reordering {reorder_amount} units',\n    ylabel='Stock Levels',\n    xlabel='Date'\n)\naxes[0].axhline(0, color='r', lw=2, ls='--')\naxes[0].legend()\n\n# plot stockouts\naxes[-1].vlines(dates[(daily_ending_stock==0).values],0,1, color='r', lw=5)\naxes[-1].set_yticks([])\naxes[-1].annotate(\"Stockouts\", xy=(0.01, 0.2),xycoords='axes fraction', fontsize=14)\n\nfig.subplots_adjust(hspace=0.025)\n\n\n\n\n\n\n\n\n\nAmazing, by reordering 300 units, we no longer ran into any stockouts! This is exactly what we were hoping for.\nWe now have a system we can use to figure out the optimal stock to reorder for a single product. How can we now scale this to our entire inventory, or even inform the purchasing of new products?"
  },
  {
    "objectID": "posts/censored_demand/2024-02-06-censored-demand.html#can-we-do-something-more-simple",
    "href": "posts/censored_demand/2024-02-06-censored-demand.html#can-we-do-something-more-simple",
    "title": "Modeling Anything With First Principles: Demand under extreme stockouts",
    "section": "Can we do something more simple?",
    "text": "Can we do something more simple?\nWas all of this really necessary? Let’s see for ourselves by taking a naive approach. We’ll fit a basic regression where we control for the global stockout rate and dummy-encode stockouts - meant to represent a basic tabular data science approach.\n\nimport statsmodels.api as sm\nmodel = sm.OLS.from_formula(\n    \"rentals ~ total_stockout_rate + stockout\", \n    data=daily_rentals.query(f\"product_id=='product_{j}'\")\n).fit()\n\n# Predict the latest demand rate as if it didnt have a stockout\nX = daily_rentals.query(f\"product_id=='product_{j}'\").tail(1).assign(stockout=0)\nfcast = model.predict(X)\n\n\n\nCode\nnaive_rental_rate_fcast = np.vstack([fcast for _ in range(100)]) # broadcast the forecast\n\n# Simulate what would happen under different reorder amounts based on naive demand estimate\nstockout_results = np.empty((9,2))\nfor i, units in enumerate(range(0,450, 50)):\n    stockout_rate = sim_stockout_rate(units, lambd=naive_rental_rate_fcast)\n    stockout_results[i,:] = [units, stockout_rate]\n\nfig, ax = plt.subplots(figsize=(5,3))\nax.plot(stockout_results[:,0], stockout_results[:,1])\nax.set_ylabel('Est. Stockout Rate', fontsize=10)\nax.set_xlabel('Units Reordered', fontsize=10)\nax.set_title('Naive Approach:\\nEstimated Stockout Rate as we reorder more units', fontsize=10)\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see that this approach leads to a recommendation of 200 units reordered. Is that reasonable? Let’s see what happens.\n\n\nCode\ninit_state = get_current_state(rentals=rentals, stock=stock)\n\n# Try adding x new units at time T=100\nreorder_amount = 200\nreorder_policy = jnp.zeros((J_PRODUCTS, MAX_PERIODS)).at[j,100].set(reorder_amount)\nrental_inventory = MultinomialDemandInventory(n_products=J_PRODUCTS, policies=reorder_policy)\n\n# only simulate 1 sample - we're going to use the true data generating process \n# to pretend we actually reordered this amount of units and watched what happened afterward\nobserve_future = numpyro.infer.Predictive( rental_inventory.model, num_samples = 1)\nresults = observe_future(random.PRNGKey(SEED), init_state, start_time=100, end_time=200)\n\nfig, axes = plt.subplots(2,1,\n        gridspec_kw=dict(width_ratios=[15], height_ratios=[10,1]), \n        sharex=True)\n\ndaily_ending_stock = pd.concat((\n    daily_rentals.loc[f'product_{j}'].ending_units,\n    pd.Series(results['ending_stock'][..., j].squeeze(), index=dates[100:])\n))\n\ndaily_ending_stock.plot(color='k',ax=axes[0], label='Stock Levels')\naxes[0].axvline(dates[100], color='k',ls='--', label='Reorder')\naxes[0].set(\n    ylabel='Stock Levels', \n    xlabel='Date', \n    title=f'Naive Approach:\\nObserved Stock Levels after reordering {reorder_amount} units')\naxes[0].axhline(0, color='r', lw=2, ls='--')\naxes[0].legend()\n\n# plot stockouts\naxes[-1].vlines(dates[(daily_ending_stock==0).values],0,1, color='r', lw=5)\naxes[-1].set_yticks([])\naxes[-1].annotate(\"Stockouts\", xy=(0.01, 0.2),xycoords='axes fraction', fontsize=14)\n\nfig.subplots_adjust(hspace=0.025)\n\n\n\n\n\n\n\n\n\nAs we can see above, using more traditional tabular machine learning methods just don’t cut it. This problem needs the more advanced model structure that we had implemented above."
  },
  {
    "objectID": "posts/2025-03-06-multicollinearity-misconceptions.html",
    "href": "posts/2025-03-06-multicollinearity-misconceptions.html",
    "title": "Common Misconceptions with Multicollinearity",
    "section": "",
    "text": "I recently saw a viral linkedin post discussing how multicollinearity will ruin your regression estimates. The solution? Simply throw out variables with high Variance Inflation Factors and re-run your regression.\nLet me tell you, don’t do this. A fear of multicollinearity is this misconception in data science that is far too prevalent.\nI’ve seen the same assumptions with correlation - if two variables are very highly correlated people assume they shouldn’t be in the same model. I’m going to show you why 1) these approaches are a mistake and 2) you can’t escape domain knowledge when informing your regression.\n\nA simulation\nWe’re going to simulate a fake scenario. You want to know the effect of some variable \\(X_3\\) on \\(y\\) with your regression. The data generating process is below\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nX3\n\nX3\n\n\n\nX1-&gt;X3\n\n\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX3-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n\n\nThe simulation below makes it clear the true effect of \\(X_3\\) on \\(y\\) is 0.7, so we know our regression should hopefully return it as an estimate\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nimport matplotlib.pyplot as plt\n\nN = 250\nSIG = 0.5\nTRUE_EFFECT = 0.7\n\nrng = np.random.default_rng(99)\ndf = (\n    pd.DataFrame({\n    \"x1\":rng.normal(0, 1, size=N), \n    \"x2\":rng.normal(0, 1, size=N)}\n    )\n    # simulate x3 as a function of x1\n    .assign(x3=lambda d: rng.normal(1.1 + d.x1*3.8, SIG))\n    # simulate y as a function of x1,x2,x3\n    .assign(y = lambda d: rng.normal(0.5 + d.x1*-4 + d.x2*-0.5 + d.x3*TRUE_EFFECT, SIG))\n)\ndf.head().round(2)\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\ny\n\n\n\n\n0\n0.08\n0.54\n1.96\n1.94\n\n\n1\n-0.46\n1.59\n0.08\n1.33\n\n\n2\n0.05\n-0.69\n0.81\n1.01\n\n\n3\n0.69\n0.47\n3.44\n0.53\n\n\n4\n-1.76\n-0.77\n-5.98\n3.36\n\n\n\n\n\n\n\nLets take a look at the variance inflation factors below\n\n\nCode\nX = df.filter(like='x')\npd.DataFrame(\n    {\"VIF\": [vif(X.values, i) for i in range(X.shape[1])]}, \n    index=X.columns\n).round(2)\n\n\n\n\n\n\n\n\n\nVIF\n\n\n\n\nx1\n10.35\n\n\nx2\n1.00\n\n\nx3\n10.35\n\n\n\n\n\n\n\nBased on this, we supposedly should remove \\(X_1\\) from our regression (VIFs of 5-10 are considered high).\nIf we look at the correlation, we reach the same conclusions - \\(X_1\\) has high correlation with \\(X_3\\) so supposedly “they provide the same information and only 1 is useful”\n\nX.corr().loc['x3']\n\nx1    0.990898\nx2   -0.020267\nx3    1.000000\nName: x3, dtype: float64\n\n\n\n\nDoes it work?\nWe’re going to fit two regressions - one that follows the advice of throwing out variables with high VIFs or correlation - and another that is informed by domain knowledge.\n\n# vif/correlation informed variable selection\nm1 = sm.OLS.from_formula(\"y ~ x2 + x3\", data=df).fit()\n\n# domain knowledge informed variable selection\nm2 = sm.OLS.from_formula(\"y ~ x1 + x2 + x3\", data=df).fit()\n\n\n\nCode\ndef plot_ci(model, ax, color='C0', label=None):\n    lb = (model.params - model.bse*2)\n    ub = model.params + model.bse*2\n    ax.hlines(0, lb.loc['x3'], ub.loc['x3'], lw=2, label=label, color=color)\n    ax.scatter(model.params.loc['x3'], 0, facecolor='white', edgecolor=color, zorder=10, s=10)\n\n\nfig, ax = plt.subplots()\nplot_ci(m1, ax=ax, color='C1', label='VIF/Corr. informed model')\nplot_ci(m2, ax=ax, color='C0', label='Causally informed model')\n\nax.axvline(0, ls='--', color='k')\nax.axvline(TRUE_EFFECT, color='r', ls='--', lw=1, label='True Effect')\nax.set_xlim(-1,1)\nax.set_ylim(-1,1)\nax.set_yticks([])\nax.set(title=\"Estimated effect of X3 -&gt; y\", xlabel=\"Beta Coefficient\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nNot only is the VIF informed model wrong, but more importantly it actually thinks \\(X_3\\) has a negative effect instead of a positive effect!\n\n\nWhy did this happen?\nLets go back to our data generating process\n\n\n\n\n\n\n\nG\n\n\n\nX1\n\nX1\n\n\n\nX3\n\nX3\n\n\n\nX1-&gt;X3\n\n\n\n\n\nY\n\nY\n\n\n\nX1-&gt;Y\n\n\n\n\n\nX3-&gt;Y\n\n\n\n\n\nX2\n\nX2\n\n\n\nX2-&gt;Y\n\n\n\n\n\n\n\n\n\n\nIt’s clear that \\(X_1\\) causally effects both \\(X_3\\) and \\(y\\) - this makes it a confounding variable, or a “backdoor” for \\(X_3\\). If we want our regression to have proper estimates of \\(X_3\\), we therefore need to make sure our regression model accounts for \\(X_1\\) to de-bias \\(X_3\\).\nA classic (and dark) example is the ice-cream and drowning relationship. As people eat more ice cream there tend to be more drownings. Why? Warm weather causes ice cream and increased pool and beach use lead to the unfortunate increase in drownings. Accounting for warm weather explains away this relationship, and thats exactly how regression can work.\nThis is why its so important to incorporate domain knowledge in your modeling. Mapping pout the data generating process and using it to inform your model is always the right choice. For more on this check out A Crash Course on Good and Bad Controls. I also really like Richard McElreath’s discussion on this in his 2nd edition of Statistical Rethinking."
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "",
    "text": "The Weibull distirbution is an excellent choice for many survival analysis problems - it has an interpretable parameterization that is highly flexible to a large number of phenomenon. The main advantage is that it can model how the risk of failure accelerates over time. This post will focus on the \\(\\text{Weibull}(k, \\lambda)\\) parameterization, although I hope to cover the Gumbel reparameterization in the future.\nThis post requires some base understanding of survival analysis. I’ll try to have another post in the future that discusses survival analysis at a more introductory level\n\n\nSurvival Curves model time to some event (such as a failure) - they can tell you the probability of a binary event occurring at each future time point in somethings lifetime. An example survival curve is shown below:\n\n\nCode\nfrom typing import *\nimport numpy as np\nimport pymc as pm\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nfrom weibull import Weibull\nimport scipy\nplt.style.use(\"fivethirtyeight\")\n\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nt = np.arange(1, 101)\nSt =  dist.survival(t)\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(t,St, label=\"Survival Curve\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1,0))\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Survival probability\", fontsize=16)\nax.set_title(\"How to Read a Survival Curve\", fontsize=16)\nax.legend(fontsize=16)\n\n# Annotations\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\nx = 20\nax.vlines(x, -0.05, St[x-1], color=\"k\", ls=\"--\", alpha=0.75, lw=1.5)\nax.hlines(St[x-1], 0, x, color=\"k\", ls=\"--\", alpha=0.75, lw=1.5)\nax.tick_params(axis='both', which='major', labelsize=14)\n\nax.annotate(f'\"There\\'s a ~60% probability of an event \\nstill not occurring by time t=20\"', \n        xy=(x,St[x-1]),\n        xycoords=\"data\", \n        xytext=(0.4, 0.8),\n        textcoords='axes fraction',\n        arrowprops=dict(facecolor='black', shrink=0.05),\n        horizontalalignment='left',\n        verticalalignment='top',\n        fontsize=12,\n        bbox=dict(boxstyle=\"Round,pad=0.5\", fc=\"white\", ec=\"gray\", lw=2)\n)\nplt.show()\n\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\n\n\n\n\n\n\n\nThey basically tell you the probability of a unit not observing an event up until some time point.\n\n\n\nHazard Curves are another way to think about survival analysis problems. A hazard curve tells you the instantaneous risk of an event occurring at each time point.\nHazard Curves tend to be very informative as they allow you to see how risk changes over time - sometimes it might decelerate, or sometimes it might even accelerate exponentially. Here’s an example of a hazard curve below.\n\n\nCode\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nt = np.arange(1, 101)\nht =  dist.hazard(t)\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(t,ht, label=\"Hazard Curve\")\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Hazard Rate\", fontsize=16)\nax.set_title(\"An Example Hazard Curve\", fontsize=16)\nax.legend(fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#a-quick-recap-on-survival-curves",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#a-quick-recap-on-survival-curves",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "",
    "text": "Survival Curves model time to some event (such as a failure) - they can tell you the probability of a binary event occurring at each future time point in somethings lifetime. An example survival curve is shown below:\n\n\nCode\nfrom typing import *\nimport numpy as np\nimport pymc as pm\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nfrom weibull import Weibull\nimport scipy\nplt.style.use(\"fivethirtyeight\")\n\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nt = np.arange(1, 101)\nSt =  dist.survival(t)\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(t,St, label=\"Survival Curve\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1,0))\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Survival probability\", fontsize=16)\nax.set_title(\"How to Read a Survival Curve\", fontsize=16)\nax.legend(fontsize=16)\n\n# Annotations\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\nx = 20\nax.vlines(x, -0.05, St[x-1], color=\"k\", ls=\"--\", alpha=0.75, lw=1.5)\nax.hlines(St[x-1], 0, x, color=\"k\", ls=\"--\", alpha=0.75, lw=1.5)\nax.tick_params(axis='both', which='major', labelsize=14)\n\nax.annotate(f'\"There\\'s a ~60% probability of an event \\nstill not occurring by time t=20\"', \n        xy=(x,St[x-1]),\n        xycoords=\"data\", \n        xytext=(0.4, 0.8),\n        textcoords='axes fraction',\n        arrowprops=dict(facecolor='black', shrink=0.05),\n        horizontalalignment='left',\n        verticalalignment='top',\n        fontsize=12,\n        bbox=dict(boxstyle=\"Round,pad=0.5\", fc=\"white\", ec=\"gray\", lw=2)\n)\nplt.show()\n\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\n\n\n\n\n\n\n\nThey basically tell you the probability of a unit not observing an event up until some time point."
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#a-quick-recap-on-hazard-curves",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#a-quick-recap-on-hazard-curves",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "",
    "text": "Hazard Curves are another way to think about survival analysis problems. A hazard curve tells you the instantaneous risk of an event occurring at each time point.\nHazard Curves tend to be very informative as they allow you to see how risk changes over time - sometimes it might decelerate, or sometimes it might even accelerate exponentially. Here’s an example of a hazard curve below.\n\n\nCode\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nt = np.arange(1, 101)\nht =  dist.hazard(t)\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(t,ht, label=\"Hazard Curve\")\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Hazard Rate\", fontsize=16)\nax.set_title(\"An Example Hazard Curve\", fontsize=16)\nax.legend(fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-k-parameter",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-k-parameter",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "The \\(k\\) Parameter",
    "text": "The \\(k\\) Parameter\nSometimes also called \\(\\rho\\), this parameter controls the degree of acceleration of the hazard curve.\n\n\nCode\nks = [0.5, 1, 1.5, 2, 2.5]\nlambd = 30\nt = np.arange(1,51)\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nfor i in range(len(ks)):\n    dist = Weibull(ks[i], lambd)\n    ht = dist.hazard(t)\n    ax.plot(t, ht, label=f\"k={ks[i]}\", lw=5)\n\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Hazard Rate\", fontsize=16)\nax.set_title(\"Hazard Curves with Varying k Parameters\", fontsize=16)\nax.legend(fontsize=16)\nplt.show()\n\n\n\n\n\n\n\n\n\nAs shown above, here’s how different parameter values impact acceleration:\n\nWhen k &lt; 1: Hazard decelerates\nWhen k = 1: Hazard is constant. This is equivalent to an exponential distributions hazard function.\nWhen k &gt; 1: Hazard increases logarithmically\nWhen k = 2: Hazard increases with constant acceleration\nWhen k &gt; 2: Hazard increases exponentially\n\nClothing is a great example of something where the hazard increases over time - the risk of a clothing item getting damaged obviously increases over time as the item is worn, washed, gets loose stitching, etc."
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-lambda-parameter",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-lambda-parameter",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "The \\(\\lambda\\) Parameter",
    "text": "The \\(\\lambda\\) Parameter\nSometimes also referred to as a scale parameter, this parameter represents the time point where there’s a 63.2% probability of an event having occurred. Weird, I know, its just an arbitrary thing quality\n\nk = 1.5\nlambd = 30\ndist = Weibull(k, lambd)\nt = np.linspace(1,100, 10000)\nSt = dist.survival(t)\n\n# choose the point in the survival curve where the time is t=30\n# take the inverse of it so that its the probability of an event having occurred by that time.\n(1-St[t&gt;=lambd][0]).round(4)\n\n0.6321\n\n\nLet’s see this visually as well - if you look at the plot below, each survival curve indicates that there’s a 63.2% probability of an event having occurred at the exact time that \\(t=\\lambda\\)\n\n\nCode\nks = 1.5\nlambds = [15, 30, 60, 120]\nt = np.arange(1,151)\n\nfig, ax = plt.subplots(figsize=(10,5))\n\nfor i in range(len(lambds)):\n    dist = Weibull(k, lambds[i])\n    St = dist.survival(t)\n    ax.plot(t, St, label=f\"lambd={lambds[i]}\", lw=5)\n    ax.axvline(lambds[i],ls=\"--\", alpha=0.5, color=f'C{i}')\n\nax.axhline(1-0.632, color=\"k\", ls=\"--\", alpha=0.75,\n    label=\"Point where there's a 63.2% probability\\nof the event having occurred\\nby that time\")\n\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Hazard Rate\", fontsize=16)\nax.set_title(\"Hazard Curves with Varying k Parameters\", fontsize=16)\nax.legend(fontsize=12, loc=\"upper right\")\nplt.show()"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#easier-predictions-lifetime-survival-curves-and-hazard-curves",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#easier-predictions-lifetime-survival-curves-and-hazard-curves",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "Easier Predictions: Lifetime, Survival Curves, and Hazard Curves",
    "text": "Easier Predictions: Lifetime, Survival Curves, and Hazard Curves\nAverage lifetime, Survival Curves, and Hazard Curves are the basic types of predictions for working with survival analysis. Most distributions have formulas that make these things easier to calculate. Below is the underlying code for these functions.\nclass Weibull:\n\n    def __init__(self, k, lambd):\n        self.k = k\n        self.lambd = lambd\n\n    ...\n\n    def expectation(self) -&gt; np.array:\n        '''Calculates the expectation of the weibull distribution\n        '''\n        return self.lambd * sp.gamma(1 + 1/self.k)\n\n    def survival(self, t: np.array) -&gt; np.array:\n        '''Outputs the survival probability at each time point T. This is done with the survival function, \n        the complement of the Weibull Distribution's PDF.\n\n        Parameters\n        -----------\n            t: A numpy array with time points to calculate the survival curve,      \n                utilizing the distributions parameters\n        \n        Returns\n        -------\n            St: A survival curve calculated over the inputted time period\n        '''\n        CDF = 1 - self.cdf(t)\n        St = 1 - CDF\n        return St\n\n    def hazard(self, t: np.array) -&gt; np.array:\n        '''Outputs the hazard rate at each time point T.\n\n        Parameters\n        -----------\n            t: A numpy array with time points to calculate the survival curve,      \n                utilizing the distributions parameters\n        \n        Returns\n        -------\n            St: A survival curve calculated over the inputted time period\n        '''\n        ht = (self.k/self.lambd)*(t/self.lambd)**(self.k-1)\n        return ht\n    \n    ...\nWe already looked into hazard and survival curves - let’s take a look at estimating the average lifetime.\n\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nEt = dist.expectation()    \nprint(f\"Expected Average Lifetime = {round(Et,1)}\")\n\nExpected Average Lifetime = 27.1\n\n\nDoes this align with reality? Lets simulate event times from the weibull distribution to find out\n\nevent_times = dist.sample(n=1000000)\nprint(f\"Simulated Average Lifetime = {round(event_times.mean(),1)}\")\n\nSimulated Average Lifetime = 27.0"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#predicting-mean-residual-life",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#predicting-mean-residual-life",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "Predicting Mean Residual Life",
    "text": "Predicting Mean Residual Life\nAnother common and useful prediction you may want to present is the remaining life. There are a few different ways to calculate this:\n1. Use the formula\n\\[\nMRL(t) = \\frac{\\lambda \\: \\Gamma(1+1/k, \\: (\\frac{t}{\\lambda})^k)}{S(t)} - t\n\\]\nAnd we can see it implemented in python below:\nclass Weibull:\n\n    def __init__(self, k, lambd):\n        self.k = k\n        self.lambd = lambd\n\n    ...\n\n    def mean_residual_life(self, t: np.array) -&gt; np.array:\n        t = self._normalize_to_array(t)\n        St = self.survival(t)\n        numerator = (\n            self.lambd \n            * sp.gammaincc(1+1/self.k, (t/self.lambd)**self.k)\n            * sp.gamma(1+1/self.k))\n\n        result = np.divide(\n            numerator,\n            St,\n            out=np.zeros_like(numerator),\n            where=St!=0\n        ) - t[:,None]\n        \n        # The code above returns negative values when St=0. This clipping corrects those cases\n        return np.clip(result, 0, np.inf)\nAnd we can call that function like so:\n\ncurr_time = 40\nk, lambd = 1.5, 30\ndist = Weibull(k,lambd)\nremaining_life = dist.mean_residual_life(curr_time)\n\n2. Simulate\n\nT = dist.sample(n=10000000)\nunits_reached_curr_time = T[T&gt;curr_time] - curr_time\nremaining_life_simulated = units_reached_curr_time.mean()\n\nWith truncated sampling, this can also be more efficient\n\nT_cond = dist.sample(n=10000000,left_trunc=curr_time) - curr_time\nremaining_life_simulated2 = T_cond.mean()\n\n3. Transform the hazard curve\nThere’s a convenient relationship between the hazard curves, survival curves, and expectations. It turns out that\n\\[\nS(t) = \\text{exp}(-Ht)\n\\]\nwhere \\(Ht\\) is the cumulative hazard function. Additionally, the expectation is just the integral of the survival function\n\\[\nE[T] = \\int_{t=0}^\\infty S(t) \\, dt\n\\]\nUsing these relationships, we can 1. take the hazard curve from time 40 onwards 2. turn it into the cumulative hazard 3. transform that into a survival curve 4. integrate the survival curve into an expectation\n\nht = dist.hazard(np.arange(curr_time, curr_time+1000))\nHt = ht.cumsum()\nSt = np.exp(-Ht)\nremaining_life_from_hazard = scipy.integrate.simps(St)\n\n/var/folders/1p/v01fvg3j1cz1hzv988ygj8m00000gn/T/ipykernel_19389/124568412.py:4: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n  remaining_life_from_hazard = scipy.integrate.simps(St)\n\n\nComparing all of these methods we end up with the following:\n\n\nCode\nprint(\n    \"Remaining Life (from formula) = {:.4}\\n\".format(remaining_life.ravel()[0]),\n    \"Remaining Life (from simulation) = {:.4}\\n\".format(remaining_life_simulated),\n    \"Remaining Life (from truncated simulation) = {:.4}\\n\".format(remaining_life_simulated2),\n    \"Remaining Life (manually calculated from hazard) = {:.4}\\n\".format(remaining_life_from_hazard)\n)\n\n\nRemaining Life (from formula) = 15.05\n Remaining Life (from simulation) = 15.04\n Remaining Life (from truncated simulation) = 15.05\n Remaining Life (manually calculated from hazard) = 14.14\n\n\n\nThey’re all very close, but it looks like theres some slight error when manually calculating (and its more complicated)."
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#calculating-conditional-survival-curves",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#calculating-conditional-survival-curves",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "Calculating Conditional Survival Curves",
    "text": "Calculating Conditional Survival Curves\nThe last type of prediction we’ll show here is conditional survival. This is basically saying “what’s the survival curve if we’ve made it up to time=t?”\nIt turns out, all you have to do is calculate the survival curve and normalize it by the eligible area of the distirbution. This basically means calculating a survival curve past the current time, and then dividing it by the survival function at the current time of interest.\n\\[\nS(t|\\text{curr time}=40) = S(t)/S(40)\n\\]\nIt was pretty easy to add that to the Weibull.survival() function from earlier:\nclass Weibull:\n\n    def __init__(self, k, lambd):\n        self.k = k\n        self.lambd = lambd\n    \n    ...\n\n    def survival(self, t: np.array, curr_time: Optional[int] = None) -&gt; np.array:\n        '''Outputs the survival probability at each time point T. This is done with the survival function, \n        the complement of the Weibull Distribution's PDF.\n\n        Can also be used to calculate conditional survival with the `curr_time` argument.\n\n        Parameters\n        -----------\n            t: A numpy array with time points to calculate the survival curve,      \n                utilizing the distributions parameters\n            curr_time: Used to calculate the survival curve given already reaching \n                some point in time, `curr_time`.\n        \n        Returns\n        -------\n            St: A survival curve calculated over the inputted time period\n        '''\n        # Normalizing constant used for conditional survival\n        norm = 1 if curr_time is None else self.survival(curr_time)\n        \n        # check inputs\n        t = self._normalize_to_array(t)\n        if curr_time is not None and (t &lt; curr_time).sum() &gt; 1:\n            raise ValueError('t&lt;curr_time. t must be greater than or equal to curr_time')\n        \n        St = (1 - self.cdf(t))/norm\n        return St\nAnd as we can see below, it lines up perfectly with an empirically calculated kaplan meier curve\n\nfrom lifelines import KaplanMeierFitter\n\ncurr_time = 40\nT_cond = dist.sample(1000000, left_trunc=curr_time) - curr_time\nkmf = KaplanMeierFitter()\nkmf.fit(T_cond)\nkmf.plot_survival_function(lw=5, ls=\"--\")\n\nplt.plot(dist.survival(t=np.arange(40, 200+1), curr_time=curr_time), label=\"Conditional Survival Curve\")\nplt.legend()\nplt.ylabel(\"Conditional Survival Rate\")\nplt.title(f\"Conditional Survival Given a Unit Reached Time={curr_time}\")\nplt.show()\n\n\n\n\n\n\n\n\nA convenient feature about survival curves is that if we want to know the probability of an event occurring in the next 1 time period, or 2 time periods, all we have to do is take the complement of the survival probability. So that means we can also use this method to calculate “probability of an event occurring in the next 1 time period”"
  },
  {
    "objectID": "posts/2022-04-17-out-of-sample-pymc.html",
    "href": "posts/2022-04-17-out-of-sample-pymc.html",
    "title": "Making out of sample predictions with PyMC",
    "section": "",
    "text": "Introduction\nA cool thing about hierarchical models is that its easy to predict out of sample - i.e. if you want to make a prediction on a new zipcode, just sample from the state’s distribution (composed of the state average and variance across zip codes in that state).\nIn pymc3, it’s somewhat easy to accomplish this, but not as straightforward as we’d hope. This blog post will show a trick that lets you easily predict out of sample, and will reduce some of the overhead that comes from writing alot of custom prediction functions\n\n\nSimulating data\nI simulated a 2 level hierarchical model - for interpretability, I set it up as a state &gt; zipcode model. You can following along with the notebook here. The data is as follows\n\n\n\n\n\n\n\nUsing categorical variables\nCategorical variables are a somewhat new feature of pandas - they can store categories that aren’t in the observed data, and are an easy replacement for pd.factorize() (a common tool for those familiar with the bayesian workflow).\nWe can use these to trick pymc into thinking there’s a category with no observed data, and pymc ends up assigning the global distribution to that unobserved category, which we can simply reference in the future for any time we want to make a prediction on out of sample data.\n# Convert to categorical and add an `out_of_sample` category\ndf = df.assign(state = pd.Categorical(df.state).add_categories(\"out_of_sample\"))\\\n    .assign(zipcode = pd.Categorical(df.zipcode).add_categories(\"out_of_sample\"))\n\n\nFitting the model\nWe’ll use the codes from the categorical columns to index our model coefficients, and we’ll use the categories as coordinates for the model to map names to.\ncoords={\n    \"state\":df.state.cat.categories,\n    \"zipcode\":df.zipcode.cat.categories\n}\n\ndef hierarchical_normal(name, μ, dims):\n    '''Adapted from Austin Rochford'''\n    Δ = pm.Normal('Δ_{}'.format(name), 0., 1., dims=dims)\n    σ = pm.Exponential('σ_{}'.format(name), 2.5)\n    return pm.Deterministic(name, μ + Δ * σ, dims=dims)\n\n\nwith pm.Model(coords=coords) as model_nc:\n    \n    # Observed Data tracking\n    state_ = pm.Data(\"state_\", df.state.cat.codes)\n    zip_ = pm.Data(\"zip_\", df.zipcode.cat.codes)\n    obs = pm.Data(\"obs\", df.y)\n\n    # Hyperprior\n    mu_country = pm.Normal(\"mu_country\", 0, 1)\n    \n    # Prior\n    sig = pm.Exponential(\"sig\", 1)\n    \n    # Hierarchical coefficients\n    mu_state = hierarchical_normal(\"mu_state\", μ=mu_country, dims=\"state\")\n    mu_zipcode = hierarchical_normal(\"mu_zipcode\", μ=mu_state, dims=(\"zipcode\", \"state\") )\n    \n    # Observational model\n    y = pm.Normal(\"y\", mu_zipcode[zip_, state_], sig, observed=obs)\n    \n    # Fit \n    trace_nc = pm.sample(target_accept=0.9, return_inferencedata=True, random_seed=SEED)\nThere are a few key point that make out of sample prediction possible * Having the out_of_sample category for each indexed variable with no observed data * Passing the coords in the model statement * Using dims to reference which model coefficients have which coordinate labels * Having all of our input data wrapped in a pm.Data() statement\nThat last point is particularly important. For PyMC, if you want to make predictions on new data, you have to replace the data that the model references and the only way to do that (that I know of atleast) is to using a Theano shared variable. pm.Data() handles all of that fo you.\nSo we fit our model, lets take a quick look at the state level coefficients\npm.plot_forest(trace_nc, var_names=[\"mu_state\"])\n\n\n\n\n\nGreat, that out of sample variable seems to represent the global distribution across states - i.e. if we were to make a prediction for a new state we’d potentially use that distribtion (we’ll confirm further down).\nWe’ll check the zip code level below as well, looking at Maine specifically\n\n\n\n\n\nAs we can see, the out_of_sample variable has a sampled value despite there being no observed data for it. Now the question is, does this align with how we’d predict new data?\nLet’s try calculating coefficients out of sample by hand and see if it aligns with the out_of_sample values\npost = trace_nc.posterior\n\n# Pull the true data from our simulation\nstate_true = mu_state_true.random(size=4000)\n\n\n# Calculate out of sample state means by drawing from global distribution\nmu_country = post[\"mu_country\"].values.reshape(4000,-1)\nσ_state = post[\"σ_mu_state\"].values.reshape(4000,-1)\nmu_state = np.random.normal(mu_country, σ_state)\n\n# Using the indexing trick\nstate_idx_trick = post[\"mu_state\"].sel({\"state\":[\"out_of_sample\"]}).values.ravel()\n\n# Pull the true data from simulation\nzip_true = pm.Normal.dist(mu_state_true.random(size=4000), sig_zip_true).random(size=4000)\n\n# calculate out of sample mu by hand by drawing from out of sample state prediction above\nσ_zipcode = post[\"σ_mu_zipcode\"].values.reshape(4000,-1)\nmu_zipcode = np.random.normal(mu_state, σ_zipcode)\n\n# Use the indexing trick\nzip_idx_trick = (post[\"mu_zipcode\"]\n                .sel({\"state\":[\"out_of_sample\"], \"zipcode\":[\"out_of_sample\"]})\n                .values.ravel())\nWe can compare these results by plotting their distributions below\n\n\n\n\n\nNotice that the manual prediction and the indexing trick are basically identical. There’s a slight difference from the ground truth, but thats to be expected since we’re fitting a model on limited data (and anyway, it’s still quite close).\n\n\nPredicting out of sample\nLet’s go ahead and actually make prediction now - we’ll make predictions for the following data below\n\nThe first example is in sample\nThe second example is in sample for state, out of sample for zipcode\nThe third example is out of sample entirely\n\n\n\n\n\n\nAnd finally we’ll use the model to make predictions on this new data. Notice the pm.set_data() function - remember our pm.Data() calls from before? This tells PyMC to override that with new data, so when we sample from the posterior predictive it makes predictions on the new data instead of the data used to fit the model.\n\n\nClick here for helper function code\n\n# We're making some quick convenience functions to map this new data \n# to the proper indexes from the fitted model\nzip_lookup = dict(zip(df.zipcode.cat.categories, range(len(df.zipcode.cat.categories))))\nstate_lookup = dict(zip(df.state.cat.categories, range(len(df.state.cat.categories))))\n\ndef labels_to_index(series, lookup):\n    '''Converts categories to their proper codes'''\n    series = series.copy()\n    in_sample = series.isin(lookup.keys())\n    series.loc[~in_sample] = \"out_of_sample\"\n    return series.map(lookup).values.astype(\"int8\")\n\n\nwith model_nc:\n    # Set new data for the model to make predictions on\n    pm.set_data({\n        \"state_\": X.state.pipe(labels_to_index, state_lookup),\n        \"zip_\": X.zipcode.pipe(labels_to_index, zip_lookup)\n    })\n    \n    # make predictions\n    preds = pm.sample_posterior_predictive(trace_nc)\n\n\n\n\n\nThis is exactly what we were looking for - and prediction is easy, just map any out of sample states or zipcodes to the out_of_sample category. Notice how in sample predictions have smaller uncertainty intervals and out of sample data is more uncertain - this is exactly what we’d expect. This trick makes it much easier to make predictions compared to having to write out a custom prediction function that follows the same logic as the model.\nIf you have any other easy tricks for out of sample prediction let me know!"
  },
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html",
    "title": "Introduction to Surrogate Indexes",
    "section": "",
    "text": "How should you design your experiments if the metric you want to change might take months to observe?\nInspired by Susan Athey’s paper on Surrogate indexes and another working paper, Target for Long Term Outcomes, I’ve wanted to share my learnings about surrogate indexes for a long time.\nI’m hoping to cover the following in a series of blog posts:\n\nThe surrogate index estimator\nSurrogate Index in practice\n\nFitting a surrogate index on a realistic dataset\nValidating the surrogate index over a set of historical experiments\n\nTargeting Long Term Outcomes\n\nOptimizing a policy\nAttempting multi-armed bandits for early optimization\n\n\nBy simulating the data generating process from scratch, I hope this can also be a helpful tool for others to build on so they can answer their own questions they may have about estimating Long Term Treatment Effects."
  },
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-1-simulating-the-data",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-1-simulating-the-data",
    "title": "Introduction to Surrogate Indexes",
    "section": "Step 1: Simulating the data",
    "text": "Step 1: Simulating the data\nWe’ll start by simulating two datasets: A historical dataset and an experiment dataset. The advantage to simulating data is that we know the exact effects, so when we try and estimate them we can confirm our methods are recovering the true effect.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom simple_simulation import set_true_parameters, transmitter\nSEED = 99\n\n# step 1: simulate data\nrng = np.random.default_rng(seed=SEED)\nlogit_link=True\n\n# True parameters\nZdims = 20 # customer latent dims\nXdims = 5 # pre treatment covariates\nSdims = 8 # intermediate outcomes (the surrogates)\nn_users = 50000\n\nGROUND_TRUTH = set_true_parameters(Zdims, Xdims, Sdims, logit_link=True, rng=rng)\n\n# Simulate data \nhistorical_data = transmitter(GROUND_TRUTH, add_treatment=False, n_users=n_users,logit_link=logit_link, rng=rng) \nexperiment_data = transmitter(GROUND_TRUTH, add_treatment=True, n_users=n_users, logit_link=logit_link, rng=rng)\n\n# Censor the experiment dataset so that we dont know the long term outcome yet\nY_TRUE = experiment_data.Y.values\nexperiment_data = experiment_data.assign(Y=np.NaN)\n\n# Show the historical dataset \ndisplay(historical_data.head(5))\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\n\n\n\n\n\n\n\nX0\nX1\nX2\nX3\nX4\nT\nS0\nS1\nS2\nS3\nS4\nS5\nS6\nS7\nY\n\n\n\n\n0\n-0.096380\n0.840839\n-1.053685\n-0.161607\n-1.464074\n0.0\n1.140716\n0.460239\n-0.243707\n0.478655\n0.722508\n0.119872\n-0.491021\n0.657103\n1\n\n\n1\n0.750790\n-0.723678\n1.339998\n-0.109216\n-0.886968\n0.0\n0.173273\n-0.100168\n0.150519\n-0.828812\n0.288891\n-0.454165\n0.356069\n-1.335292\n1\n\n\n2\n1.135441\n-1.586930\n1.226992\n1.028425\n-0.628964\n0.0\n-0.136527\n-0.151602\n0.555157\n-1.382398\n-0.071426\n-0.453651\n0.506445\n-1.059432\n0\n\n\n3\n0.470862\n-0.472840\n0.295874\n1.280656\n-0.370790\n0.0\n0.172303\n0.171164\n0.069458\n-0.738659\n0.113751\n-0.089416\n0.115741\n0.073674\n0\n\n\n4\n0.299083\n-0.566013\n0.851715\n0.816545\n-0.128058\n0.0\n-0.072062\n0.078149\n0.067890\n-0.761437\n0.052046\n-0.173179\n0.309554\n-0.506578\n0\n\n\n\n\n\n\n\nThe underlying simulation code is below if you’re interested\n\n\nCode\nfrom typing import Dict\n\ndef transmitter(\n    params: Dict,\n    add_treatment: bool = False,\n    n_users: int = 1,\n    logit_link: bool = False,\n    rng = None\n) -&gt; pd.DataFrame:\n    '''Simulates outcomes based on some ground truth parameters. \n\n    Parameters\n    -----------\n        params: The ground truth parameters (effects and biases) to simulate based off of\n        add_treatment: adds a randomly allocated treatment when true, with effect `bTS`\n        n_users: The number of users to simulate\n        logit_link: whether the data generating process is a bernoulli outcome or not\n        rng: A numpy random generator \n\n    Returns\n    --------\n        A pandas dataframe with simulated data, including pre-treatment covariates,\n         surrogate outcomes, a treatment indicator, and a long term outcome, Y\n    '''\n    if rng is None:\n        seed = np.random.choice(range(1000))\n        rng = np.random.default_rng(seed=seed)\n\n    # unpack params\n    Zdims, Xdims, Sdims =  params['bZX'].shape[1],  params['bZX'].shape[0],  params['bXS'].shape[0]\n    alphaX,alphaS,alphaY  = params['alphaX'], params['alphaS'], params['alphaY'] # bias terms\n    bZX,bXS,bSY = params['bZX'], params['bXS'],params['bSY'] # causal relationships\n    bTS,bXTS = params['bTS'], params['bXTS'] # tx effects\n\n    # unobserved variable Z representing latent customer traits\n    Z = rng.normal(0,1, size=(Zdims, n_users))\n\n    # Some observed pre-TX measures\n    X = alphaX[:,None] + (bZX @ Z)\n\n    # Intermediate outcomes\n    S = alphaS[:,None] + (bXS @ X) \n\n    # Add in treatment effect if applicable\n    T = rng.binomial(1,0.5,size=n_users) if add_treatment else np.zeros(n_users)        \n    avg_tx_term = (bTS * T[:,None])        \n    hetergeneous_tx_term = (bXTS @ (X*T))\n    S += avg_tx_term.T + hetergeneous_tx_term\n\n    # expectation of long term outcome\n    eta = 0 + (bSY @ S)\n\n    # Long term outcome\n    if logit_link:\n        Y = rng.binomial(1, sp.expit(eta) )\n    else:\n        Y = rng.normal(eta, 0.025)\n\n    # Output as dataframe\n    Xdf = pd.DataFrame(X.T, columns=[f'X{i}' for i in range(Xdims)]).assign(T=T)\n    Sdf = pd.DataFrame(S.T, columns=[f'S{i}' for i in range(Sdims)])\n    Ydf = pd.DataFrame(Y.ravel(), columns=['Y'])\n    return pd.concat((Xdf, Sdf, Ydf),axis=1)"
  },
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-2-fitting-a-surrogate-model",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-2-fitting-a-surrogate-model",
    "title": "Introduction to Surrogate Indexes",
    "section": "Step 2: Fitting a surrogate model",
    "text": "Step 2: Fitting a surrogate model\nWe’ll use the historical dataset to fit the surrogate index model, mapping \\(S \\rightarrow Y\\)\n\nS_vars = \" + \".join( historical_data.filter(like=\"S\").columns )\nX_vars = \" + \".join( historical_data.filter(like=\"X\").columns )\n\n# Step 2: fit a surrogate index model on complete historical data\nsurrogate_model = sm.OLS.from_formula(f\"Y ~ {S_vars}\", data=historical_data).fit()\n\n# Estimate the variance in the estimator, \\hat{sigma^2}. This is used for bias corrections later\npredicted_sigma2 = np.var( surrogate_model.fittedvalues - historical_data.Y,  ddof=1 )\n\n# Show the model summary\nsurrogate_model.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nY\nR-squared:\n0.112\n\n\nModel:\nOLS\nAdj. R-squared:\n0.112\n\n\nMethod:\nLeast Squares\nF-statistic:\n1261.\n\n\nDate:\nWed, 05 Mar 2025\nProb (F-statistic):\n0.00\n\n\nTime:\n15:34:16\nLog-Likelihood:\n-33320.\n\n\nNo. Observations:\n50000\nAIC:\n6.665e+04\n\n\nDf Residuals:\n49994\nBIC:\n6.670e+04\n\n\nDf Model:\n5\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.4994\n0.002\n236.981\n0.000\n0.495\n0.504\n\n\nS0\n0.0382\n0.004\n10.857\n0.000\n0.031\n0.045\n\n\nS1\n0.1023\n0.005\n21.446\n0.000\n0.093\n0.112\n\n\nS2\n-0.0420\n0.005\n-8.853\n0.000\n-0.051\n-0.033\n\n\nS3\n0.1861\n0.004\n50.774\n0.000\n0.179\n0.193\n\n\nS4\n0.0501\n0.002\n30.302\n0.000\n0.047\n0.053\n\n\nS5\n0.0666\n0.002\n28.930\n0.000\n0.062\n0.071\n\n\nS6\n-0.0072\n0.002\n-2.978\n0.003\n-0.012\n-0.002\n\n\nS7\n0.0037\n0.003\n1.348\n0.178\n-0.002\n0.009\n\n\n\n\n\n\n\n\nOmnibus:\n255849.096\nDurbin-Watson:\n1.998\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5169.412\n\n\nSkew:\n-0.002\nProb(JB):\n0.00\n\n\nKurtosis:\n1.425\nCond. No.\n3.91e+15\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 6.03e-27. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nThere are 3 important things to note here:\n\nNote that we’re purposely not including the pre-treatment covariates, \\(X\\) in this model. Remember the DAG - all of the effect of \\(X\\) on \\(Y\\) is entirely mediated by \\(S\\), so adding \\(X\\) into the model adds no additional information.\nWe’re using Ordinary Least Squares for bernoulli outcome data. Thats not a mistake. OLS has great properties to be effective on bernoulli outcome data, and it makes this approach very simple. Other models can also be swapped in, like Random Forest or XGBoost.\nI’m not doing alot of model validation, just because this is simulated data. In practice, don’t just throw things into a model. Part 2 in this series will discuss how to validate surrogate indexes."
  },
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-3-estimate-long-term-treatment-effect",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-3-estimate-long-term-treatment-effect",
    "title": "Introduction to Surrogate Indexes",
    "section": "Step 3: Estimate Long Term Treatment Effect",
    "text": "Step 3: Estimate Long Term Treatment Effect\nWe’ll now use the surrogate index to estimate a long term treatment effect\nLet’s take our experiment dataset and estimate \\(E[\\delta_{\\text{LTO}}]\\), the average treatment effect on the long term outcome. Notice, the long term outcome, \\(Y\\), hasn’t been observed yet.\n\n\nCode\ndisplay(experiment_data.head())\n\n\n\n\n\n\n\n\n\nX0\nX1\nX2\nX3\nX4\nT\nS0\nS1\nS2\nS3\nS4\nS5\nS6\nS7\nY\n\n\n\n\n0\n0.849129\n0.941827\n0.683911\n1.435666\n1.001435\n1\n-0.559842\n-0.575105\n-0.884129\n-0.543576\n-0.289843\n0.816105\n-0.516732\n0.887577\nNaN\n\n\n1\n-0.053981\n0.716863\n0.718290\n1.600732\n0.390534\n1\n-0.271181\n0.001847\n-0.690713\n-0.609596\n0.001895\n0.902771\n-0.263201\n0.456430\nNaN\n\n\n2\n0.852506\n1.276305\n-1.803119\n-0.418187\n-0.009625\n1\n0.021439\n-0.752464\n-0.340972\n0.909959\n-0.323996\n1.041753\n-1.256339\n1.851033\nNaN\n\n\n3\n-0.650541\n0.908525\n1.985210\n1.231077\n0.265338\n1\n-0.226897\n0.259183\n-0.987532\n-0.835104\n0.359953\n0.767602\n0.108398\n-0.659191\nNaN\n\n\n4\n-0.473196\n-0.007258\n0.054155\n-0.368575\n0.331867\n0\n-0.229447\n0.020273\n0.058779\n0.200721\n-0.121112\n0.062693\n0.113565\n-0.206135\nNaN\n\n\n\n\n\n\n\nFirst, we’ll do some visulation of the experiment data.\n\n\nCode\nfig, axes = plt.subplots(2,int(Xdims/2),figsize=(7,5),sharey=True)\n\nfor i, ax in zip(range(Xdims), axes.ravel()):\n    sns.histplot( experiment_data.loc[lambda d: d['T']==0][f\"X{i}\"],ax=ax, label='Control' )\n    sns.histplot( experiment_data.loc[lambda d: d['T']==1][f\"X{i}\"],ax=ax, label='Treatment' )\n\nplt.suptitle(\"Histogram of Pre-Treatment Covariates\\nfor the Treatment and Control groups\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nAs we can see above, the pre-treatment variables are the exact same between the experiment groups. Thats because users are randomly allocated into treatment and control groups, and their pre-treatment varibles by definition are things not imapcted by the experiment.\nConversely, if we look at the surrogate outcomes below, we’ll see some differences in surrogate outcomes between the treatment and control groups.\n\n\nCode\nfig, axes = plt.subplots(2,int(Sdims/2),figsize=(7,5), sharey=True)\n\nfor i, ax in zip(range(Sdims), axes.ravel()):\n    sns.histplot( experiment_data.loc[lambda d: d['T']==0][f\"S{i}\"],ax=ax, label='Control' )\n    sns.histplot( experiment_data.loc[lambda d: d['T']==1][f\"S{i}\"],ax=ax, label='Treatment' )\n\nplt.suptitle(\"Histogram of Surrogate Outcomes\\nfor the Treatment and Control groups\")\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nIf our surrogate index estimator is correct, these observed surrogate outcomes should directly map to the Long Term Outcome deterministically, via \\(\\hat{Y} = f(S)\\), where \\(f()\\) is the surrogate index model.\nWe can show that the surrogate index estimator recovers the true average treatment effect on the long term outcome\n\ndef estimate_delta_LTO(experiment_data, surrogate_model, predicted_sigma2):\n    '''Accepts experiment data with a binary treatment, a surrogate model, and the predicted sigma^2 of the surrogate model.\n    Returns the ATE estimate and its uncertainty\n    \n    '''\n    Y_T1 = surrogate_model.predict(experiment_data.loc[lambda d: d['T']==1])\n    Y_T0 = surrogate_model.predict(experiment_data.loc[lambda d: d['T']==0])\n    \n    # Calculate the ATE\n    ATE =  Y_T1.mean() - Y_T0.mean()\n    # calculate the variance \n    var_surrogate = np.var(Y_T1,ddof=1) / len(Y_T1) + np.var(Y_T0,ddof=1) / len(Y_T0)\n    # Adjust the variance by the surrogate model error\n    var_surrogate_adj = var_surrogate + 2*predicted_sigma2/len(Y_T1)\n    ATE_sd = np.sqrt(var_surrogate_adj)\n    \n    return ATE, ATE_sd\n\nATE, ATE_sd = estimate_delta_LTO(experiment_data, surrogate_model, predicted_sigma2)\nsns.histplot(np.random.normal(ATE, ATE_sd, size=10000), stat='probability')\nplt.axvline( GROUND_TRUTH['ATE'], color='r', ls='--', label='True ATE')\nplt.legend()\nplt.xlabel(\"ATE\")\nplt.title(\"Estimated Treatment Effect vs. True Treatment Effect\")\nplt.show()\n\n\n\n\n\n\n\n\nThere we are. The surrogate estimator recovers the true average treatment effect! We didn’t even have to wait and observe the true long term outcome.\nIf you’re interested, try simulating this repeatedly to confirm it regularly recovers the true ATE with different random seeds. Even better, try setting the treatment effect to zero and see how often there are false positives."
  },
  {
    "objectID": "posts/pandera_schemas/2025-02-21-pandera-schemas.html",
    "href": "posts/pandera_schemas/2025-02-21-pandera-schemas.html",
    "title": "Pandera and Object Oriented Data Validation",
    "section": "",
    "text": "Code\nimport pandas as pd\n\n\n\nIntroduction\nPandera schema’s are a useful tool to make sure input data is as expected. If you’ve ever used dbt before, theyre just like schema tests or great-expectations.\nData validation is extremely important for just about everyone in the data science space. For instance, your model may not be expecting null values, or maybe it needs data to be iid. This is where Pandera can come in and excel, especially when you don’t have control over the input data.\n\n\nInheritance for reusing schemas\nI recently had a project where I wasn’t responsible for and had no control over the input data. For a particular model, many different datasets would be sent to different models for inference, and I couldn’t always rely on the data being properly structured.\nI was also working with a range of different models including time-varying survival and time series models. I wanted to be able to reuse as much code across models, which is where Pandera comes in.\nLets start with looking at some panel data:\n\npanel = pd.DataFrame({\n    \"entity_id\":[1,1,2,],\n    \"date\":['2023-01-01','2023-02-01','2023-01-01'],\n})\npanel.assign(metric=[0,1,1])\n\n\n\n\n\n\n\n\nentity_id\ndate\nmetric\n\n\n\n\n0\n1\n2023-01-01\n0\n\n\n1\n1\n2023-02-01\n1\n\n\n2\n2\n2023-01-01\n1\n\n\n\n\n\n\n\nIt turns out Time Series data is typically going to be panel data (although not always true). Time-varying survival data is also a special case of panel data, where each entity (for instance a customer’s subscription id) starts at some point at there are consistently spaced observations.\nClass inheritance is a perfect pattern to reflect this. We can do something like the following to reuse schema checks and save a lot of lines of code.\nclass PanelSchema:\n    ...\n\nclass TimeSeriesSchema(PanelSchema):\n    ...\n\nclass TimevaryingSurvivalSchema(PanelSchema):\n    ...\nFor example, we can make a panel schema with some obvious checks:\nclass BasePanelSchema(pa.DataFrameModel):\n    @classmethod\n    def infer_frequency(cls, df: pd.DataFrame) -&gt; str:\n        \"\"\"Identifies the frequency of dates in the panel dataset\"\"\"\n        ...\n\n    @classmethod\n    def validate(cls, df: pd.DataFrame, *args, **kwargs):\n        # Run and save dataset frequency before other validations\n        cls.Config.metadata[\"freq\"] = cls.infer_frequency(df)\n        return super().validate(df, *args, **kwargs)\n\n\nclass PanelSchema(BasePanelSchema):\n    \"\"\"Panel Data schema. This assumes that Panel Data has 1 entry per entity per date, and that all\n    dates between the min and the max date for an entity should exist as records, evenly spaced.\n    \"\"\"\n    entity_id: Series[str] = pa.Field(coerce=True, nullable=False)\n    date: Series[DateTime] = pa.Field(coerce=True, nullable=False)\n\n    class Config:\n        unique = [\"entity_id\", \"date\"]\n        strict = False\n        metadata: dict = {}\n\n    @pa.dataframe_check\n    def validate_frequency(cls, df: pd.DataFrame) -&gt; bool:\n        \"\"\"Ensures that every entity has the same frequency between dates of consecutive records\"\"\"\n        # Use the inferred frequency for some validation\n        ...\n\n    @pa.dataframe_check\n    def check_complete_date_index_per_entity(cls, df: DataFrame) -&gt; Series[bool]:\n        \"\"\"Ensures that all dates (at the applicable frequency) between the min and the max date for\n        an entity should exist as records.\n        \"\"\"\n        ...\nAs a starting point, this makes it really easy to extend to time series models.\n\n\nCode\n# Make a fake sales dataframe\npd.DataFrame({\n    \"entity_id\": 1,\n    \"date\": pd.date_range(start='2023-01-01', freq='MS', periods=4),\n    \"y\":[100, 105, 96, 120]\n})\n\n\n\n\n\n\n\n\n\nentity_id\ndate\ny\n\n\n\n\n0\n1\n2023-01-01\n100\n\n\n1\n1\n2023-02-01\n105\n\n\n2\n1\n2023-03-01\n96\n\n\n3\n1\n2023-04-01\n120\n\n\n\n\n\n\n\nI can take my PanelSchema and inherit it for different data models. A simple example is lets say I need a non-null outcome:\nclass UnivariateTimeSeriesSchema(PanelSchema):\n    y: Series[float] = pa.Field(coerce=True, nullable=False)\nand just like that, I have all of those previous data checks extended to my time series model.\nThere are some downsides though. My biggest issue with this is that you can’t parameterize field names, for instance your outcome needs to be named y and you need to have a column named entity_id in this case.\nFor time-varying survival schema, they can be defined as\nclass TimeVaryingSurvivalSchema(PanelSchema):\n    \"\"\"A schema for Time Varying Survival Analysis, should be a unique and complete panel dataset\n    on the `entity_id` and `time` columns.\n    \"\"\"\n    tenure: Series[int] = pa.Field(ge=0, coerce=True, nullable=False)\n    event: Series[int] = pa.Field(isin=[0, 1], coerce=True, nullable=False)\n    exposure: Series[float] = pa.Field(le=1, ge=0, coerce=True, nullable=False)\n\n    @pa.dataframe_check\n    def check_tenure_is_consecutive(cls, df: DataFrame) -&gt; Series[bool]:\n        ...\n\n    @pa.dataframe_check\n    def check_max_one_event_per_entity(cls, df: DataFrame) -&gt; Series[bool]:\n        ...\n\n    @pa.dataframe_check\n    def check_event_is_last_obs_per_entity(cls, df: DataFrame) -&gt; Seris[bool]:\n        ...\nIn this case, it inherits all of the previous schema checks from PanelSchema. We extend it further to add some survival analysis schema specific checks - for each entity_id, we expect there to be a tenure column indicating each consecutive time period. A common example for tenure is the months since a customer started a subscription.\n\n\nCode\npanel.assign(\n    tenure=[0,1,0], \n    exposure=[1,0.5, 0.25],\n    event = [0, 1, 1]\n)\n\n\n\n\n\n\n\n\n\nentity_id\ndate\ntenure\nexposure\nevent\n\n\n\n\n0\n1\n2023-01-01\n0\n1.00\n0\n\n\n1\n1\n2023-02-01\n1\n0.50\n1\n\n\n2\n2\n2023-01-01\n0\n0.25\n1\n\n\n\n\n\n\n\nWhats great about this is that its easy to read and understand. Time Series and Time-varying survival are just sub-categories of panel data. And our code mimics that perfectly.\n\n\nConcluding Thoughts\nPandera has some nice use cases, particularly when you’re not sure what data you’re going to get thrown at you. The read-ability is fantastic, and it can save a ton of code.\nThat said there were some caveats I’ve run into (some of which I mentioned above):\n\nField names can’t be parameterized. For instance, maybe for Survival analysis I want the outcome to be named churn instead of event.\nWill this scale for massive datasets? I’m not sure if the code will be able to stay this clean at larger scales, especially for consecutive row based operations like validating frequencies. Not sure how that one will play out when the data needs to be batched.\nPandera isn’t the most flexible since you’re typically working with uninstantiated classes and class methods. There can be some growing pains for less experienced programmers\nThere is a really cool yaml output feature that lets you take a schema and get a yaml list of all of the checks it will run, but it doesn’t support custom checks and that basically makes it unusable.\nI’m not sure its a particularly active project\n\nOverall, its definitely a useful tool to keep in your back pocket, but I have two pieces of advice:\n\nIf you have the luxury of predictable data inputs or you’re modeling a single dataset over time, you probably don’t need this and can stick to dbt validation or something simple.\nIf you’re working on large datasets, maybe do a POC and make sure your schema approach will work for data thats loaded in batches.\n\nNote: I may update this later to have actual working code to demo, but for now it’s pseudocode."
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "",
    "text": "Code\nfrom typing import Optional, Callable, List, Dict\nimport numpy as np\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import inspect, Predictive, MCMC, NUTS\n\nimport jaxlib\nimport jax\nimport jax.numpy as jnp\nfrom jax import random\nfrom jaxtyping import Float, Array, Int\n\n\nimport arviz as az\nnumpyro.set_host_device_count(2)\nSEED: int = 99\n\nprint(\n    \"numpyro version:\", numpyro.__version__,\n    \"\\njax version:\", jax.__version__,\n    \"\\njaxlib version\", jaxlib.__version__,\n    \"\\nArviZ version: \", az.__version__\n)\n\n\nnumpyro version: 0.17.0 \njax version: 0.5.0 \njaxlib version 0.5.0 \nArviZ version:  0.20.0\nThe goal of this post is to figure out how to use numpyro internals to auto-label variable dimensions for ArviZ. PyMC is heavily integrated with ArviZ and their dimension labelling is fantastic - I want it to be just as easy with numpyro.\nThis isn’t going to be a very fun blog post, it’s meant to follow my thought process as I work through this problem. Maybe that’s helpful for some people? But mostly it will be a good reference for myself in the future."
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#what-are-coords-and-dims",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#what-are-coords-and-dims",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "What are coords and dims?",
    "text": "What are coords and dims?\nI’m assuming readers are familiar with this, but here is a quick refresher. ArviZ takes in a bayesian posterior and organizes it as an xarray dataset - picture pandas but for tensors. ArviZ expects coords and dims to map parameter dimensions to names and categories as show below\ncoords = {\n    \"category\": ['A', 'B', 'C', 'D', 'E'],\n    'features': ['X1', 'X1', 'X3']\n}\n\ndims = {\n    'beta': ['category', 'features'],\n    \"gamma\": ['category']\n}\nidata = az.from_numpyro(mcmc_object, coords=coords, dims=dims)\nThe coords map the category names to a positional index, where A corresponds to index 0, B corresponds to index 1, and so on.\nOur dims tell us that our gamma parameter is indexed by category. This would estimate a gamma parameter for each category, i.e. gamma[0] is the gamma estimate for category A. beta in this case has a separate estimate for each category {A, B, C, D, E} and for each feature {X1, X2, X3}.\nThis is helpful for so many reasons - plots are easier to read with the category names displayed for each parameter, and you can do operations using the dimension names\nThe code snippet below returns the average estimate for beta when the category=A, which is extremely easy to read\nbeta = idata.posterior['beta']\navg_beta_groupA = beta.sel(category='A').mean((\"chain\", \"draw\"))\nA properly labelled InferenceData object from ArviZ is a massive quality of life improvement."
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#numpyros-default-behavior",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#numpyros-default-behavior",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "Numpyro’s default behavior",
    "text": "Numpyro’s default behavior\nLet’s explore numpyro’s default behavior and see where it falls short. We’ll start with simulating some regression data\n\nrng = np.random.default_rng(SEED)\nN, n_feats = 1000, 3\nz_cardinality = 10\n\nalpha = 1.3\nbeta = rng.normal(size=n_feats)\ngamma = rng.normal(size=z_cardinality)\nsigma = 1\n\nX = rng.normal(size=(N, n_feats))\nz = rng.choice(range(z_cardinality), size=N)\n\nmu = alpha + np.dot(X, beta) + gamma[z]\ny = rng.normal(mu, sigma)\n\n# we'll make a coords dictionary for later\ncoords = {\n    \"Z\": ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'],\n    \"obs_id\": np.arange(N),\n    \"features\": ['X1', 'X2', 'X3'],\n}\n\nNext, lets fit a linear regression model and try and pass it into ArviZ to see what the dims for our parameters look like\n\n\nCode\ndef run_inference(\n    model: Callable,\n    key: random.PRNGKey,\n    num_warmup: int = 50, \n    num_chains: int = 2, \n    num_samples: int = 50,\n    **kwargs\n) -&gt; MCMC:\n    kernel = NUTS(model)\n    mcmc = MCMC(kernel, num_warmup=num_warmup, num_chains=num_chains, num_samples=num_samples, progress_bar=False)\n    mcmc.run(key, **kwargs)\n    return mcmc\n\ndef print_site_dims(idata, vars = ['alpha', 'beta', 'mu']):\n    for var in vars:\n        print(f\"{var}:\", idata.posterior[var].dims )\n\n\n\ndef model(\n    X: Float[Array, \"obs features\"], \n    Z: Float[Array, \" obs\"], \n    y: Float[Array, \" obs\"] = None\n) -&gt; Float[Array, \" obs\"]:\n\n    n_features = X.shape[-1]\n    n_groups = len(np.unique(Z))\n    \n    with numpyro.plate(\"Z\", n_groups):\n        alpha = numpyro.sample(\"alpha\", dist.Normal(0, 3))\n\n    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n    with numpyro.plate(\"features\", n_features):\n        beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n\n    with numpyro.plate(\"obs_id\", X.shape[0]):\n        mu = numpyro.deterministic(\"mu\", alpha[z] + jnp.dot(X, beta))\n        return numpyro.sample(\"y\", dist.Normal(mu, sigma))\n\nmcmc = run_inference(model, random.PRNGKey(0), X=X, Z=z, y=y)\nidata = az.from_numpyro(mcmc)\nprint_site_dims(idata)\n\nalpha: ('chain', 'draw', 'alpha_dim_0')\nbeta: ('chain', 'draw', 'beta_dim_0')\nmu: ('chain', 'draw', 'mu_dim_0')\n\n\nWhat we really want is for those dimensions labeled “{site}_dim_0” to be labelled by their plate names, so we need to see if there’s a way to automatically grab the plate metadata. Luckily there’s an inspect module in numpyro that seems like it might cover some of this."
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#pulling-model-relations",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#pulling-model-relations",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "Pulling model relations",
    "text": "Pulling model relations\n\nmodel_relations = inspect.get_model_relations(model, model_kwargs=dict(X=X, Z=z, y=y))\nmodel_relations\n\n{'sample_sample': {'alpha': [],\n  'sigma': [],\n  'beta': [],\n  'mu': ['alpha', 'beta'],\n  'y': ['sigma', 'mu']},\n 'sample_param': {'alpha': [], 'sigma': [], 'beta': [], 'mu': [], 'y': []},\n 'sample_dist': {'alpha': 'Normal',\n  'sigma': 'HalfNormal',\n  'beta': 'Normal',\n  'mu': 'Deterministic',\n  'y': 'Normal'},\n 'param_constraint': {},\n 'plate_sample': {'Z': ['alpha'], 'features': ['beta'], 'obs_id': ['mu', 'y']},\n 'observed': []}\n\n\nIt looks like plate_sample has exactly what we need, its just in the wrong order. It should really be formatted as\n{\n    \"alpha\": [\"Z\"]\n    \"beta\": [\"features\"],\n    \"mu\": [\"obs_id\"],\n    \"y\": [\"obs_id\"]\n}\nAll we need to do is make a quick dictionary reversing function\n\nfrom collections import defaultdict\ndef reverse_plate_mapping(plate_mapping: Dict[str, List[str]]) -&gt; Dict[str, List[str]]:\n    reversed_map = defaultdict(list)\n    for key, values in plate_mapping.items():\n        for value in values:\n            reversed_map[value].append(key)\n    return reversed_map\n\ndims = reverse_plate_mapping(model_relations['plate_sample'])\n\nidata = az.from_numpyro(mcmc, coords=coords, dims=dims)\nprint_site_dims(idata)\n\nalpha: ('chain', 'draw', 'Z')\nbeta: ('chain', 'draw', 'features')\nmu: ('chain', 'draw', 'obs_id')\n\n\nPerfect! Next lets make sure this works when a site is within multiple plates - I’m concerned that my approach for reversing the plate mapping wont get the order right when a site is within multiple plates"
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#does-our-approach-preserve-the-dimension-order",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#does-our-approach-preserve-the-dimension-order",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "Does our approach preserve the dimension order?",
    "text": "Does our approach preserve the dimension order?\nWe’ll have to simulate some new data where \\(\\beta\\) has group-specific effects. I’m going to make each feature have a group specific effect so that I can test if my solutions will work with nested plates.\n\n# Simulate new data\nbeta = rng.normal(size=(z_cardinality, n_feats))\nmu = alpha + (X * beta[z]).sum(-1) + gamma[z]\ny = rng.normal(mu, sigma)\n\ndef model2(\n    X: Float[Array, \"obs features\"], \n    Z: Float[Array, \" obs\"], \n    y: Float[Array, \" obs\"] = None\n) -&gt; Float[Array, \" obs\"]:\n\n    feature_plate = numpyro.plate(\"features\", X.shape[-1], dim=-2)\n    group_plate = numpyro.plate(\"Z\", len(np.unique(Z)), dim=-1)\n    \n    with group_plate:\n        alpha = numpyro.sample(\"alpha\", dist.Normal(0, 3))\n\n    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n    with group_plate, feature_plate:\n        beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n\n    with numpyro.plate(\"obs_id\", X.shape[0]):\n        mu = numpyro.deterministic(\"mu\", alpha[z] + (X*beta[:,z].T).sum(-1))\n        return numpyro.sample(\"y\", dist.Normal(mu, sigma))\n\nmcmc = run_inference(model2, random.PRNGKey(0), X=X, Z=z, y=y)\nidata = az.from_numpyro(mcmc)\nprint_site_dims(idata)\n\nalpha: ('chain', 'draw', 'alpha_dim_0')\nbeta: ('chain', 'draw', 'beta_dim_0', 'beta_dim_1')\nmu: ('chain', 'draw', 'mu_dim_0')\n\n\n\nmodel2_relations = inspect.get_model_relations(model2, model_kwargs=dict(X=X, Z=z, y=y))\nmodel2_relations\n\n{'sample_sample': {'alpha': [],\n  'sigma': [],\n  'beta': [],\n  'mu': ['alpha', 'beta'],\n  'y': ['sigma', 'mu']},\n 'sample_param': {'alpha': [], 'sigma': [], 'beta': [], 'mu': [], 'y': []},\n 'sample_dist': {'alpha': 'Normal',\n  'sigma': 'HalfNormal',\n  'beta': 'Normal',\n  'mu': 'Deterministic',\n  'y': 'Normal'},\n 'param_constraint': {},\n 'plate_sample': {'features': ['beta'],\n  'Z': ['alpha', 'beta'],\n  'obs_id': ['mu', 'y']},\n 'observed': []}\n\n\nThis is tricky. Beta should have dims (chain, draw, features, Z), but how do we know the plate_sample dictionary will return the correct order? What happens if we reverse the plate order?\n\n\nCode\ndef model2_reversed(\n    X: Float[Array, \"obs features\"], \n    Z: Float[Array, \" obs\"], \n    y: Float[Array, \" obs\"] = None\n) -&gt; Float[Array, \" obs\"]:\n\n    # reversed the plates\n    feature_plate = numpyro.plate(\"features\", X.shape[-1], dim=-1)\n    group_plate = numpyro.plate(\"Z\", len(np.unique(Z)), dim=-2)\n    \n    with group_plate:\n        alpha = numpyro.sample(\"alpha\", dist.Normal(0, 3))\n\n    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n    with feature_plate, group_plate:\n        beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n\n    with numpyro.plate(\"obs_id\", X.shape[0]):\n        mu = numpyro.deterministic(\"mu\", alpha[z].squeeze(-1) + (X*beta[z]).sum(-1))\n        return numpyro.sample(\"y\", dist.Normal(mu, sigma))\n\nmcmc_reversed = run_inference(model2_reversed, random.PRNGKey(0), X=X, Z=z, y=y)\n\n\n\nmodel2_reversed_relations = inspect.get_model_relations(model2_reversed, model_kwargs=dict(X=X, Z=z, y=y))\n\nprint(model2_relations['plate_sample'])\nprint(model2_reversed_relations['plate_sample'])\n\n{'features': ['beta'], 'Z': ['alpha', 'beta'], 'obs_id': ['mu', 'y']}\n{'features': ['beta'], 'Z': ['alpha', 'beta'], 'obs_id': ['mu', 'y']}\n\n\nThis is bad news - looking at the plate_samples’s above, we see the same result despite the plates having different dims. This approach isn’t going to maintain the plate order, so we’ll end up mislabelling dims in our previous approach."
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#finding-better-plate-metadata",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#finding-better-plate-metadata",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "Finding better plate metadata",
    "text": "Finding better plate metadata\nI took a closer look at the get_model_relations function and it looks like this is where they’re pulling plate information\ndef get_model_relations(model, model_args=None, model_kwargs=None):\n    ...\n\n    sample_plates = {\n        name: [frame.name for frame in site[\"cond_indep_stack\"]]\n        for name, site in trace.items()\n        if site[\"type\"] in [\"sample\", \"deterministic\"]\n    }\n\n    ...\nI’m going to try and see what other information is in there by copying their approach of pulling a model trace. Some helper functions are hidden in the code-fold below\n\n\nCode\nfrom functools import partial\nfrom numpyro import handlers\nfrom numpyro.infer.initialization import init_to_sample\nfrom numpyro.ops.pytree import PytreeTrace\n\nmodel_kwargs = dict(X=X, Z=z, y=y)\n\ndef _get_dist_name(fn):\n    if isinstance(\n        fn, (dist.Independent, dist.ExpandedDistribution, dist.MaskedDistribution)\n    ):\n        return _get_dist_name(fn.base_dist)\n    return type(fn).__name__\n\ndef get_trace(model, model_kwargs):\n    # We use `init_to_sample` to get around ImproperUniform distribution,\n    # which does not have `sample` method.\n    subs_model = handlers.substitute(\n        handlers.seed(model, 0),\n        substitute_fn=init_to_sample,\n    )\n    trace = handlers.trace(subs_model).get_trace(**model_kwargs)\n    # Work around an issue where jax.eval_shape does not work\n    # for distribution output (e.g. the function `lambda: dist.Normal(0, 1)`)\n    # Here we will remove `fn` and store its name in the trace.\n    for name, site in trace.items():\n        if site[\"type\"] == \"sample\":\n            site[\"fn_name\"] = _get_dist_name(site.pop(\"fn\"))\n        elif site[\"type\"] == \"deterministic\":\n            site[\"fn_name\"] = \"Deterministic\"\n    return PytreeTrace(trace)\n\n\n\ntrace = jax.eval_shape(partial(get_trace, model2, model_kwargs)).trace\ntrace['beta']\n\n{'args': (),\n 'intermediates': [],\n 'value': ShapeDtypeStruct(shape=(3, 10), dtype=float32),\n '_control_flow_done': True,\n 'type': 'sample',\n 'name': 'beta',\n 'kwargs': {'rng_key': None, 'sample_shape': ()},\n 'scale': None,\n 'is_observed': False,\n 'cond_indep_stack': [CondIndepStackFrame(name='features', dim=-2, size=3),\n  CondIndepStackFrame(name='Z', dim=-1, size=10)],\n 'infer': {},\n 'fn_name': 'Normal'}\n\n\nThis is perfect! For each site, we can see the plates they are nested within and the dimensions of those plates.\nWe also see for the reversed model, the order of the plates and the dim values follow a consistent pattern.\n\ntrace_reversed = jax.eval_shape(partial(get_trace, model2_reversed, model_kwargs)).trace\ntrace_reversed['beta']\n\n{'args': (),\n 'intermediates': [],\n 'value': ShapeDtypeStruct(shape=(10, 3), dtype=float32),\n '_control_flow_done': True,\n 'type': 'sample',\n 'name': 'beta',\n 'kwargs': {'rng_key': None, 'sample_shape': ()},\n 'scale': None,\n 'is_observed': False,\n 'cond_indep_stack': [CondIndepStackFrame(name='Z', dim=-2, size=10),\n  CondIndepStackFrame(name='features', dim=-1, size=3)],\n 'infer': {},\n 'fn_name': 'Normal'}\n\n\nWe should be able to make a working dims mapping from this for each site in our model.\nIt turns out that they already do this for us on L337\nsample_plates = {\n    name: [frame.name for frame in site[\"cond_indep_stack\"]]\n    for name, site in trace.items()\n    if site[\"type\"] in [\"sample\", \"deterministic\"]\n}\nThe working implementation is below (but could probably be cleaned up)\n\ndef get_site_dims(model: Callable, **kwargs):\n    def _get_dist_name(fn):\n        if isinstance(\n            fn, (dist.Independent, dist.ExpandedDistribution, dist.MaskedDistribution)\n        ):\n            return _get_dist_name(fn.base_dist)\n        return type(fn).__name__\n\n    def get_trace():\n        # We use `init_to_sample` to get around ImproperUniform distribution,\n        # which does not have `sample` method.\n        subs_model = handlers.substitute(\n            handlers.seed(model, 0),\n            substitute_fn=init_to_sample,\n        )\n        trace = handlers.trace(subs_model).get_trace(**kwargs)\n        # Work around an issue where jax.eval_shape does not work\n        # for distribution output (e.g. the function `lambda: dist.Normal(0, 1)`)\n        # Here we will remove `fn` and store its name in the trace.\n        for name, site in trace.items():\n            if site[\"type\"] == \"sample\":\n                site[\"fn_name\"] = _get_dist_name(site.pop(\"fn\"))\n            elif site[\"type\"] == \"deterministic\":\n                site[\"fn_name\"] = \"Deterministic\"\n        return PytreeTrace(trace)\n\n    # We use eval_shape to avoid any array computation.\n    trace = jax.eval_shape(get_trace).trace\n    sample_plates = {\n        name: [frame.name for frame in site[\"cond_indep_stack\"]]\n        for name, site in trace.items()\n        if site[\"type\"] in [\"sample\", \"deterministic\"]\n    }\n    return {k:v for k,v in sample_plates.items() if len(v) &gt; 0}\n\n\nidata = az.from_numpyro(\n    mcmc,\n    dims=get_site_dims(model2, **model_kwargs),\n    coords=coords\n)\n\nprint_site_dims(idata)\n\nalpha: ('chain', 'draw', 'Z')\nbeta: ('chain', 'draw', 'features', 'Z')\nmu: ('chain', 'draw', 'obs_id')"
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#a-quick-tangent-on-tensor-shapes-and-why-the-zerosumnormal-is-different",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#a-quick-tangent-on-tensor-shapes-and-why-the-zerosumnormal-is-different",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "A quick tangent on tensor shapes and why the ZeroSumNormal is different",
    "text": "A quick tangent on tensor shapes and why the ZeroSumNormal is different\nI recently ported the ZeroSumNormal distribution from PyMC to numpyro and it follows some different conventions than the typical distribution.\nTo understand it, you probably need to have some background on tensor shapes - theres a great blog from Eric Ma here.\nTypically in numpyro, when we put a sample site within a plate, the batch shape is determined by the plates. The batch shape is independent, but not identically distributed across dimensions. For instance in our previous alpha parameter we had 10 groups we stratified it by - we’re getting independent draws for alpha for each group.\nThe ZeroSumNormal instead works by using an event shape instead of a batch shape. Event shapes often have some dependency across dimensions, and in this case the zero sum constraint creates some dependency across the dimensions.\nWhile we typically might define a categorical parameter like this in numpyro:\nwith numpyro.plate(\"groups\", n_groups):\n    gamma = numpyro.sample(\"gamma\", dist.Normal(0,1))\nwith the ZeroSumNormal we define categorical parameters like this:\ngamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(n_groups,)))\nWe need to figure out how to have dims mapped for the ZeroSumNormal despite it not using a plate. The first test will be seeing what happens when we do nest it under a plate\n\ndef test_model():\n    n_groups = 10\n    with numpyro.plate(\"groups\", n_groups):\n        gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(n_groups,)))\n\nmcmc = run_inference(test_model, random.PRNGKey(0), num_chains=1)\nmcmc.get_samples()['gamma'].shape\n\n(50, 10, 10)\n\n\nIt looks like this unfortunately doesnt work, it creates a batch_shape=10, event_shape=10 when we only want the event_shape in this case."
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#a-custom-primitive-for-labelling-event-dims",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#a-custom-primitive-for-labelling-event-dims",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "A custom primitive for labelling event dims",
    "text": "A custom primitive for labelling event dims\nThis is going to be a problem that I’m not sure is solve-able with the current tools. But what if we could create a primitive that could save dim names for us in the trace without actually doing anything? ie\n# option 1\nwith pseudo_plate(\"groups\", n_groups):\n    gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(n_groups,)))\n\n# option 2\nwith site_labeller(\"groups\"):\n    gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(n_groups,)))\nThe main idea is that we could store information thats retrieavable in the trace, mimicking plates but without actually expanding the parameter shape like a plate would\nBelow is an implementation and a quick test model to make sure it has the correct shape saved\n\nfrom numpyro.primitives import Messenger, Message, CondIndepStackFrame\n\nclass pseudo_plate(numpyro.plate):\n    \n    def __init__(\n        self,\n        name: str,\n        size: int,\n        subsample_size: Optional[int] = None,\n        dim: Optional[int] = None,\n    ) -&gt; None:\n        self.name = name\n        self.size = size\n        if dim is not None and dim &gt;= 0:\n            raise ValueError(\"dim arg must be negative.\")\n        self.dim, self._indices = self._subsample(\n            self.name, self.size, subsample_size, dim\n        )\n        self.subsample_size = self._indices.shape[0]\n\n    # We'll try only adding our pseudoplate to the CondIndepStack without doing anything else\n    def process_message(self, msg: Message) -&gt; None:\n        cond_indep_stack: list[CondIndepStackFrame] = msg[\"cond_indep_stack\"]\n        frame = CondIndepStackFrame(self.name, self.dim, self.subsample_size)\n        cond_indep_stack.append(frame)\n\n\ndef test_model():\n    n_groups = 10\n    with pseudo_plate(\"groups\", 0):\n        gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(n_groups,)))\n\nmcmc = run_inference(test_model, random.PRNGKey(0), num_chains=1)\nmcmc.get_samples()['gamma'].shape\n\n(50, 10)\n\n\nAs we can see above, we got the correct shape for gamma. Lets see if this works in a model\n\ndef model3(\n    X: Float[Array, \"obs features\"], \n    Z: Float[Array, \" obs\"], \n    y: Float[Array, \" obs\"] = None,\n    try_pseudo_plate = True,\n) -&gt; Float[Array, \" obs\"]:\n\n    feature_plate = numpyro.plate(\"features\", X.shape[-1], dim=-2)\n    group_plate = numpyro.plate(\"Z\", len(np.unique(Z)), dim=-1)\n    \n    alpha = numpyro.sample(\"alpha\", dist.Normal(0, 3))\n    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n    with group_plate, feature_plate:\n        beta = numpyro.sample(\"beta\", dist.Normal(0, 1))\n    if try_pseudo_plate:\n        with pseudo_plate(\"groups\", 0):\n            gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(group_plate.size,)))\n    else:\n        gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(group_plate.size,)))\n\n    with numpyro.plate(\"obs_id\", X.shape[0]):\n        mu = numpyro.deterministic(\"mu\", alpha + gamma[z] + (X*beta[:,z].T).sum(-1))\n        return numpyro.sample(\"y\", dist.Normal(mu, sigma), obs=y)\n\nmcmc_zsn_pseudoplate = run_inference(model3, random.PRNGKey(0), num_chains=2, num_warmup=1000, num_samples=1000, **model_kwargs)\nidata_zsn_pseudoplate = az.from_numpyro(\n    mcmc_zsn_pseudoplate,\n    dims=get_site_dims(model3, **model_kwargs),\n    coords=coords\n)\n\nprint_site_dims(idata_zsn_pseudoplate, vars=['alpha', 'beta', 'gamma', 'mu'])\n\nalpha: ('chain', 'draw')\nbeta: ('chain', 'draw', 'features', 'Z')\ngamma: ('chain', 'draw', 'groups')\nmu: ('chain', 'draw', 'obs_id')\n\n\nThis seemed to work! But are there any unintended consequences of having another “plate” in the CondIndepStack? Lets fit a second version of the model without the pseudo_plate and see if they return similar results\n\nmcmc_zsn = run_inference(model3, random.PRNGKey(0), num_chains=2, num_warmup=1000, num_samples=1000, **model_kwargs, try_pseudo_plate=False)\n\nWe’ll check that parameter estimation for the site is the same\n\nimport matplotlib.pyplot as plt\naz.plot_forest(\n    [mcmc_zsn_pseudoplate, mcmc_zsn],\n    model_names=['With Pseudo Plate', 'Without Pseudo Plate'],\n     var_names=['gamma']\n)\nplt.title(\"Gamma Estimates Across both Models\")\nplt.show()\n\n\n\n\n\n\n\n\nAnd more importantly, we’ll make sure this doesnt impact the final likelihood estimate\n\nidata_zsn_pseudoplate = az.from_numpyro(mcmc_zsn_pseudoplate)\nidata_zsn = az.from_numpyro(mcmc_zsn)\n\ndelta = idata_zsn.log_likelihood['y'] - idata_zsn_pseudoplate.log_likelihood['y']\n\nprint(\"Difference in Log Likelihood with and without the pseudoplate:\", float(delta.mean()))\n\nDifference in Log Likelihood with and without the pseudoplate: 0.0\n\n\nI’m a bit shocked to say that this works so far."
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#another-edge-case-both-batch-and-event-dims",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#another-edge-case-both-batch-and-event-dims",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "Another edge case: both batch and event dims",
    "text": "Another edge case: both batch and event dims\nNext we need to make sure it works if we also have a batch dimension\n\ndef test_model_batches():\n    n_groups = 10\n    with pseudo_plate(\"groups\", n_groups), numpyro.plate(\"batches\", 3):\n        gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(n_groups,)))\n\ntrace = jax.eval_shape(partial(get_trace, test_model_batches, model_kwargs={})).trace\ntrace['gamma']['value']\n\nShapeDtypeStruct(shape=(3, 1, 10), dtype=float32)\n\n\nSadly this didnt quite work - it seems like the dim-handling isn’t working as we want it to because the batch shape is (3,1) instead of just (3,) - this is because of the way we aranged the plates, we put the batches in the position of dim=-2. Lets look at the trace\n\ntrace['gamma']\n\n{'args': (),\n 'intermediates': [],\n 'value': ShapeDtypeStruct(shape=(3, 1, 10), dtype=float32),\n '_control_flow_done': True,\n 'type': 'sample',\n 'name': 'gamma',\n 'kwargs': {'rng_key': None, 'sample_shape': ()},\n 'scale': None,\n 'is_observed': False,\n 'cond_indep_stack': [CondIndepStackFrame(name='batches', dim=-2, size=3),\n  CondIndepStackFrame(name='groups', dim=-1, size=10)],\n 'infer': {},\n 'fn_name': 'ZeroSumNormal'}"
  },
  {
    "objectID": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#improving-the-primitive-to-store-new-metadata",
    "href": "posts/numpyro_dim_labelling/2025-02-23-numpyro-dim_labels.html#improving-the-primitive-to-store-new-metadata",
    "title": "Automatic Dim Labelling with Numpyro?",
    "section": "Improving the primitive to store new metadata",
    "text": "Improving the primitive to store new metadata\nTaking a step back, it’s clear I thought about this wrong - the Conditional Indepdence Stack is for batch dimensions that are conditionally independent, and by putting our pseudoplate in there, we’re messing with how the batch dimensions are arranged.\nIt’s also hard to follow exactly how the conditional indepence stack is used throughout the code, and if that will have any unexpected consequences.\nI’m going to try something new - lets make a new primitive that creates a dep_stack and we’ll put the pseudo plates in there and see if it fixes things. This is going to get even more ugly so I’m hiding the implementation below in a code fold\n\n\nCode\nfrom typing import Tuple\nfrom collections import namedtuple\nimport numpyro\n\nfrom numpyro.primitives import Messenger, Message, apply_stack, _subsample_fn\nfrom numpyro.util import find_stack_level\n\nDepStackFrame = namedtuple(\"DepStackFrame\", [\"name\", \"dim\", \"size\"])\n\n\nclass pseudo_plate(numpyro.plate):\n    \n    def __init__(\n        self,\n        name: str,\n        size: int,\n        subsample_size: Optional[int] = None,\n        dim: Optional[int] = None,\n    ) -&gt; None:\n        self.name = name\n        self.size = size\n        if dim is not None and dim &gt;= 0:\n            raise ValueError(\"dim arg must be negative.\")\n        self.dim, self._indices = self._subsample(\n            self.name, self.size, subsample_size, dim\n        )\n        self.subsample_size = self._indices.shape[0]\n\n    # We'll try only adding our pseudoplate to the CondIndepStack without doing anything else\n    def process_message(self, msg: Message) -&gt; None:\n        if msg[\"type\"] not in (\"param\", \"sample\", \"plate\", \"deterministic\"):\n            if msg[\"type\"] == \"control_flow\":\n                raise NotImplementedError(\n                    \"Cannot use control flow primitive under a `plate` primitive.\"\n                    \" Please move those `plate` statements into the control flow\"\n                    \" body function. See `scan` documentation for more information.\"\n                )\n            return\n\n        if (\n            \"block_plates\" in msg.get(\"infer\", {})\n            and self.name in msg[\"infer\"][\"block_plates\"]\n        ):\n            return\n            \n        frame = DepStackFrame(self.name, self.dim, self.subsample_size)\n        msg['dep_stack'] = msg.get('dep_stack', []) + [frame]\n\n    def _get_event_shape(self, dep_stack: List[DepStackFrame]) -&gt; Tuple[int]:\n        n_dims = max(-f.dim for f in dep_stack)\n        event_shape = [1] * n_dims\n        for f in dep_stack:\n            event_shape[f.dim] = f.size\n        return tuple(event_shape)\n\n    # We need to make sure dims get aranged properly when there are multiple plates\n    @staticmethod\n    def _subsample(name, size, subsample_size, dim):\n        msg = {\n            \"type\": \"plate\",\n            \"fn\": _subsample_fn,\n            \"name\": name,\n            \"args\": (size, subsample_size),\n            \"kwargs\": {\"rng_key\": None},\n            \"value\": (\n                None\n                if (subsample_size is not None and size != subsample_size)\n                else jnp.arange(size)\n            ),\n            \"scale\": 1.0,\n            \"cond_indep_stack\": [],\n            \"dep_stack\": [],\n        }\n        apply_stack(msg)\n        subsample = msg[\"value\"]\n        subsample_size = msg[\"args\"][1]\n        if subsample_size is not None and subsample_size != subsample.shape[0]:\n            warnings.warn(\n                \"subsample_size does not match len(subsample), {} vs {}.\".format(\n                    subsample_size, len(subsample)\n                )\n                + \" Did you accidentally use different subsample_size in the model and guide?\",\n                stacklevel=find_stack_level(),\n            )\n        dep_stack = msg[\"dep_stack\"]\n        occupied_dims = {f.dim for f in dep_stack}\n        if dim is None:\n            new_dim = -1\n            while new_dim in occupied_dims:\n                new_dim -= 1\n            dim = new_dim\n        else:\n            assert dim not in occupied_dims\n        return dim, subsample\n\n\nNote that I’m copying alot of the code above directly from the plate primitive.\n\ndef test_model_batches():\n    n_groups = 10\n    with pseudo_plate(\"groups\", n_groups), numpyro.plate(\"batches\", 3):\n        gamma = numpyro.sample(\"gamma\", dist.ZeroSumNormal(1, event_shape=(n_groups,)))\n\n\ntrace = jax.eval_shape(partial(get_trace, test_model_batches, model_kwargs={})).trace\ntrace['gamma']['value']\n\nShapeDtypeStruct(shape=(3, 10), dtype=float32)\n\n\nPerfect this now works, we just need to combine our two plate stacks!\n\ntrace['gamma']['cond_indep_stack'] + trace['gamma']['dep_stack']\n\n[CondIndepStackFrame(name='batches', dim=-1, size=3),\n DepStackFrame(name='groups', dim=-1, size=10)]"
  },
  {
    "objectID": "posts/experiment_design.html",
    "href": "posts/experiment_design.html",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "",
    "text": "This blog post introduces the Potential Outcomes Model and introduces why experiments are often necessary to measure what we want. This topic is already covered extensively in other more rigorous resources. This post provides just another example."
  },
  {
    "objectID": "posts/experiment_design.html#a-hypothetical-world",
    "href": "posts/experiment_design.html#a-hypothetical-world",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "A Hypothetical world",
    "text": "A Hypothetical world\nWhat if we envision some hypothetical world we can observe the outcome for each customer who reached out to customer support, with and without having the treatment of receiving a promo?\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport scipy.special as sp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nrng = np.random.default_rng(100)\n\nN=100_000\nupset = rng.normal(0, 1, N)\n\ndef sim_treatment(upset, rng, return_prob=False):\n    beta = 1.5\n    p_treatment = sp.expit( -2.5 + upset * beta)\n    if return_prob:\n        return p_treatment\n    return rng.binomial(1, p_treatment)\n\ndef sim_outcome(upset, treatment, rng):\n    eps = rng.normal(0, 150, size=len(upset))\n    ltv = 2500 + 500*treatment + -500*upset + eps \n    return ltv.round(2)\n\ndata = pd.DataFrame({\n    \"Person\": np.arange(N),\n    \"upset\": upset,\n    \"T\": sim_treatment(upset, rng),\n    \"Y(0)\": sim_outcome(upset, np.zeros(N), rng),\n    \"Y(1)\": sim_outcome(upset, np.ones(N), rng)\n}).set_index(\"Person\")\\\n  .assign(ITE = lambda d: d[\"Y(1)\"] - d[\"Y(0)\"])\\\n  .assign(Y = lambda d: np.where(d[\"T\"] == 1, d[\"Y(1)\"], d[\"Y(0)\"]) )\n\ndata.head()[[\"T\", \"Y(0)\", \"Y(1)\", \"ITE\"]]\n\n\n\n\n\n\n\n\n\nT\nY(0)\nY(1)\nITE\n\n\nPerson\n\n\n\n\n\n\n\n\n0\n0\n3108.62\n3583.87\n475.25\n\n\n1\n0\n2347.01\n2878.23\n531.22\n\n\n2\n1\n2176.28\n2379.30\n203.02\n\n\n3\n0\n2146.09\n2559.96\n413.87\n\n\n4\n0\n2806.50\n3623.16\n816.66\n\n\n\n\n\n\n\nAs shown above, in this hypothetical world we can see the exact individual treatment effect (ITE) for every customer.\n- Person 0 would have spent $475.25 more over their lifetime  if they received the promo\n- Person 2 would have spend $203.02 more over their lifetime if they received the promo\nIf we want to know the Average Treatment Effect (ATE, often denoted \\(\\tau\\)), all we have to do is take the mean of all of the individual treatment effects. As we can see, the ATE is about $500\n\\[\n\\tau = \\frac{1}{N} \\sum^{N}_{i=0} Y_i(1) - Y_i(0)\n\\]\n\ndata.ITE.mean()\n\n500.09949529999994\n\n\nWe can also represent this in hypothetical terms that will be useful later - the average treatment effect of the treated (ATT), and the average treatment effect of the untreated (ATU). The true ATE ends up being the weighted average of these terms, weighted by the proportion of individuals seeing the treatment, \\(\\pi\\)\n\\[\n\\begin{align}\n\\tau & = \\pi \\cdot E[\\tau | T=1] + (1-\\pi) \\cdot E[\\tau | T= 0] \\\\\n     & = \\pi \\cdot \\text{ATT} + (1-\\pi) \\cdot \\text{ATU}\n\\end{align}\n\\]\nWe can confirm that this is equivalent to the ATE from above with code\n\npi = data[\"T\"].value_counts(normalize=True)\n(pi * data.groupby(\"T\").mean()[\"ITE\"]).sum()\n\n500.0994953"
  },
  {
    "objectID": "posts/experiment_design.html#getting-hit-with-the-real-world",
    "href": "posts/experiment_design.html#getting-hit-with-the-real-world",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Getting hit with the real world",
    "text": "Getting hit with the real world\nSo how can we create a scenario where we can observe each person with and without having received the promo? Sadly, we can’t. But is there a way to make use of data we already have? Here’s the actual data we might have access to. Notice that now the hypothetical potential outcomes are no longer visible (just like in the real world).\n\n\nCode\n# Real world data\ndf = (\n    data[[\"upset\", \"T\", \"Y(0)\", \"Y(1)\", \"ITE\", \"Y\"]]\n    .assign(**{\n        \"Y(0)\":lambda d: np.where(d[\"T\"]==1, np.NaN, d[\"Y(0)\"]),\n        \"Y(1)\":lambda d: np.where(d[\"T\"]==0, np.NaN, d[\"Y(1)\"]),\n        \"ITE\": np.NAN\n        })\n)\n\ndf.iloc[:,1:].head()\n\n\n\n\n\n\n\n\n\nT\nY(0)\nY(1)\nITE\nY\n\n\nPerson\n\n\n\n\n\n\n\n\n\n0\n0\n3108.62\nNaN\nNaN\n3108.62\n\n\n1\n0\n2347.01\nNaN\nNaN\n2347.01\n\n\n2\n1\nNaN\n2379.3\nNaN\n2379.30\n\n\n3\n0\n2146.09\nNaN\nNaN\n2146.09\n\n\n4\n0\n2806.50\nNaN\nNaN\n2806.50\n\n\n\n\n\n\n\nOne (unfortunately incorrect) idea might be take the average of Y(1) and subtract the average of Y(0), also known as the simple difference in outcomes (SDO).\n\\[\n\\text{SDO} = E[ Y(1) | T = 1 ] - E[ Y(0) | T = 0 ]\n\\]\n\nNotice that I use the terms \\(E[ Y(0) | T = 0 ]\\) and \\(E[ Y(1) | T = 1 ]\\). Reading these as plain english “the expected value (aka mean) of Y(0) given no treatment” and “the expected value (aka mean) of Y(1) given a treatment”\n\n\n(\n    df.groupby(\"T\")\n    .mean()[[\"Y\"]].T\n    .assign(tau = lambda d: d[1] - d[0])\n    .rename(columns={0:\"E[ Y(0) | T = 0 ]\", 1:\"E[ Y(1) | T = 1 ]\"})\n    .rename_axis(None, axis=1)\n    .round(2)\n    .reset_index(drop=True)\n)\n\n\n\n\n\n\n\n\nE[ Y(0) | T = 0 ]\nE[ Y(1) | T = 1 ]\ntau\n\n\n\n\n0\n2579.46\n2491.48\n-87.98\n\n\n\n\n\n\n\nUnder the SDO it looks like the treatment has a negative effect - this is saying that giving customers a promo makes their LTV $88 worse? That seems seriously wrong, and is a huge problem. It should be $500 like we saw in our hypothetical world. So what went wrong?"
  },
  {
    "objectID": "posts/experiment_design.html#selection-bias",
    "href": "posts/experiment_design.html#selection-bias",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Selection Bias",
    "text": "Selection Bias\nWe can illustrate the problem by bringing another variable into the mix - customer unhappiness (we’re pretending we can measure it directly for examples sake).\n\nfig, ax = plt.subplots(1,2, figsize=(8,3))\nax[0].set_title(\"Histogram of\\nCustomer unhappiness\")\ndf.upset.hist(ax=ax[0])\n\nax[1].set_title(\"More upset customers are\\nmore likely to receive a promo\")\nax[1].set_ylabel(\"Proportion Receiving Promo\")\ndf.groupby(df.upset//0.25*0.25).mean()[\"T\"].plot(ax=ax[1])\nplt.tight_layout()\n\ndf.head()\n\n\n\n\n\n\n\n\nupset\nT\nY(0)\nY(1)\nITE\nY\n\n\nPerson\n\n\n\n\n\n\n\n\n\n\n0\n-1.157550\n0\n3108.62\nNaN\nNaN\n3108.62\n\n\n1\n0.289756\n0\n2347.01\nNaN\nNaN\n2347.01\n\n\n2\n0.780854\n1\nNaN\n2379.3\nNaN\n2379.30\n\n\n3\n0.543974\n0\n2146.09\nNaN\nNaN\n2146.09\n\n\n4\n-0.961383\n0\n2806.50\nNaN\nNaN\n2806.50\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt looks like the most unhappy customers are the most likely to receive a treatment as shown in the DAG below.\n\n\n\n\n\n\n\nG\n\n\n\na\n\nunhappy customer\n\n\n\nb\n\nreceive promo\n\n\n\na-&gt;b\n\n\n\n\n\nc\n\nlifetime value\n\n\n\na-&gt;c\n\n\n\n\n\nb-&gt;c\n\n\n\n\n\n\n\n\n\n\nThis is an example of selection bias (more specifically, its collider bias, a common confound). When comparing customers who had the treatment vs. didnt have the treatment, we accidentally also end up comparing unhappy customers vs. happier customers, and obviously unhappier customers tend to have worse lifetime value. We need to find a way to compare the impact of the treatment while controlling for the happiness of customers so that we are making a more fair comparison. For example, if we had 2 equally unhappy customers and 1 received the treatment while the other didnt, we’d get a more reasonable comparison for evaluating the treatment effect."
  },
  {
    "objectID": "posts/experiment_design.html#identification-under-selection-bias",
    "href": "posts/experiment_design.html#identification-under-selection-bias",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Identification Under Selection bias",
    "text": "Identification Under Selection bias\nHow can we represent the scenario above with math? This is where the Potential Outcomes model starts coming into play. Note I’m borrowing this directly from Scott Cunningham. For the full proof, see his book, Causal Inference the Mixtape.\n\\[\n\\begin{align}\n\\text{Simple Difference in Outcomes}\n&= \\underbrace{E[Y(1)] - E[Y(0)]}_{ \\text{Average Treatment Effect}}\\\\\n&+ \\underbrace{E\\big[Y(0)\\mid T=1\\big] - E\\big[Y(0)\\mid T=0\\big]}_{ \\text{Selection bias}}\\\\\n& + \\underbrace{(1-\\pi)(ATT - ATU)}_{ \\text{Heterogeneous treatment effect bias}}\n\\end{align}\n\\]\nThis equation for the Potential Outcomes model basically says that anytime you make a comparison on observational data, it ends up being the sum of the true average treatment effect, selection bias, and Heterogeneous Treatment effect (HTE) bias. HTEs are just a fancy way of saying the personalized effect, aka promos might be more impactful for some users than others.\nSo how does this relate to what we did before? Well when we tried to compare users who saw the treatment vs. those that didnt\n\\[\n\\text{SDO} = E[ Y(1) | T = 1 ] - E[ Y(0) | T = 0 ]\n\\]\nwe didnt take into account the fact that users who saw the treatment tend to be different than those who didn’t. Users who saw the treatment tend to be more unhappy by design.\nSo if we subtract out the selection bias from the SDO (I got this via simple algebra), aka we control for the unhappiness between customers, we can get closer to identifying the true ATE.\nNote that selection bias was \\[\nE\\big[Y(0)\\mid T=1\\big] - E\\big[Y(0)\\mid T=0\\big]\n\\]\nThis is just saying selection bias is the fundamental difference between users who get picked for treatment vs. those who dont.\nIn our case, the fundamental difference between whether users are selected for treatment is based upon their unhappiness. So if we can subtract out the effect of unhappiness, we can subtract out the selection bias\n\ndf.groupby(\"T\").mean()[[\"upset\"]].T\n\n\n\n\n\n\n\nT\n0\n1\n\n\n\n\nupset\n-0.159046\n1.018004\n\n\n\n\n\n\n\nWe can do this with OLS. The most obvious way is to fit a model relating unhappiness to LTV, and then subtract out that effect.\n\nmodel1 = sm.OLS.from_formula(\"Y ~ upset\", data=df.loc[lambda d: d[\"T\"]==0]).fit()\nY0_hat = model1.predict(df)\n\nselection_bias = (\n    df.assign(selection_bias = Y0_hat)\n    .groupby(\"T\").mean()\n    [[\"selection_bias\"]]\n)\nselection_bias.T.round(2)\n\n\n\n\n\n\n\nT\n0\n1\n\n\n\n\nselection_bias\n2579.46\n1990.96\n\n\n\n\n\n\n\nAnd finally we can subtract out the effect, ending up with an estimate very close to the true ATE of 500\n\n(\n    df.assign(selection_bias = Y0_hat)\n    .groupby(\"T\").mean()[[\"Y\", \"selection_bias\"]].T\n    .assign(difference = lambda d: d[1] - d[0])\n    [[\"difference\"]].T\n    .reset_index(drop=True)\n    .rename(columns={\"Y\":\"SDO\"})\n    .assign(tau = lambda d: d.SDO - d.selection_bias)\n    .round(2)\n)\n\n\n\n\n\n\n\n\nSDO\nselection_bias\ntau\n\n\n\n\n0\n-87.98\n-588.5\n500.52\n\n\n\n\n\n\n\nThere’s actually an even more simple way to control for selection bias - it can just be included as a term in an OLS regression model.\n\ndef statsmodels_to_df(model):\n    table = np.array(model.summary().tables[1].data)\n    return pd.DataFrame(table[1:, 1:], columns=table[0,1:], index=table[1:,0])\n\nmodel2 = sm.OLS.from_formula(\" Y ~ T + upset\", data=df).fit()\nstatsmodels_to_df(model2)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\n\n\nIntercept\n2499.9363\n0.516\n4847.317\n0.000\n2498.925\n2500.947\n\n\nT\n500.5529\n1.502\n333.191\n0.000\n497.608\n503.497\n\n\nupset\n-500.0068\n0.518\n-965.940\n0.000\n-501.021\n-498.992\n\n\n\n\n\n\n\nAs we can see above the estimate of the treatment effect is the beta coefficient for T and it closely matches our manual estimate above."
  },
  {
    "objectID": "posts/experiment_design.html#a-quick-note-on-heterogeneous-treatment-effects",
    "href": "posts/experiment_design.html#a-quick-note-on-heterogeneous-treatment-effects",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "A quick note on Heterogeneous Treatment Effects",
    "text": "A quick note on Heterogeneous Treatment Effects\nWe’ve controlled for selection bias, what about Heterogeneous Treatment Effect bias? We actually don’t need to control for these once we’ve controlled for selection bias. These average treatment effect ends up being the average of all of the HTEs of individuals, which is fine because as long as we’ve accounted for selection bias, the HTEs tend to cancel out. They’re essentially captured by the error term, \\(\\epsilon\\) in OLS \\[\ny = \\alpha + \\beta X + \\epsilon\n\\]\nWe can also see that in our code, where the distribution of true HTE bias from our hypothetical dataset is centered at zero. Any time we’ve accounted for all selection bias, the HTE should be zero centered and cancel itself out as N increases.\n\n\nCode\nATE = data[\"ITE\"].mean()\nHTE = data.ITE.values - ATE\nsns.histplot(HTE)\nplt.xlabel(\"HTE\")\nplt.title(\"Distribution of HTEs (each customers difference from the ATE)\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe bias of HTEs for each person is just the distance their treatment effect is from the average treatment effect. Again, this follows the same property as the error term in OLS regression, which is why it can be such a powerful tool for causal inference when used correctly."
  },
  {
    "objectID": "posts/2025-03-06-metric-choice.html",
    "href": "posts/2025-03-06-metric-choice.html",
    "title": "An easy way to choose evaluation metrics",
    "section": "",
    "text": "Disclaimer\n\n\n\nI’m not going to dive into forecasting evaluation here. I’m going to highlight a simple technique to consider when you’re struggling with metric choice.\n\n\n\nThe Problem\nYou work at a company that wants to forecast demand with historical sales data. Your team is going back and forth on what the perfect evaluation metric is. The goal is to use the evaluation metric to decide which forecast to use.\nThe team’s a bit hung up, some feel strongly RMSE is the right choice, while others want to use MAPE. It turns out there can often be easy ways to choose a metric with simulation.\nWe’ll focus on the question, “if we have a perfect forecast, what would our error metrics look like?” and we’ll use it to inform metric choice.\n\n\nSimulating a fake sales time series\nWe’re going to simulate data where we know the real demand, and we add realistic amount of noise to it that resembles our actual dataset. In our case we assume the real dataset is poisson distributed, so we make our simulation have the same level of noise\n\\[\n\\text{sales} \\sim \\text{Poisson}(\\lambda = \\text{demand})\n\\]\nThe plot below shows the true underlying demand overlayed with daily sales\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\npd.options.display.float_format = '{:.2f}'.format\nnp.random.seed(1)\n\n# Parameters\nyears = 5\nperiod = 365 \nt = np.arange(0, years * period)  \n\n# We add a phase shift to make sure the first peak happens at day 182\nseasonality = np.cos(2 * np.pi * (t - 182) / period) \nmu = 1 + np.exp( 2*seasonality )\ny = np.random.poisson(mu)\n\ndf = pd.DataFrame({\"demand\":mu, \"sales\":y}, index=pd.date_range(\"2014-01-01\", periods=len(t)))\n\n# resample to weekly level\ndfW = df.resample(\"W\").sum()\n\n# Plotting\nfig, ax = plt.subplots(figsize=(12,5))\ndfW[['demand']].plot(ls='--', color='r', alpha=0.75, ax=ax, label = 'True Demand')\ndfW[['sales']].plot(color='k', alpha=0.75, ax=ax, label='Sales')\nax.set(title='True Demand and Simulated Weekly Sales', ylabel='Sales')\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat we’re doing is really simple - we know we have a perfect forecast (its the true underlying demand), so ideally a good choice of metric should look reasonable here.\n\n\nIn-sample forecast evaluation\nIn-sample forecast evaluation will be used for simplicity since its a simulation anyways, but with real data its important to use n-step-ahead evaluation or time series cross validation.\n\n\nCode\nfrom sklearn.metrics import (\n    mean_absolute_percentage_error as mape, \n    mean_absolute_error as mae, \n    root_mean_squared_error as rmse\n)\nargs = (dfW.sales.values, dfW.demand.values,)\npd.DataFrame(\n    [mape(*args), mae(*args), rmse(*args) ], \n    columns=['metric'],\n    index=['MAPE', 'MAE', 'RMSE']\n).round(2)\n\n\n\n\n\n\n\n\n\nmetric\n\n\n\n\nMAPE\n0.25\n\n\nMAE\n3.62\n\n\nRMSE\n5.04\n\n\n\n\n\n\n\nMAE and RMSE error look fine here, but MAPE looks terrible - a perfect forecast here is off by 25% on average (weekly). Imagine telling a stakeholder that your great forecast is off by 25% on average.\n\n\n\n\n\n\nImportant\n\n\n\nIn general, MAPE is pretty well known to be a meh choice of metric, but its especially apparent with low count data like this example.\n\n\nWe also established a definition of what a good forecast is - we know a perfect forecast on data of this magnitude and noise tends to have an MAE of 3.62 sales / week. If we’re anywhere near that we know we’re doing a good job.\nThis process has made it clear that MAE or RMSE error are suitable choices to consider while MAPE isn’t. Theory can drive the next step: MAE minimizes the median and is very interpretable while RMSE minimizes the mean but you lose a bit of interpretability.\nThere are also plenty of other metrics and evaluation techniques to consider, I just chose 3 of the most simple metrics for this example.\n\n\nConcluding Thoughts\nThis exercise was really simple: “if we had a perfect forecast, what would our error metrics look like?”\nUsing simulation to answer this question not only informs metric choice, but it also helps to establish a defintion of what a good forecast is.\nI acknowledge there’s alot more that goes on in forecasting and decision making than just this. But in reality, alot of stakeholders (and even data team members) can get hung up on questions like this.\nI had to use this exact process once to make sure my team didn’t use MAPE as an evaluation metric - telling someone that the best you can do with MAPE is a 25% error is a pretty convincing argument not to use it."
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html",
    "title": "Desiging an Experimentation Strategy",
    "section": "",
    "text": "Experiments have alot more use cases than many give them credit for. At their simplest, they’re a tool for mitigating risk when making product decisions. But at their best, they’re a tool that can help optimize an entire business.\nEvery business has its own unique problems and goals, and this post is a case study where strategy was made across experiments to fit the needs of the business."
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#early-stage-needs---starting-simple-building-trust",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#early-stage-needs---starting-simple-building-trust",
    "title": "Desiging an Experimentation Strategy",
    "section": "Early stage needs - Starting Simple, Building trust",
    "text": "Early stage needs - Starting Simple, Building trust\nWhen I joined the company, I was just the third data science hire, and there really wasn’t any experimentation practice in place beyond stakeholders watching an optimizely dashboard. Listening to stakeholders, there was a desire to build more trust in decision making and make sure their products decisions were actually improving things for the customer.\nWe started simple - a google doc template where stakeholders could explain the product feature they wanted to implement and their hypothesis on what outcome the product feature might improve. From there we worked with them to design a measurement strategy to answer their question, which wasn’t always an experiment. At the end of the day, experimentation is just one of many tools for causal inference. Having this process and flexibility helped us earn buy-in - we weren’t just there to tell them what we could and couldn’t do. We were there to educate so that we weren’t just recommending solutions, but so that stakeholders were playing a really active role in what our measurement strategy should be.\n\n\n\n\n\nWe also improved the experiment output from an optimizely dashboard to an in-house report. All of this built alot more trust in the results, and helped us move on to new questions like\n\n“Can we use different outcome metrics beyond conversion rate?”\n“Can we make experiments shorter?” (power analyses, variance reduction)\n“Can we launch a feature without hurting the business even if we can’t confirm it’s better?”” (uncertainty quantification)\n“What’s the right risk tolerance in terms of being able to detect an effect versus risking a false positive” (here’s a fun example)\n“Can we start experimenting like this on our other products?”\n“Can we learn how our change in one part of the funnel may have an impact further downstream?”\n“How can we run experiments that mitigate regret?” (Bandit algorithms)\n“Can we stop an experiment early and still have confidence in the results?” (sequential testing, alpha spending)\n“Who is the experiment working best for?” ( Multiple Hypothesis Testing, Heterogeneous Treatment Effects )\n\nAgain, we weren’t just throwing cool implementations at them we thought they needed, we were listening to what our stakeholders wanted first and then prioritizing it. As we moved up the ladder in terms of trust, I was able to get buy in to build out scalable tooling for others in the company to start usin, and we were able to scale the process to other teams.\nThere was also 1 key tenant I tried to keep in mind as our methodology became more advanced: could we directly observe the treatment effect? (simple difference in outcomes). No matter how advanced of a method we might consider, throwing some output from a blackbox model at a stakeholder was never the way to go. It can’t be understated how important directly observing the treatment effect is for trust."
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#whats-a-surrogate-index",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#whats-a-surrogate-index",
    "title": "Desiging an Experimentation Strategy",
    "section": "What’s a surrogate index?",
    "text": "What’s a surrogate index?\nA surrogate index was a new piece of research from Susan Athey, basically a way to predict what a long term treatment effect would be given intermediate outcomes that could be observed sooner. It also neatly addressed the biggest problem we were facing.\nThe basic idea is illustrated in the DAG below.\n\n\n\n\n\nflowchart LR\n  B(Treatment) --&gt; c(Outcome 1)\n  B(Treatment) --&gt; d(Outcome 2)\n  B(Treatment) --&gt; e(Outcome 3)\n  c(Outcome 1) --&gt; f(Long Term\\nOutcome)\n  d(Outcome 2) --&gt; f(Long Term\\nOutcome)\n  e(Outcome 3) --&gt; f(Long Term\\nOutcome)\n\n\n\n\n\n\nLet’s say the long term outcome can be entirely explained by 3 intermediate outcomes, such as mid-funnel metrics that can be observed sooner. A model can be fit to that relationship on historical data like so:\n\n\n\n\n\nflowchart LR\n  c(Outcome 1) --&gt; f(Long Term\\nOutcome)\n  d(Outcome 2) --&gt; f(Long Term\\nOutcome)\n  e(Outcome 3) --&gt; f(Long Term\\nOutcome)\n\n\n\n\n\n\nThe set of intermediate outcomes is known as a surrogate index; they’re a surrogate for the long term outcome. If you have a correct model for the DAG above, than all you need to do is observe outcomes 1-3. You could then just run an experiment as you normally would, but instead of using some game-able mid-funnel metric as the primary outcome, you could measure outcomes 1-3 for both the treatment and control group, and predict what the long-term outcome would be for each group. The difference in the prediction for the treatment group and the control group ends up being identical to the true long-term treatment effect (assuming that the model and dag are specified correctly).\nAs always, it’s easier said than done. You need to make sure that the entire intermediate effect between the treatment and the long-term outcome is captured by the DAG and the model. Or in english, you need to understand the exact effect that the intermediate outcomes have on the long term outcome, and you need to be right about it; you can’t be missing anything.\nBut there are ways to validate you’re on the right track. First, knowing something about DAGs and causal inference, you can call on the local markov property; the treatment should be conditionally independent of the long term outcome after conditioning on the surrogate index. That’s easy to test. One can alse make predictions on the long term outcome with their surrogate index and see if it ended up being correct when the long-term outcome gets observed (hopefully for repeated experiments). Also do-able. Even better, if you have many historical experiments, you could do this validation on that dataset across all of the experiments.\n\n\nCode\n# for simplicity, not actually simulating a true data generating process\nnp.random.seed(99)\nx = np.random.normal(0,3,size=25)\ntrue_north_metric = x*1.4 + np.random.normal(0, 0.5, size=len(x))\npredicted_true_north = np.random.normal(true_north_metric, 1)\n\nfig, ax = plt.subplots(figsize=(7,5))\nax.scatter(predicted_true_north, true_north_metric)\nax.plot([-8,7], [-8,7], ls='--')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set(xlabel='Predicted True North', ylabel='True North Metric', title='Surrogate Index vs. Long Term Outcome over many experiments')\nplt.show()\n\n\n\n\n\n\n\n\n\nUnfortunately, we didn’t have a large dataset of historical experiments. For the experiments we did have, they didn’t have enough power to detect reasonably sized changes on our true-north bottom funnel metric. Those experiment had all been designed with mid-funnel metrics in mind, and therefore had smaller sample sizes."
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#building-a-plan",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#building-a-plan",
    "title": "Desiging an Experimentation Strategy",
    "section": "Building a plan",
    "text": "Building a plan\nIn total there were 5 big problems to address, each with a different solution. There also weren’t alot of resources to devote to them. But if we could free up time for others running experiments, they could use some of that time to contribute to this roadmap.\nIn the end, the biggest priority was to implement surrogate indices, because that was having the biggest painpoint on our business - we couldnt keep working with game-able metrics that weren’t improving the business.\nWith that in mind, switchback experimentation was in direct conflict with that goal. Switchback experiments don’t allow you to observe long term outcomes because users arent the ones who are randomized, time periods are what is randomized. We could atleast monitor and set up guardrails to make sure there wasnt alot of spillover.\nThis also meant that bandit algorithms shouldn’t be part of our toolkit any time soon, even though there had been alot of interest in them. Bandit algorithms wouldn’t be ideal for learning the impact on long term outcomes, and using them to learn a surrogate index would be really difficult since the treatment propensity would be constantly changing over time.\nI’ll summarize some of the easy and hard solutions I came up with:\n\nInfrastructure\n\nSolution: templated SQL for scalable experiment queries\n\n“Who did the experiment work best for?”\n\nEasy solution(s): Scalable code for Multiple hypothesis testing with corrections\nHard solution: Heterogeneuous Treatment Effect models\n\nSample Size problems\n\nEasy solution(s): Quickly implement Informative priors. Since we had revenue as part of our overall evaluation criteria for some experiments, this provided a massive speedup\nHard solution(s): CUPED, Cohort Models\n\nSpillover Effects\n\nEasy Solution(s): Monitoring, guardrails\nHard Solution(s): Switchback Experimentation Engine\n\nOptimizing for Long-term Outcomes\n\nEasy Solution(s): Cohort monitoring, Longer experiments, long term hold-out groups\nHard solution(s): Surrogate Index approach\n\n\nThe final proposed plan ended up being the following:\n\nPartner with the data team for each product pillar to build templated SQL for their experiments to save them time.\nAutomate the secondary analysis of experiments so that stakeholders could understand who the experiments worked best for. We actually chose the easy solution for this, multiple hypothesis testing with corrections. It was easy to implement it and it aligned with our principle of making sure the treatment effect is observable. Plus, if we ever got the point of heterogenous treatment effect modeling, it’d be alot more believeable for stakeholders if they could also look at the multiple hypothesis test reporting. This was a low lift way to free up the teams time and help our stakeholders answer what they wanted. Importantly, we lumped in metric observability with this so that we could understand which experiments were impacting which metrics.\nReduce experiment duration with informative priors and CUPED. Many experiments had an overall evaluation criteria that incorporated customer revenue, Customer revenue was wide-tailed (near lognormally distributed). Using priors was an incredibly easy and insanely effective way to reduce variance for experiments, speeding them up significantly. CUPED would be a way to take that even further. We planned for a data scientist to implement this with their newly freed up time after automating secondary analyses.\nSet up monitoring for spillover effects, but don’t implement switchback experiments.\nCenter the life-insurance product’s experimentation strategy around a surrogate index. This meant running longer experiments so that there was enough power to detect effects on the true north metric. Hopefully after 6 months, we’d have enough information to build a correct surrogate index to start using.\n\nThere was still a big red flag. We’d have to run longer experiments, and we’d have to wait atleast 6 months to even start using this. That was met with alot of hesitation. Deviating from 1-2 week experiment cadences admittedly sounds a bit nuts. Companies need to grow, and to do that they need to change and try new things. But at the same time we had a problem we couldn’t avoid - the simple fact of the matter was that if we wanted to have conviction in our product changes, we’d have to wait 3-6 months to actually observe its down funnel impact.\nWe decided to compromise rigor for speed. We planned to immediately implement a surrogate index based on the small amount of data we had, begin running experiments with enough power to detect an effect on our true north metric (3-4 weeks), and use the surrogate index, a guardrail, and our usual mid-funnel outcome as joint primary outcome metrics for upcoming experiments. While not ideal, it’d allow us to start using these ideas immediately, and we’d also be able to build up a better dataset of experiments so that even if the first iteration of the surrogate index didn’t work, we could eventually learn the correct one. That, and we’d have enough power to observe treatment effects on the long term outcome (after waiting a long time).\nReally, our overall strategy was a shift to improving observability. We created better observability on how experiments were affecting different outcomes and different customers and we created better observability on how our experiments were impacting our true north metric. The funny part is, early in my career I thought this was a big red flag - the more you observe the more likely you could be to identify a false positive. But thats impractical advice. You really have to be looking at the whole picture, that way you can start to understand the business and learn more. Just do so responsibly."
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html",
    "title": "Uncertainty Intervals or p-values?",
    "section": "",
    "text": "Uncertainty Intervals are better than p-values. Sure, its better to use both, but p-values are just a point estimate and they bring no concept of uncertainty in our estimate - this can lead to situations where we expose ourselves to high downside risk.\nTake the following example for instance. Let’s say we’re running a “Do no harm” A/B test where we want to roll out an experiment as long as it doesnt harm conversion rate.\nIf you want to follow along with the code, see here."
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#the-experiment-design",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#the-experiment-design",
    "title": "Uncertainty Intervals or p-values?",
    "section": "The experiment design",
    "text": "The experiment design\nGiven the stakeholders want to rule out a drop in conversion, and ruling out small differences requires large sample sizes, we decide to design an experiment with good power to detect the presence of a 1/2% absolute drop (if one were to truly exist)\nWe ran a power analysis and found that in order to have a 90% probability of detecting (power=0.9) a 1/2% absolute drop in conversion rate with 80 percent confidence ( 𝛼=0.2 ), we need N=32500 per group\n\nStatisticians might not love this interpretation of a power analysis, but its a useful and interpretable translation and tends to coincide with what we’re aiming for anyway. In reality, frequentist power analyses assume that the null hypothesis is correct, which isn’t quite what we want, not to mention, frequentist power analyses use backwards probabilities which are just plain confusing - see here to for more\n\nNote that we’re prioritizing power here for a reason. If 𝛼 is false positive rate, and power is probability of detection, then don’t we want to prioritize our probability of detecting a drop if one truly exists? A false negative here would be more expensive then a false positive\npA = 0.1 # historical conversion rate\nabs_delta = 0.005 # minimum detectable effect to test for\n\n# Statsmodels requires an effect size \n# (aka an effect normalized by its standard deviation)\nstdev = np.sqrt( pA*(1-pA) ) # bernoulli stdev, sigma = sqrt(p(1-p))\nES = abs_delta / stdev \n\n# estimate required sample size\nsm.stats.tt_ind_solve_power(\n    -ES, \n    alpha=0.2,\n    power=0.9,\n    alternative=\"smaller\"\n)\nRunning the code above leads us to conclude are sample size should be roughly 32,500 users per group.\n\nThe experiment\nI’m going to simulate fake data for this experiment where * The control has a true conversion rate of 10% * the variant has a true conversion rate of 9.25%\nFor examples sake we’ll pretend we don’t know that the variant is worse\n\n\nClick here for code\n\n# Settings\nnp.random.seed(1325)\nN = 32500\npA = 0.1\npB = 0.0925\n\n# Simulation\ndef simulate_experiment(pA, pB, N_per_group):\n    \n    df = pd.DataFrame({\n        \"group\":[\"A\"]*N + [\"B\"]*N,\n        \"convert\":np.r_[\n             np.random.binomial(1, p=pA, size=N),\n             np.random.binomial(1, p=pB, size=N)\n        ]\n    })\n    \n    return df\n\ndf = simulate_experiment(pA, pB, N)\n\n\n\n\n\n\n\nLooking at the data above, we’re seeing a better conversion rate in group B. We run a two-proportions z-test and we find that there’s a non-significant p-value, meaning we found insufficient evidence of the variant having lower conversion than the control.\ndef pval_from_summary(tab):\n    \n    _, pval = sm.stats.proportions_ztest(\n        count=tab[\"converts\"][::-1], \n        nobs=tab[\"N\"][::-1],\n        alternative=\"smaller\"\n    )\n    return pval\n\n(df.pipe(summarize_experiment)\n   .pipe(pval_from_summary))\n\\[ p = 0.38 \\]\nWe recommend to our stakeholders to roll out the variant since it “does no harm”\nThere are some serious red flags here\n\nFirst of all, p-values are all about the null hypothesis. So just because we don’t find a significant drop in conversion rate, that doesnt mean one doesnt exist. It just means we didnt find evidence for it in this test\nThere was no visualization of the uncertainty in the result"
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#understanding-uncertainty-with-the-beta-distribution",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#understanding-uncertainty-with-the-beta-distribution",
    "title": "Uncertainty Intervals or p-values?",
    "section": "Understanding Uncertainty with the Beta Distribution",
    "text": "Understanding Uncertainty with the Beta Distribution\nFor binary outcomes, the beta distribution is highly effective for understanding uncertainty.\nIt has 2 parameters * alpha, the number of successes * beta, the number of failures\nIt’s output is easy to interpret: Its a distribution of plausible probabilities that lead to the outcome.\nSo we can simply count our successes and failures from out observed data, plug it into a beta distribution to simulate outcomes, and visualize it as a density plot to understand uncertainty\n\n\n\n\n\nIt’s also easy to work with - if we want to understand the plausible differences between groups, we can just take the differences in our estimates\n\\[\n\\delta = \\hat{p_B} - \\hat{p_A}\n\\]\n\n\n\n\n\nWith visualization, we get a very different picture than our non-significant p-value. We see that there’s plenty of plausibility that the control could be worse.\nWe can further calculate the probability of a drop, \\(P(B &lt; A)\\), and find that theres a greater than 60% probability that the variant is worse than the control\n# P(B &lt; A)\n(pB_hat &lt; pA_hat).mean()\n\\[\nP(B &lt; A) = 0.62\n\\]\nRemember when we designed the experiment? Considering our main goal was to do no harm, we might not feel so confident in that now, and rightly so, we know the variants worse since we simulated it.\nUnless we feel very confident in our choice of testing for a 1/2% drop and know that we can afford anything up to that, then we we really shouldnt roll out this variant without further evaluation\nThis is particularly important with higher uncertainty As we can see in the example below, where the observed conversion rate is better in the variant, but the downside risk is as high as a 4% drop in conversion rate\n\n\n\n\n\n\\[\np = 0.59\n\\]"
  },
  {
    "objectID": "posts/2022-04-03-explainable-ai-is-not-causal.html",
    "href": "posts/2022-04-03-explainable-ai-is-not-causal.html",
    "title": "Explainable AI is not Causal Inference",
    "section": "",
    "text": "Explainable AI is all the rage these days. Black box ML models come along with some fun tools such as LIME, SHAP, or Partial Depence Plots that try to give visibility into how the model is interpreting data and making predictions. It’s a common misconception that these are causal inference techniques - sadly we’ve all been mislead.\nWe’re going to walk through an example that shows these tools fall victim to the same rules of causal inference as everything else. A confound is still a confound, and if you want to measure some causal effect there’s still no way around that without careful deliberation of which variables to include in your models.\nThe code for this blogpost can be found here\n\nStarting simple: simulating some fake data\nLet’s start with a simple scenario. Our goal is to estimate some causal effects. We’re going to simulate out data ourself so we know the true causal effects. We can see how good some popular “explainable AI” algorithms actually are at causal inference. We’ll simulate data from the following DAG:\n\n\n\n\n\n\nWhat’s a DAG? A dag is a directed acyclic graph, or fancy talk for a flowchart that goes in 1 direction. It’s really just a diagram of a true data generating process. They’re typically assumed based on domain knowledge (like all models), although ocassionally there are some validation checks you can perform.\nEdges in the graph are assumed to be true causal effects. So for example,\n\nX3 influences Y\nX5 influences X1 which influences Y\nSome unobserved variable U influences both X1 and Y. By unobserved, what I mean is that its some variable we don’t have data for.\n\nFor those familiar with causal inference, this DAG in particular is also riddled with confounds.\n\nOk back on track. We’ll get out one of the more popular Explainable AI tools nowadays, XGBoost. I’m going to start in the most dangerous way possible - I’m going to toss everything in the model.\n\n\nTest 1: What’s the impact of X1 on Y?\nWe know for a fact that X1 influences Y. Let’s see how well Partial Dependence Plots and SHAP values do at identifying the true causal effect\n\n\n\n\n\nThese SHAP values arent just wrong, but the effect is in the wrong direction. The reason for this: there’s a Fork Confound.\n\n\n\n\n\nSome variable Z confounds Xs true effect on Y.\n\nA very common example of a fork confound is warm weather (Z) on the relationship between ice cream sales (X) and crime (Y). Ice cream sales obviously have no influence on crime, but ice cream sales are higher during warmer weather, and crime is higher during warmer weather.\n\nSo back to our main point - Explainable AI can’t get around a fork confound. This is our first lesson on why SHAP / explainable AI is different from causal inference.\nLuckily in this case, statistics can solve this problem.\nUsing some domain knowledge about the generating process, we notice an instrument, X5, that can be used to estimate the causal effect of X1 on Y\n\n\n\n\n\nI won’t go into the details of instrumental variable analysis since the goal of this article is to highlight that Explainable AI can’t replace causal inference. To learn more about it, see Scott Cunningham’s Causal Inference the Mixtape.\nBut for now, I will show that a classic causal inference method succeeds where XGBoost and SHAP values fail\nfrom linearmodels import IV2SLS\nfrom src.dagtools import get_effect\n\n# Instrumental variable analysis\niv_model = IV2SLS.from_formula(\"Y ~ 1 + [X1 ~ X5]\", data=df).fit()\n\n# pull true effect\ntrue_effect = get_effect(DAG, \"X1\", \"Y\")\n\n# Plot\nfig, ax = plt.subplots(1,1,figsize=(6,4))\nax.set_title(\"Instrumental Variable Analysis\\nRecovers True effect\")\nplot_model_estimate(iv_model, true_effect=true_effect, feat=\"X1\", ax=ax)\n\n\n\n\n\nAs we can see, a simple statistics technique succeeds where explainable AI fails.\n\n\nWhat about estimating the effect of X4 on Y?\nThis relationship is slightly more complicated, but certainly measurable. X4 influences X2 which influences Y. Here’s the DAG again for reference\n\n\n\n\n\nThe plots below show how well explainable AI does at estimating the causal effect of this relationship.\n\n\n\n\n\nUnfortunately, they don’t pick up an effect at all! And if our goal was to increase Y we’d end up missing a pretty good lever for it. There’s another simple explanation here for why explainable AI: there’s a Pipe Confound\n\n\n\n\n\nWhen trying to measure the effect of X -&gt; Y, conditioning on Z (aka including it in a model as a covariate with X) ends up blocking inference.\nFor more details on how a Pipe confound works, I recommend chapters 5 and 6 of Richard McElreath’s Statistical Rethinking v2 (where I borrowed the example from as well).\nThe main things to note here are that pipes are common and Explainable AI doesn’t get around them.\nWe can recover an unbiased estimate of the true effect simply with OLS\n# Fit simple OLS model\nmodel = sm.OLS.from_formula(\"Y ~ X4\", data=df).fit()\n\n# pull true effect\ntrue_effect = get_effect(DAG, \"X4\", \"X2\") * get_effect(DAG, \"X2\", \"Y\")\n\n# Plot (see notebok for plot_model_estimate function)\nfig, ax = plt.subplots(1,1,figsize=(6,4))\nax.set_title(\"Instrumental Variable Analysis\\nRecovers True effect\")\nplot_model_estimate(model, true_effect=true_effect, feat=\"X4\", ax=ax)\n\n\n\n\n\n\n\nCan we use Explainable AI for causal inference at all?\nWe can! We just need to be deliberate in which variables we include in our models, and the only way to do that right is to use DAGs! The example below looks at an XGBoost model that doesnt condition on X2 (allowing us to estimate the causal effect of X4 -&gt; Y).\n\n\n\n\n\n\n\nTake Aways\nExplainable AI is not some magic tool for causal inference. What these tools are good at is explaining why complicated models make the decisions they do. Explainable AI tools suffer from the same limitations for causal inference as all other statistical estimators.\nAt the end of the day when causal inference is your goal, nothing beats using DAGs to inform deliberate variable selection.\nIf you’re new to the subject, I highly recommend the following resources that will teach you how to use causal inference properly:\n\nChapter’s 5 and 6 of Statistical Rethinking v2, by Richard McElreath\nCausal Inference for the Brave and True by Matheus Facure\nCausal Inference the Mixtape, by Scott Cunningham"
  },
  {
    "objectID": "posts/2022-04-05-ab-test-duration.html",
    "href": "posts/2022-04-05-ab-test-duration.html",
    "title": "How long should you run an A/B test for?",
    "section": "",
    "text": "For some people in industry new to A/B testing, they might wonder “Why cant we just run an A/B test for 2 days and be done with it?”. Even those familiar with it might wonder why their team’s Data Scientist is insisting on so much. And even Data Scientists may be looking for easier ways to explain the need to them. The goal of this article is to cover just that, from a naive and explainable point of view.\nSo, How long should you run an A/B test for? Well let’s say you step into a casino with $5000 and you walk away with $6000. You just made a 20% return. Is it fair to say that a night out in the casino leads to a 20% return? Is it fair to say that our A/B test we ran for 2 days leads to a 20% lift in conversion? How do we know for sure?\nWe should run an A/B test for as long as it takes to rule out random chance.\nWhile vague, and technically not the full picture, your friendly neightborhood data scientist should be able to answer this for you. The code for this blogpost can be found here."
  },
  {
    "objectID": "posts/2022-04-05-ab-test-duration.html#footnotes",
    "href": "posts/2022-04-05-ab-test-duration.html#footnotes",
    "title": "How long should you run an A/B test for?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUnless you’re using bayesian inference, which can really mitigate this risk.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About Me",
    "section": "",
    "text": "About Me\nI’m a data scientist interested in causal inference and bayesian methods. I mainly use this blog to practice what I learn, but hopefully others find this helpful as well!\nFor a work sample, please refer to this post\n\n\n  \n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nDouble ML in Numpyro using scope\n\n\n\n\n\n\ncausal inference\n\n\nnumpyro\n\n\n\n\n\n\n\n\n\nApr 1, 2025\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nAn easy way to choose evaluation metrics\n\n\n\n\n\n\nmetrics\n\n\nmodel evaluation\n\n\ntime series\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nCommon Misconceptions with Multicollinearity\n\n\n\n\n\n\nmulticollinearity\n\n\ncausal inference\n\n\nregression\n\n\n\n\n\n\n\n\n\nMar 6, 2025\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nAutomatic Dim Labelling with Numpyro?\n\n\n\n\n\n\nnumpyro\n\n\ntensors\n\n\nArviZ\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\n33 min\n\n\n\n\n\n\n\n\n\n\n\n\nPandera and Object Oriented Data Validation\n\n\n\n\n\n\nobject oriented programming\n\n\ndata processing\n\n\n\n\n\n\n\n\n\nFeb 21, 2025\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nModeling Anything With First Principles: Demand under extreme stockouts\n\n\n\n\n\n\nTime Series\n\n\nDemand Modeling\n\n\nCausal Inference\n\n\nSupply Chain\n\n\nDiscrete Choice\n\n\nSurvival Analysis\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n32 min\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Surrogate Indexes\n\n\n\n\n\n\nexperimentation\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nDesiging an Experimentation Strategy\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nUseful Tools for Weibull Survival Analysis\n\n\n\n\n\n\nsurvival analysis\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we need A/B tests? The Potential Outcomes Model\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nMaking out of sample predictions with PyMC\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow long should you run an A/B test for?\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nUncertainty Intervals or p-values?\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n7 min\n\n\n\n\n\n\n\n\n\n\n\n\nExplainable AI is not Causal Inference\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n5 min\n\n\n\n\n\n\nNo matching items"
  }
]