[
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html",
    "title": "Introduction to Surrogate Indexes",
    "section": "",
    "text": "How should you design your experiments if the metric you want to change might take months to observe?\nInspired by Susan Athey’s paper on Surrogate indexes and another working paper, Target for Long Term Outcomes, I’ve wanted to share my learnings about surrogate indexes for a long time.\nI’m hoping to cover the following in a series of blog posts:\n\nThe surrogate index estimator\nSurrogate Index in practice\n\nFitting a surrogate index on a realistic dataset\nValidating the surrogate index over a set of historical experiments\n\nTargeting Long Term Outcomes\n\nOptimizing a policy\nAttempting multi-armed bandits for early optimization\n\n\nBy simulating the data generating process from scratch, I hope this can also be a helpful tool for others to build on so they can answer their own questions they may have about estimating Long Term Treatment Effects."
  },
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-1-simulating-the-data",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-1-simulating-the-data",
    "title": "Introduction to Surrogate Indexes",
    "section": "Step 1: Simulating the data",
    "text": "Step 1: Simulating the data\nWe’ll start by simulating two datasets: A historical dataset and an experiment dataset. The advantage to simulating data is that we know the exact effects, so when we try and estimate them we can confirm our methods are recovering the true effect.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom simple_simulation import set_true_parameters, transmitter\nSEED = 99\n\n# step 1: simulate data\nrng = np.random.default_rng(seed=SEED)\nlogit_link=True\n\n# True parameters\nZdims = 20 # customer latent dims\nXdims = 5 # pre treatment covariates\nSdims = 8 # intermediate outcomes (the surrogates)\nn_users = 50000\n\nGROUND_TRUTH = set_true_parameters(Zdims, Xdims, Sdims, logit_link=True, rng=rng)\n\n# Simulate data \nhistorical_data = transmitter(GROUND_TRUTH, add_treatment=False, n_users=n_users,logit_link=logit_link, rng=rng) \nexperiment_data = transmitter(GROUND_TRUTH, add_treatment=True, n_users=n_users, logit_link=logit_link, rng=rng)\n\n# Censor the experiment dataset so that we dont know the long term outcome yet\nY_TRUE = experiment_data.Y.values\nexperiment_data = experiment_data.assign(Y=np.NaN)\n\n# Show the historical dataset \ndisplay(historical_data.head(5))\n\n\n\n\n\n  \n    \n      \n      X0\n      X1\n      X2\n      X3\n      X4\n      T\n      S0\n      S1\n      S2\n      S3\n      S4\n      S5\n      S6\n      S7\n      Y\n    \n  \n  \n    \n      0\n      -0.122906\n      0.870259\n      -1.054623\n      -0.139527\n      -1.433731\n      0.0\n      1.136292\n      0.470017\n      -0.263080\n      0.486101\n      0.723525\n      0.129917\n      -0.493243\n      0.679735\n      0\n    \n    \n      1\n      0.736599\n      -0.706638\n      1.337368\n      -0.111941\n      -0.838790\n      0.0\n      0.151892\n      -0.107404\n      0.140590\n      -0.815408\n      0.274605\n      -0.446996\n      0.354266\n      -1.318440\n      1\n    \n    \n      2\n      1.110126\n      -1.617668\n      1.262500\n      1.049915\n      -0.592917\n      0.0\n      -0.166384\n      -0.147854\n      0.565484\n      -1.401817\n      -0.086840\n      -0.453305\n      0.529608\n      -1.086965\n      0\n    \n    \n      3\n      0.475479\n      -0.507716\n      0.324226\n      1.259292\n      -0.366888\n      0.0\n      0.154545\n      0.161311\n      0.087151\n      -0.750579\n      0.103096\n      -0.098244\n      0.131146\n      0.032963\n      1\n    \n    \n      4\n      0.290787\n      -0.575874\n      0.867617\n      0.844031\n      -0.098109\n      0.0\n      -0.089913\n      0.078953\n      0.067596\n      -0.773764\n      0.041729\n      -0.170939\n      0.318613\n      -0.506544\n      0\n    \n  \n\n\n\n\nThe underlying simulation code is below if you’re interested\n\n\nCode\nfrom typing import Dict\n\ndef transmitter(\n    params: Dict,\n    add_treatment: bool = False,\n    n_users: int = 1,\n    logit_link: bool = False,\n    rng = None\n) -> pd.DataFrame:\n    '''Simulates outcomes based on some ground truth parameters. \n\n    Parameters\n    -----------\n        params: The ground truth parameters (effects and biases) to simulate based off of\n        add_treatment: adds a randomly allocated treatment when true, with effect `bTS`\n        n_users: The number of users to simulate\n        logit_link: whether the data generating process is a bernoulli outcome or not\n        rng: A numpy random generator \n\n    Returns\n    --------\n        A pandas dataframe with simulated data, including pre-treatment covariates,\n         surrogate outcomes, a treatment indicator, and a long term outcome, Y\n    '''\n    if rng is None:\n        seed = np.random.choice(range(1000))\n        rng = np.random.default_rng(seed=seed)\n\n    # unpack params\n    Zdims, Xdims, Sdims =  params['bZX'].shape[1],  params['bZX'].shape[0],  params['bXS'].shape[0]\n    alphaX,alphaS,alphaY  = params['alphaX'], params['alphaS'], params['alphaY'] # bias terms\n    bZX,bXS,bSY = params['bZX'], params['bXS'],params['bSY'] # causal relationships\n    bTS,bXTS = params['bTS'], params['bXTS'] # tx effects\n\n    # unobserved variable Z representing latent customer traits\n    Z = rng.normal(0,1, size=(Zdims, n_users))\n\n    # Some observed pre-TX measures\n    X = alphaX[:,None] + (bZX @ Z)\n\n    # Intermediate outcomes\n    S = alphaS[:,None] + (bXS @ X) \n\n    # Add in treatment effect if applicable\n    T = rng.binomial(1,0.5,size=n_users) if add_treatment else np.zeros(n_users)        \n    avg_tx_term = (bTS * T[:,None])        \n    hetergeneous_tx_term = (bXTS @ (X*T))\n    S += avg_tx_term.T + hetergeneous_tx_term\n\n    # expectation of long term outcome\n    eta = 0 + (bSY @ S)\n\n    # Long term outcome\n    if logit_link:\n        Y = rng.binomial(1, sp.expit(eta) )\n    else:\n        Y = rng.normal(eta, 0.025)\n\n    # Output as dataframe\n    Xdf = pd.DataFrame(X.T, columns=[f'X{i}' for i in range(Xdims)]).assign(T=T)\n    Sdf = pd.DataFrame(S.T, columns=[f'S{i}' for i in range(Sdims)])\n    Ydf = pd.DataFrame(Y.ravel(), columns=['Y'])\n    return pd.concat((Xdf, Sdf, Ydf),axis=1)"
  },
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-2-fitting-a-surrogate-model",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-2-fitting-a-surrogate-model",
    "title": "Introduction to Surrogate Indexes",
    "section": "Step 2: Fitting a surrogate model",
    "text": "Step 2: Fitting a surrogate model\nWe’ll use the historical dataset to fit the surrogate index model, mapping \\(S \\rightarrow Y\\)\n\nS_vars = \" + \".join( historical_data.filter(like=\"S\").columns )\nX_vars = \" + \".join( historical_data.filter(like=\"X\").columns )\n\n# Step 2: fit a surrogate index model on complete historical data\nsurrogate_model = sm.OLS.from_formula(f\"Y ~ {S_vars}\", data=historical_data).fit()\n\n# Estimate the variance in the estimator, \\hat{sigma^2}. This is used for bias corrections later\npredicted_sigma2 = np.var( surrogate_model.fittedvalues - historical_data.Y,  ddof=1 )\n\n# Show the model summary\nsurrogate_model.summary()\n\n\n\nOLS Regression Results\n\n  Dep. Variable:            Y          R-squared:             0.109 \n\n\n  Model:                   OLS         Adj. R-squared:        0.109 \n\n\n  Method:             Least Squares    F-statistic:           1227. \n\n\n  Date:             Tue, 26 Sep 2023   Prob (F-statistic):    0.00  \n\n\n  Time:                 20:23:12       Log-Likelihood:      -33395. \n\n\n  No. Observations:       50000        AIC:                6.680e+04\n\n\n  Df Residuals:           49994        BIC:                6.686e+04\n\n\n  Df Model:                   5                                     \n\n\n  Covariance Type:      nonrobust                                   \n\n\n\n\n               coef     std err      t      P>|t|  [0.025    0.975]  \n\n\n  Intercept     0.4987     0.002   236.271  0.000     0.495     0.503\n\n\n  S0            0.0361     0.004    10.244  0.000     0.029     0.043\n\n\n  S1            0.1035     0.005    21.661  0.000     0.094     0.113\n\n\n  S2           -0.0399     0.005    -8.396  0.000    -0.049    -0.031\n\n\n  S3            0.1833     0.004    49.916  0.000     0.176     0.190\n\n\n  S4            0.0482     0.002    29.074  0.000     0.045     0.051\n\n\n  S5            0.0674     0.002    29.208  0.000     0.063     0.072\n\n\n  S6           -0.0057     0.002    -2.350  0.019    -0.010    -0.001\n\n\n  S7            0.0054     0.003     1.970  0.049  2.64e-05     0.011\n\n\n\n\n  Omnibus:       250873.640   Durbin-Watson:         1.993\n\n\n  Prob(Omnibus):    0.000     Jarque-Bera (JB):   5244.738\n\n\n  Skew:             0.004     Prob(JB):               0.00\n\n\n  Kurtosis:         1.413     Cond. No.           1.96e+16\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.[2] The smallest eigenvalue is 2.41e-28. This might indicate that there arestrong multicollinearity problems or that the design matrix is singular.\n\n\nThere are 3 important things to note here:\n\nNote that we’re purposely not including the pre-treatment covariates, \\(X\\) in this model. Remember the DAG - all of the effect of \\(X\\) on \\(Y\\) is entirely mediated by \\(S\\), so adding \\(X\\) into the model adds no additional information.\nWe’re using Ordinary Least Squares for bernoulli outcome data. Thats not a mistake. OLS has great properties to be effective on bernoulli outcome data, and it makes this approach very simple. Other models can also be swapped in, like Random Forest or XGBoost.\nI’m not doing alot of model validation, just because this is simulated data. In practice, don’t just throw things into a model. Part 2 in this series will discuss how to validate surrogate indexes."
  },
  {
    "objectID": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-3-estimate-long-term-treatment-effect",
    "href": "posts/surrogates/2023-07-09-surrogate_index_intro.html#step-3-estimate-long-term-treatment-effect",
    "title": "Introduction to Surrogate Indexes",
    "section": "Step 3: Estimate Long Term Treatment Effect",
    "text": "Step 3: Estimate Long Term Treatment Effect\nWe’ll now use the surrogate index to estimate a long term treatment effect\nLet’s take our experiment dataset and estimate \\(E[\\delta_{\\text{LTO}}]\\), the average treatment effect on the long term outcome. Notice, the long term outcome, \\(Y\\), hasn’t been observed yet.\n\n\nCode\ndisplay(experiment_data.head())\n\n\n\n\n\n\n  \n    \n      \n      X0\n      X1\n      X2\n      X3\n      X4\n      T\n      S0\n      S1\n      S2\n      S3\n      S4\n      S5\n      S6\n      S7\n      Y\n    \n  \n  \n    \n      0\n      0.524763\n      2.462224\n      -0.550430\n      0.771123\n      0.435625\n      0\n      0.699780\n      0.013300\n      -1.550533\n      0.516532\n      0.555615\n      0.259520\n      -0.912439\n      1.968729\n      NaN\n    \n    \n      1\n      -0.852718\n      2.363775\n      -1.233271\n      -0.064883\n      0.007073\n      1\n      0.401477\n      0.150007\n      -1.056908\n      1.090094\n      0.319569\n      1.315168\n      -1.060364\n      1.639560\n      NaN\n    \n    \n      2\n      -1.845904\n      -0.143859\n      1.205996\n      -2.574407\n      -0.338278\n      1\n      -0.617663\n      -0.031722\n      0.353283\n      0.503690\n      -0.077284\n      0.706088\n      0.401262\n      -2.388860\n      NaN\n    \n    \n      3\n      -0.860914\n      -1.382552\n      0.295809\n      -2.023966\n      -0.018137\n      0\n      -0.691752\n      -0.197120\n      1.080598\n      0.329710\n      -0.489959\n      -0.151432\n      0.610699\n      -1.754289\n      NaN\n    \n    \n      4\n      -0.257026\n      0.005047\n      -0.333462\n      -0.386979\n      -0.126490\n      1\n      -0.391908\n      -0.302902\n      0.220642\n      0.201999\n      -0.343136\n      0.883784\n      -0.312573\n      -0.012875\n      NaN\n    \n  \n\n\n\n\nFirst, we’ll do some visulation of the experiment data.\n\n\nCode\nfig, axes = plt.subplots(2,int(Xdims/2),figsize=(8,5),sharey=True)\n\nfor i, ax in zip(range(Xdims), axes.ravel()):\n    sns.histplot( experiment_data.loc[lambda d: d['T']==0][f\"X{i}\"],ax=ax, label='Control' )\n    sns.histplot( experiment_data.loc[lambda d: d['T']==1][f\"X{i}\"],ax=ax, label='Treatment' )\n\nplt.suptitle(\"Histogram of Pre-Treatment Covariates\\nfor the Treatment and Control groups\")\nplt.tight_layout()\n\n\n\n\n\nAs we can see above, the pre-treatment variables are the exact same between the experiment groups. Thats because users are randomly allocated into treatment and control groups, and their pre-treatment varibles by definition are things not imapcted by the experiment.\nConversely, if we look at the surrogate outcomes below, we’ll see some differences in surrogate outcomes between the treatment and control groups.\n\n\nCode\nfig, axes = plt.subplots(2,int(Sdims/2),figsize=(8,5), sharey=True)\n\nfor i, ax in zip(range(Sdims), axes.ravel()):\n    sns.histplot( experiment_data.loc[lambda d: d['T']==0][f\"S{i}\"],ax=ax, label='Control' )\n    sns.histplot( experiment_data.loc[lambda d: d['T']==1][f\"S{i}\"],ax=ax, label='Treatment' )\n\nplt.suptitle(\"Histogram of Surrogate Outcomes\\nfor the Treatment and Control groups\")\nplt.tight_layout()\n\n\n\n\n\nIf our surrogate index estimator is correct, these observed surrogate outcomes should directly map to the Long Term Outcome deterministically, via \\(\\hat{Y} = f(S)\\), where \\(f()\\) is the surrogate index model.\nWe can show that the surrogate index estimator recovers the true average treatment effect on the long term outcome\n\ndef estimate_delta_LTO(experiment_data, surrogate_model, predicted_sigma2):\n    '''Accepts experiment data with a binary treatment, a surrogate model, and the predicted sigma^2 of the surrogate model.\n    Returns the ATE estimate and its uncertainty\n    \n    '''\n    Y_T1 = surrogate_model.predict(experiment_data.loc[lambda d: d['T']==1])\n    Y_T0 = surrogate_model.predict(experiment_data.loc[lambda d: d['T']==0])\n    \n    # Calculate the ATE\n    ATE =  Y_T1.mean() - Y_T0.mean()\n    # calculate the variance \n    var_surrogate = np.var(Y_T1,ddof=1) / len(Y_T1) + np.var(Y_T0,ddof=1) / len(Y_T0)\n    # Adjust the variance by the surrogate model error\n    var_surrogate_adj = var_surrogate + 2*predicted_sigma2/len(Y_T1)\n    ATE_sd = np.sqrt(var_surrogate_adj)\n    \n    return ATE, ATE_sd\n\nATE, ATE_sd = estimate_delta_LTO(experiment_data, surrogate_model, predicted_sigma2)\nsns.histplot(np.random.normal(ATE, ATE_sd,size=10000), stat='probability')\nplt.axvline( GROUND_TRUTH['ATE'], color='r', ls='--', label='True ATE')\nplt.legend()\nplt.xlabel(\"ATE\")\nplt.title(\"Estimated Treatment Effect vs. True Treatment Effect\")\nplt.show()\n\n\n\n\nThere we are. The surrogate estimator recovers the true average treatment effect! We didn’t even have to wait and observe the true long term outcome.\nIf you’re interested, try simulating this repeatedly to confirm it regularly recovers the true ATE with different random seeds. Even better, try setting the treatment effect to zero and see how often there are false positives."
  },
  {
    "objectID": "posts/experiment_design.html",
    "href": "posts/experiment_design.html",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "",
    "text": "This blog post introduces the Potential Outcomes Model and introduces why experiments are often necessary to measure what we want. This topic is already covered extensively in other more rigorous resources. This post provides just another example."
  },
  {
    "objectID": "posts/experiment_design.html#a-hypothetical-world",
    "href": "posts/experiment_design.html#a-hypothetical-world",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "A Hypothetical world",
    "text": "A Hypothetical world\nWhat if we envision some hypothetical world we can observe the outcome for each customer who reached out to customer support, with and without having the treatment of receiving a promo?\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport scipy.special as sp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nrng = np.random.default_rng(100)\n\nN=100_000\nupset = rng.normal(0, 1, N)\n\ndef sim_treatment(upset, rng, return_prob=False):\n    beta = 1.5\n    p_treatment = sp.expit( -2.5 + upset * beta)\n    if return_prob:\n        return p_treatment\n    return rng.binomial(1, p_treatment)\n\ndef sim_outcome(upset, treatment, rng):\n    eps = rng.normal(0, 150, size=len(upset))\n    ltv = 2500 + 500*treatment + -500*upset + eps \n    return ltv.round(2)\n\ndata = pd.DataFrame({\n    \"Person\": np.arange(N),\n    \"upset\": upset,\n    \"T\": sim_treatment(upset, rng),\n    \"Y(0)\": sim_outcome(upset, np.zeros(N), rng),\n    \"Y(1)\": sim_outcome(upset, np.ones(N), rng)\n}).set_index(\"Person\")\\\n  .assign(ITE = lambda d: d[\"Y(1)\"] - d[\"Y(0)\"])\\\n  .assign(Y = lambda d: np.where(d[\"T\"] == 1, d[\"Y(1)\"], d[\"Y(0)\"]) )\n\ndata.head()[[\"T\", \"Y(0)\", \"Y(1)\", \"ITE\"]]\n\n\n\n\n\n\n  \n    \n      \n      T\n      Y(0)\n      Y(1)\n      ITE\n    \n    \n      Person\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      3108.62\n      3583.87\n      475.25\n    \n    \n      1\n      0\n      2347.01\n      2878.23\n      531.22\n    \n    \n      2\n      1\n      2176.28\n      2379.30\n      203.02\n    \n    \n      3\n      0\n      2146.09\n      2559.96\n      413.87\n    \n    \n      4\n      0\n      2806.50\n      3623.16\n      816.66\n    \n  \n\n\n\n\nAs shown above, in this hypothetical world we can see the exact individual treatment effect (ITE) for every customer.\n- Person 0 would have spent $475.25 more over their lifetime  if they received the promo\n- Person 2 would have spend $203.02 more over their lifetime if they received the promo\nIf we want to know the Average Treatment Effect (ATE, often denoted \\(\\tau\\)), all we have to do is take the mean of all of the individual treatment effects. As we can see, the ATE is about $500\n\\[\n\\tau = \\frac{1}{N} \\sum^{N}_{i=0} Y_i(1) - Y_i(0)\n\\]\n\ndata.ITE.mean()\n\n500.09949529999994\n\n\nWe can also represent this in hypothetical terms that will be useful later - the average treatment effect of the treated (ATT), and the average treatment effect of the untreated (ATU). The true ATE ends up being the weighted average of these terms, weighted by the proportion of individuals seeing the treatment, \\(\\pi\\)\n\\[\n\\begin{align}\n\\tau & = \\pi \\cdot E[\\tau | T=1] + (1-\\pi) \\cdot E[\\tau | T= 0] \\\\\n     & = \\pi \\cdot \\text{ATT} + (1-\\pi) \\cdot \\text{ATU}\n\\end{align}\n\\]\nWe can confirm that this is equivalent to the ATE from above with code\n\npi = data[\"T\"].value_counts(normalize=True)\n(pi * data.groupby(\"T\").mean()[\"ITE\"]).sum()\n\n500.0994953"
  },
  {
    "objectID": "posts/experiment_design.html#getting-hit-with-the-real-world",
    "href": "posts/experiment_design.html#getting-hit-with-the-real-world",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Getting hit with the real world",
    "text": "Getting hit with the real world\nSo how can we create a scenario where we can observe each person with and without having received the promo? Sadly, we can’t. But is there a way to make use of data we already have? Here’s the actual data we might have access to. Notice that now the hypothetical potential outcomes are no longer visible (just like in the real world).\n\n\nCode\n# Real world data\ndf = (\n    data[[\"upset\", \"T\", \"Y(0)\", \"Y(1)\", \"ITE\", \"Y\"]]\n    .assign(**{\n        \"Y(0)\":lambda d: np.where(d[\"T\"]==1, np.NaN, d[\"Y(0)\"]),\n        \"Y(1)\":lambda d: np.where(d[\"T\"]==0, np.NaN, d[\"Y(1)\"]),\n        \"ITE\": np.NAN\n        })\n)\n\ndf.iloc[:,1:].head()\n\n\n\n\n\n\n  \n    \n      \n      T\n      Y(0)\n      Y(1)\n      ITE\n      Y\n    \n    \n      Person\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      3108.62\n      NaN\n      NaN\n      3108.62\n    \n    \n      1\n      0\n      2347.01\n      NaN\n      NaN\n      2347.01\n    \n    \n      2\n      1\n      NaN\n      2379.3\n      NaN\n      2379.30\n    \n    \n      3\n      0\n      2146.09\n      NaN\n      NaN\n      2146.09\n    \n    \n      4\n      0\n      2806.50\n      NaN\n      NaN\n      2806.50\n    \n  \n\n\n\n\nOne (unfortunately incorrect) idea might be take the average of Y(1) and subtract the average of Y(0), also known as the simple difference in outcomes (SDO).\n\\[\n\\text{SDO} = E[ Y(1) | T = 1 ] - E[ Y(0) | T = 0 ]\n\\]\n\nNotice that I use the terms \\(E[ Y(0) | T = 0 ]\\) and \\(E[ Y(1) | T = 1 ]\\). Reading these as plain english “the expected value (aka mean) of Y(0) given no treatment” and “the expected value (aka mean) of Y(1) given a treatment”\n\n\n(\n    df.groupby(\"T\")\n    .mean()[[\"Y\"]].T\n    .assign(tau = lambda d: d[1] - d[0])\n    .rename(columns={0:\"E[ Y(0) | T = 0 ]\", 1:\"E[ Y(1) | T = 1 ]\"})\n    .rename_axis(None, axis=1)\n    .round(2)\n    .reset_index(drop=True)\n)\n\n\n\n\n\n  \n    \n      \n      E[ Y(0) | T = 0 ]\n      E[ Y(1) | T = 1 ]\n      tau\n    \n  \n  \n    \n      0\n      2579.46\n      2491.48\n      -87.98\n    \n  \n\n\n\n\nUnder the SDO it looks like the treatment has a negative effect - this is saying that giving customers a promo makes their LTV $88 worse? That seems seriously wrong, and is a huge problem. It should be $500 like we saw in our hypothetical world. So what went wrong?"
  },
  {
    "objectID": "posts/experiment_design.html#selection-bias",
    "href": "posts/experiment_design.html#selection-bias",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Selection Bias",
    "text": "Selection Bias\nWe can illustrate the problem by bringing another variable into the mix - customer unhappiness (we’re pretending we can measure it directly for examples sake).\n\nfig, ax = plt.subplots(1,2, figsize=(8,3))\nax[0].set_title(\"Histogram of\\nCustomer unhappiness\")\ndf.upset.hist(ax=ax[0])\n\nax[1].set_title(\"More upset customers are\\nmore likely to receive a promo\")\nax[1].set_ylabel(\"Proportion Receiving Promo\")\ndf.groupby(df.upset//0.25*0.25).mean()[\"T\"].plot(ax=ax[1])\nplt.tight_layout()\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      upset\n      T\n      Y(0)\n      Y(1)\n      ITE\n      Y\n    \n    \n      Person\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      -1.157550\n      0\n      3108.62\n      NaN\n      NaN\n      3108.62\n    \n    \n      1\n      0.289756\n      0\n      2347.01\n      NaN\n      NaN\n      2347.01\n    \n    \n      2\n      0.780854\n      1\n      NaN\n      2379.3\n      NaN\n      2379.30\n    \n    \n      3\n      0.543974\n      0\n      2146.09\n      NaN\n      NaN\n      2146.09\n    \n    \n      4\n      -0.961383\n      0\n      2806.50\n      NaN\n      NaN\n      2806.50\n    \n  \n\n\n\n\n\n\n\nIt looks like the most unhappy customers are the most likely to receive a treatment as shown in the DAG below.\n\n\n\n\n\n\n\nG\n\n  \n\na\n\n unhappy customer   \n\nb\n\n receive promo   \n\na->b\n\n    \n\nc\n\n lifetime value   \n\na->c\n\n    \n\nb->c\n\n   \n\n\n\n\n\nThis is an example of selection bias (more specifically, its collider bias, a common confound). When comparing customers who had the treatment vs. didnt have the treatment, we accidentally also end up comparing unhappy customers vs. happier customers, and obviously unhappier customers tend to have worse lifetime value. We need to find a way to compare the impact of the treatment while controlling for the happiness of customers so that we are making a more fair comparison. For example, if we had 2 equally unhappy customers and 1 received the treatment while the other didnt, we’d get a more reasonable comparison for evaluating the treatment effect."
  },
  {
    "objectID": "posts/experiment_design.html#identification-under-selection-bias",
    "href": "posts/experiment_design.html#identification-under-selection-bias",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Identification Under Selection bias",
    "text": "Identification Under Selection bias\nHow can we represent the scenario above with math? This is where the Potential Outcomes model starts coming into play. Note I’m borrowing this directly from Scott Cunningham. For the full proof, see his book, Causal Inference the Mixtape.\n\\[\n\\begin{align}\n\\text{Simple Difference in Outcomes}\n&= \\underbrace{E[Y(1)] - E[Y(0)]}_{ \\text{Average Treatment Effect}}\\\\\n&+ \\underbrace{E\\big[Y(0)\\mid T=1\\big] - E\\big[Y(0)\\mid T=0\\big]}_{ \\text{Selection bias}}\\\\\n& + \\underbrace{(1-\\pi)(ATT - ATU)}_{ \\text{Heterogeneous treatment effect bias}}\n\\end{align}\n\\]\nThis equation for the Potential Outcomes model basically says that anytime you make a comparison on observational data, it ends up being the sum of the true average treatment effect, selection bias, and Heterogeneous Treatment effect (HTE) bias. HTEs are just a fancy way of saying the personalized effect, aka promos might be more impactful for some users than others.\nSo how does this relate to what we did before? Well when we tried to compare users who saw the treatment vs. those that didnt\n\\[\n\\text{SDO} = E[ Y(1) | T = 1 ] - E[ Y(0) | T = 0 ]\n\\]\nwe didnt take into account the fact that users who saw the treatment tend to be different than those who didn’t. Users who saw the treatment tend to be more unhappy by design.\nSo if we subtract out the selection bias from the SDO (I got this via simple algebra), aka we control for the unhappiness between customers, we can get closer to identifying the true ATE.\nNote that selection bias was \\[\nE\\big[Y(0)\\mid T=1\\big] - E\\big[Y(0)\\mid T=0\\big]\n\\]\nThis is just saying selection bias is the fundamental difference between users who get picked for treatment vs. those who dont.\nIn our case, the fundamental difference between whether users are selected for treatment is based upon their unhappiness. So if we can subtract out the effect of unhappiness, we can subtract out the selection bias\n\ndf.groupby(\"T\").mean()[[\"upset\"]].T\n\n\n\n\n\n  \n    \n      T\n      0\n      1\n    \n  \n  \n    \n      upset\n      -0.159046\n      1.018004\n    \n  \n\n\n\n\nWe can do this with OLS. The most obvious way is to fit a model relating unhappiness to LTV, and then subtract out that effect.\n\nmodel1 = sm.OLS.from_formula(\"Y ~ upset\", data=df.loc[lambda d: d[\"T\"]==0]).fit()\nY0_hat = model1.predict(df)\n\nselection_bias = (\n    df.assign(selection_bias = Y0_hat)\n    .groupby(\"T\").mean()\n    [[\"selection_bias\"]]\n)\nselection_bias.T.round(2)\n\n\n\n\n\n  \n    \n      T\n      0\n      1\n    \n  \n  \n    \n      selection_bias\n      2579.46\n      1990.96\n    \n  \n\n\n\n\nAnd finally we can subtract out the effect, ending up with an estimate very close to the true ATE of 500\n\n(\n    df.assign(selection_bias = Y0_hat)\n    .groupby(\"T\").mean()[[\"Y\", \"selection_bias\"]].T\n    .assign(difference = lambda d: d[1] - d[0])\n    [[\"difference\"]].T\n    .reset_index(drop=True)\n    .rename(columns={\"Y\":\"SDO\"})\n    .assign(tau = lambda d: d.SDO - d.selection_bias)\n    .round(2)\n)\n\n\n\n\n\n  \n    \n      \n      SDO\n      selection_bias\n      tau\n    \n  \n  \n    \n      0\n      -87.98\n      -588.5\n      500.52\n    \n  \n\n\n\n\nThere’s actually an even more simple way to control for selection bias - it can just be included as a term in an OLS regression model.\n\ndef statsmodels_to_df(model):\n    table = np.array(model.summary().tables[1].data)\n    return pd.DataFrame(table[1:, 1:], columns=table[0,1:], index=table[1:,0])\n\nmodel2 = sm.OLS.from_formula(\" Y ~ T + upset\", data=df).fit()\nstatsmodels_to_df(model2)\n\n\n\n\n\n  \n    \n      \n      coef\n      std err\n      t\n      P>|t|\n      [0.025\n      0.975]\n    \n  \n  \n    \n      Intercept\n      2499.9363\n      0.516\n      4847.317\n      0.000\n      2498.925\n      2500.947\n    \n    \n      T\n      500.5529\n      1.502\n      333.191\n      0.000\n      497.608\n      503.497\n    \n    \n      upset\n      -500.0068\n      0.518\n      -965.940\n      0.000\n      -501.021\n      -498.992\n    \n  \n\n\n\n\nAs we can see above the estimate of the treatment effect is the beta coefficient for T and it closely matches our manual estimate above."
  },
  {
    "objectID": "posts/experiment_design.html#a-quick-note-on-heterogeneous-treatment-effects",
    "href": "posts/experiment_design.html#a-quick-note-on-heterogeneous-treatment-effects",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "A quick note on Heterogeneous Treatment Effects",
    "text": "A quick note on Heterogeneous Treatment Effects\nWe’ve controlled for selection bias, what about Heterogeneous Treatment Effect bias? We actually don’t need to control for these once we’ve controlled for selection bias. These average treatment effect ends up being the average of all of the HTEs of individuals, which is fine because as long as we’ve accounted for selection bias, the HTEs tend to cancel out. They’re essentially captured by the error term, \\(\\epsilon\\) in OLS \\[\ny = \\alpha + \\beta X + \\epsilon\n\\]\nWe can also see that in our code, where the distribution of true HTE bias from our hypothetical dataset is centered at zero. Any time we’ve accounted for all selection bias, the HTE should be zero centered and cancel itself out as N increases.\n\n\nCode\nATE = data[\"ITE\"].mean()\nHTE = data.ITE.values - ATE\nsns.histplot(HTE)\nplt.xlabel(\"HTE\")\nplt.title(\"Distribution of HTEs (each customers difference from the ATE)\")\nplt.show()\n\n\n\n\n\nThe bias of HTEs for each person is just the distance their treatment effect is from the average treatment effect. Again, this follows the same property as the error term in OLS regression, which is why it can be such a powerful tool for causal inference when used correctly."
  },
  {
    "objectID": "posts/2022-04-17-out-of-sample-pymc.html",
    "href": "posts/2022-04-17-out-of-sample-pymc.html",
    "title": "Making out of sample predictions with PyMC",
    "section": "",
    "text": "Simulating data\nI simulated a 2 level hierarchical model - for interpretability, I set it up as a state > zipcode model. You can following along with the notebook here. The data is as follows\n\n\n\n\n\n\n\nUsing categorical variables\nCategorical variables are a somewhat new feature of pandas - they can store categories that aren’t in the observed data, and are an easy replacement for pd.factorize() (a common tool for those familiar with the bayesian workflow).\nWe can use these to trick pymc into thinking there’s a category with no observed data, and pymc ends up assigning the global distribution to that unobserved category, which we can simply reference in the future for any time we want to make a prediction on out of sample data.\n# Convert to categorical and add an `out_of_sample` category\ndf = df.assign(state = pd.Categorical(df.state).add_categories(\"out_of_sample\"))\\\n    .assign(zipcode = pd.Categorical(df.zipcode).add_categories(\"out_of_sample\"))\n\n\nFitting the model\nWe’ll use the codes from the categorical columns to index our model coefficients, and we’ll use the categories as coordinates for the model to map names to.\ncoords={\n    \"state\":df.state.cat.categories,\n    \"zipcode\":df.zipcode.cat.categories\n}\n\ndef hierarchical_normal(name, μ, dims):\n    '''Adapted from Austin Rochford'''\n    Δ = pm.Normal('Δ_{}'.format(name), 0., 1., dims=dims)\n    σ = pm.Exponential('σ_{}'.format(name), 2.5)\n    return pm.Deterministic(name, μ + Δ * σ, dims=dims)\n\n\nwith pm.Model(coords=coords) as model_nc:\n    \n    # Observed Data tracking\n    state_ = pm.Data(\"state_\", df.state.cat.codes)\n    zip_ = pm.Data(\"zip_\", df.zipcode.cat.codes)\n    obs = pm.Data(\"obs\", df.y)\n\n    # Hyperprior\n    mu_country = pm.Normal(\"mu_country\", 0, 1)\n    \n    # Prior\n    sig = pm.Exponential(\"sig\", 1)\n    \n    # Hierarchical coefficients\n    mu_state = hierarchical_normal(\"mu_state\", μ=mu_country, dims=\"state\")\n    mu_zipcode = hierarchical_normal(\"mu_zipcode\", μ=mu_state, dims=(\"zipcode\", \"state\") )\n    \n    # Observational model\n    y = pm.Normal(\"y\", mu_zipcode[zip_, state_], sig, observed=obs)\n    \n    # Fit \n    trace_nc = pm.sample(target_accept=0.9, return_inferencedata=True, random_seed=SEED)\nThere are a few key point that make out of sample prediction possible * Having the out_of_sample category for each indexed variable with no observed data * Passing the coords in the model statement * Using dims to reference which model coefficients have which coordinate labels * Having all of our input data wrapped in a pm.Data() statement\nThat last point is particularly important. For PyMC, if you want to make predictions on new data, you have to replace the data that the model references and the only way to do that (that I know of atleast) is to using a Theano shared variable. pm.Data() handles all of that fo you.\nSo we fit our model, lets take a quick look at the state level coefficients\npm.plot_forest(trace_nc, var_names=[\"mu_state\"])\n\n\n\n\n\nGreat, that out of sample variable seems to represent the global distribution across states - i.e. if we were to make a prediction for a new state we’d potentially use that distribtion (we’ll confirm further down).\nWe’ll check the zip code level below as well, looking at Maine specifically\n\n\n\n\n\nAs we can see, the out_of_sample variable has a sampled value despite there being no observed data for it. Now the question is, does this align with how we’d predict new data?\nLet’s try calculating coefficients out of sample by hand and see if it aligns with the out_of_sample values\npost = trace_nc.posterior\n\n# Pull the true data from our simulation\nstate_true = mu_state_true.random(size=4000)\n\n\n# Calculate out of sample state means by drawing from global distribution\nmu_country = post[\"mu_country\"].values.reshape(4000,-1)\nσ_state = post[\"σ_mu_state\"].values.reshape(4000,-1)\nmu_state = np.random.normal(mu_country, σ_state)\n\n# Using the indexing trick\nstate_idx_trick = post[\"mu_state\"].sel({\"state\":[\"out_of_sample\"]}).values.ravel()\n\n# Pull the true data from simulation\nzip_true = pm.Normal.dist(mu_state_true.random(size=4000), sig_zip_true).random(size=4000)\n\n# calculate out of sample mu by hand by drawing from out of sample state prediction above\nσ_zipcode = post[\"σ_mu_zipcode\"].values.reshape(4000,-1)\nmu_zipcode = np.random.normal(mu_state, σ_zipcode)\n\n# Use the indexing trick\nzip_idx_trick = (post[\"mu_zipcode\"]\n                .sel({\"state\":[\"out_of_sample\"], \"zipcode\":[\"out_of_sample\"]})\n                .values.ravel())\nWe can compare these results by plotting their distributions below\n\n\n\n\n\nNotice that the manual prediction and the indexing trick are basically identical. There’s a slight difference from the ground truth, but thats to be expected since we’re fitting a model on limited data (and anyway, it’s still quite close).\n\n\nPredicting out of sample\nLet’s go ahead and actually make prediction now - we’ll make predictions for the following data below\n\nThe first example is in sample\nThe second example is in sample for state, out of sample for zipcode\nThe third example is out of sample entirely\n\n\n\n\n\n\nAnd finally we’ll use the model to make predictions on this new data. Notice the pm.set_data() function - remember our pm.Data() calls from before? This tells PyMC to override that with new data, so when we sample from the posterior predictive it makes predictions on the new data instead of the data used to fit the model.\n\n\nClick here for helper function code\n\n# We're making some quick convenience functions to map this new data \n# to the proper indexes from the fitted model\nzip_lookup = dict(zip(df.zipcode.cat.categories, range(len(df.zipcode.cat.categories))))\nstate_lookup = dict(zip(df.state.cat.categories, range(len(df.state.cat.categories))))\n\ndef labels_to_index(series, lookup):\n    '''Converts categories to their proper codes'''\n    series = series.copy()\n    in_sample = series.isin(lookup.keys())\n    series.loc[~in_sample] = \"out_of_sample\"\n    return series.map(lookup).values.astype(\"int8\")\n\n\nwith model_nc:\n    # Set new data for the model to make predictions on\n    pm.set_data({\n        \"state_\": X.state.pipe(labels_to_index, state_lookup),\n        \"zip_\": X.zipcode.pipe(labels_to_index, zip_lookup)\n    })\n    \n    # make predictions\n    preds = pm.sample_posterior_predictive(trace_nc)\n\n\n\n\n\nThis is exactly what we were looking for - and prediction is easy, just map any out of sample states or zipcodes to the out_of_sample category. Notice how in sample predictions have smaller uncertainty intervals and out of sample data is more uncertain - this is exactly what we’d expect. This trick makes it much easier to make predictions compared to having to write out a custom prediction function that follows the same logic as the model.\nIf you have any other easy tricks for out of sample prediction let me know!"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "",
    "text": "The Weibull distirbution is an excellent choice for many survival analysis problems - it has an interpretable parameterization that is highly flexible to a large number of phenomenon. The main advantage is that it can model how the risk of failure accelerates over time. This post will focus on the \\(\\text{Weibull}(k, \\lambda)\\) parameterization, although I hope to cover the Gumbel reparameterization in the future.\nThis post requires some base understanding of survival analysis. I’ll try to have another post in the future that discusses survival analysis at a more introductory level\n\n\nSurvival Curves model time to some event (such as a failure) - they can tell you the probability of a binary event occurring at each future time point in somethings lifetime. An example survival curve is shown below:\n\n\nCode\nfrom typing import *\nimport numpy as np\nimport pymc as pm\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport seaborn as sns\nfrom weibull import Weibull\nimport scipy\nplt.style.use(\"seaborn\")\n\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nt = np.arange(1, 101)\nSt =  dist.survival(t)\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(t,St, label=\"Survival Curve\")\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1,0))\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Survival probability\", fontsize=16)\nax.set_title(\"How to Read a Survival Curve\", fontsize=16)\nax.legend(fontsize=16)\n\n# Annotations\nxlim = ax.get_xlim()\nylim = ax.get_ylim()\nx = 20\nax.vlines(x, -0.05, St[x-1], color=\"k\", ls=\"--\", alpha=0.75, lw=1.5)\nax.hlines(St[x-1], 0, x, color=\"k\", ls=\"--\", alpha=0.75, lw=1.5)\nax.tick_params(axis='both', which='major', labelsize=14)\n\nax.annotate(f'\"There\\'s a ~60% probability of an event \\nstill not occurring by time t=20\"', \n        xy=(x,St[x-1]),\n        xycoords=\"data\", \n        xytext=(0.4, 0.8),\n        textcoords='axes fraction',\n        arrowprops=dict(facecolor='black', shrink=0.05),\n        horizontalalignment='left',\n        verticalalignment='top',\n        fontsize=12,\n        bbox=dict(boxstyle=\"Round,pad=0.5\", fc=\"white\", ec=\"gray\", lw=2)\n)\nplt.show()\n\n\n\n\n\nThey basically tell you the probability of a unit not observing an event up until some time point.\n\n\n\nHazard Curves are another way to think about survival analysis problems. A hazard curve tells you the instantaneous risk of an event occurring at each time point.\nHazard Curves tend to be very informative as they allow you to see how risk changes over time - sometimes it might decelerate, or sometimes it might even accelerate exponentially. Here’s an example of a hazard curve below.\n\n\nCode\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nt = np.arange(1, 101)\nht =  dist.hazard(t)\n\nfig, ax = plt.subplots(figsize=(8,5))\nax.plot(t,ht, label=\"Hazard Curve\")\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Hazard Rate\", fontsize=16)\nax.set_title(\"An Example Hazard Curve\", fontsize=16)\nax.legend(fontsize=16)\n\nplt.show()"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-k-parameter",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-k-parameter",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "The \\(k\\) Parameter",
    "text": "The \\(k\\) Parameter\nSometimes also called \\(\\rho\\), this parameter controls the degree of acceleration of the hazard curve.\n\n\nCode\nks = [0.5, 1, 1.5, 2, 2.5]\nlambd = 30\nt = np.arange(1,51)\n\nfig, ax = plt.subplots(figsize=(8,5))\n\nfor i in range(len(ks)):\n    dist = Weibull(ks[i], lambd)\n    ht = dist.hazard(t)\n    ax.plot(t, ht, label=f\"k={ks[i]}\", lw=5)\n\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Hazard Rate\", fontsize=16)\nax.set_title(\"Hazard Curves with Varying k Parameters\", fontsize=16)\nax.legend(fontsize=16)\nplt.show()\n\n\n\n\n\nAs shown above, here’s how different parameter values impact acceleration:\n\nWhen k < 1: Hazard decelerates\nWhen k = 1: Hazard is constant. This is equivalent to an exponential distributions hazard function.\nWhen k > 1: Hazard increases logarithmically\nWhen k = 2: Hazard increases with constant acceleration\nWhen k > 2: Hazard increases exponentially\n\nClothing is a great example of something where the hazard increases over time - the risk of a clothing item getting damaged obviously increases over time as the item is worn, washed, gets loose stitching, etc."
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-lambda-parameter",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#the-lambda-parameter",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "The \\(\\lambda\\) Parameter",
    "text": "The \\(\\lambda\\) Parameter\nSometimes also referred to as a scale parameter, this parameter represents the time point where there’s a 63.2% probability of an event having occurred. Weird, I know, its just an arbitrary thing quality\n\nk = 1.5\nlambd = 30\ndist = Weibull(k, lambd)\nt = np.linspace(1,100, 10000)\nSt = dist.survival(t)\n\n# choose the point in the survival curve where the time is t=30\n# take the inverse of it so that its the probability of an event having occurred by that time.\n(1-St[t>=lambd][0]).round(4)\n\n0.6321\n\n\nLet’s see this visually as well - if you look at the plot below, each survival curve indicates that there’s a 63.2% probability of an event having occurred at the exact time that \\(t=\\lambda\\)\n\n\nCode\nks = 1.5\nlambds = [15, 30, 60, 120]\nt = np.arange(1,151)\n\nfig, ax = plt.subplots(figsize=(10,5))\n\nfor i in range(len(lambds)):\n    dist = Weibull(k, lambds[i])\n    St = dist.survival(t)\n    ax.plot(t, St, label=f\"lambd={lambds[i]}\", lw=5)\n    ax.axvline(lambds[i],ls=\"--\", alpha=0.5, color=f'C{i}')\n\nax.axhline(1-0.632, color=\"k\", ls=\"--\", alpha=0.75,\n    label=\"Point where there's a 63.2% probability\\nof the event having occurred\\nby that time\")\n\nax.set_xlabel(\"Time\", fontsize=16)\nax.set_ylabel(\"Hazard Rate\", fontsize=16)\nax.set_title(\"Hazard Curves with Varying k Parameters\", fontsize=16)\nax.legend(fontsize=12, loc=\"upper right\")\nplt.show()"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#easier-predictions-lifetime-survival-curves-and-hazard-curves",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#easier-predictions-lifetime-survival-curves-and-hazard-curves",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "Easier Predictions: Lifetime, Survival Curves, and Hazard Curves",
    "text": "Easier Predictions: Lifetime, Survival Curves, and Hazard Curves\nAverage lifetime, Survival Curves, and Hazard Curves are the basic types of predictions for working with survival analysis. Most distributions have formulas that make these things easier to calculate. Below is the underlying code for these functions.\nclass Weibull:\n\n    def __init__(self, k, lambd):\n        self.k = k\n        self.lambd = lambd\n\n    ...\n\n    def expectation(self) -> np.array:\n        '''Calculates the expectation of the weibull distribution\n        '''\n        return self.lambd * sp.gamma(1 + 1/self.k)\n\n    def survival(self, t: np.array) -> np.array:\n        '''Outputs the survival probability at each time point T. This is done with the survival function, \n        the complement of the Weibull Distribution's PDF.\n\n        Parameters\n        -----------\n            t: A numpy array with time points to calculate the survival curve,      \n                utilizing the distributions parameters\n        \n        Returns\n        -------\n            St: A survival curve calculated over the inputted time period\n        '''\n        CDF = 1 - self.cdf(t)\n        St = 1 - CDF\n        return St\n\n    def hazard(self, t: np.array) -> np.array:\n        '''Outputs the hazard rate at each time point T.\n\n        Parameters\n        -----------\n            t: A numpy array with time points to calculate the survival curve,      \n                utilizing the distributions parameters\n        \n        Returns\n        -------\n            St: A survival curve calculated over the inputted time period\n        '''\n        ht = (self.k/self.lambd)*(t/self.lambd)**(self.k-1)\n        return ht\n    \n    ...\nWe already looked into hazard and survival curves - let’s take a look at estimating the average lifetime.\n\nk = 1.5\nlambd = 30\n\ndist = Weibull(k, lambd)\nEt = dist.expectation()    \nprint(f\"Expected Average Lifetime = {round(Et,1)}\")\n\nExpected Average Lifetime = 27.1\n\n\nDoes this align with reality? Lets simulate event times from the weibull distribution to find out\n\nevent_times = dist.sample(n=1000000)\nprint(f\"Simulated Average Lifetime = {round(event_times.mean(),1)}\")\n\nSimulated Average Lifetime = 27.1"
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#predicting-residual-remaining-life",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#predicting-residual-remaining-life",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "Predicting Residual Remaining Life",
    "text": "Predicting Residual Remaining Life\nAnother common and useful prediction you may want to present is the remaining life. There are a few different ways to calculate this:\n1. Use the formula\n\\[\nMRL(t) = \\frac{\\lambda \\: \\Gamma(1 + 1/k, (\\frac{t}{\\lambda})^k)}{S(t)} - t\n\\]\nAnd we can see it implemented in python below:\nclass Weibull:\n\n    def __init__(self, k, lambd):\n        self.k = k\n        self.lambd = lambd\n\n    ...\n\n    def mean_residual_life(self, t: np.array) -> np.array:\n        t = self._normalize_to_array(t)\n        St = self.survival(t)\n        numerator = (\n            self.lambd \n            * sp.gammaincc(1+1/self.k, (t/self.lambd)**self.k)\n            * sp.gamma(1+1/self.k))\n\n        result = np.divide(\n            numerator,\n            St,\n            out=np.zeros_like(numerator),\n            where=St!=0\n        ) - t[:,None]\n        \n        # The code above returns negative values when St=0. This clipping corrects those cases\n        return np.clip(result, 0, np.inf)\n\ncurr_time = 40\nk, lambd = 1.5, 30\ndist = Weibull(k,lambd)\nremaining_life = dist.mean_residual_life(curr_time)\n\n2. Simulate\n\nT = dist.sample(n=10000000)\nunits_reached_curr_time = T[T>curr_time] - curr_time\nremaining_life_simulated = units_reached_curr_time.mean()\n\nWith truncated sampling, this can also be more efficient\n\nT_cond = dist.sample(n=10000000,left_trunc=curr_time) - curr_time\nremaining_life_simulated2 = T_cond.mean()\n\n3. Transform the hazard curve\nThere’s a convenient relationship between the hazard curves, survival curves, and expectations. It turns out that\n\\[\nS(t) = \\text{exp}(-Ht)\n\\]\nwhere \\(Ht\\) is the cumulative hazard function. Additionally, the expectation is just the integral of the survival function\n\\[\nE[T] = \\int_{t=0}^\\infty S(t) \\, dt\n\\]\nUsing these relationships, we can 1. take the hazard curve from time 40 onwards 2. turn it into the cumulative hazard 3. transform that into a survival curve 4. integrate the survival curve into an expectation\n\nht = dist.hazard(np.arange(curr_time, curr_time+1000))\nHt = ht.cumsum()\nSt = np.exp(-Ht)\nremaining_life_from_hazard = scipy.integrate.simps(St)\n\nComparing all of these methods we end up with the following:\n\n\nCode\nprint(\n    \"Remaining Life (from formula) = {:.4}\\n\".format(remaining_life.ravel()[0]),\n    \"Remaining Life (from simulation) = {:.4}\\n\".format(remaining_life_simulated),\n    \"Remaining Life (from truncated simulation) = {:.4}\\n\".format(remaining_life_simulated2),\n    \"Remaining Life (manually calculated from hazard) = {:.4}\\n\".format(remaining_life_from_hazard)\n)\n\n\nRemaining Life (from formula) = 15.05\n Remaining Life (from simulation) = 15.04\n Remaining Life (from truncated simulation) = 15.05\n Remaining Life (manually calculated from hazard) = 14.14\n\n\n\nThey’re all very close, but it looks like theres some slight error when manually calculating (and its more complicated)."
  },
  {
    "objectID": "posts/weibull_survival_analysis/weibull_survival_tools.html#calculating-conditional-survival-curves",
    "href": "posts/weibull_survival_analysis/weibull_survival_tools.html#calculating-conditional-survival-curves",
    "title": "Useful Tools for Weibull Survival Analysis",
    "section": "Calculating Conditional Survival Curves",
    "text": "Calculating Conditional Survival Curves\nThe last type of prediction we’ll show here is conditional survival. This is basically saying “what’s the survival curve if we’ve made it up to time=t?”\nIt turns out, all you have to do is calculate the survival curve and normalize it by the eligible area of the distirbution. This basically means calculating a survival curve past the current time, and then dividing it by the survival function at the current time of interest.\n\\[\nS(t|\\text{curr time}=40) = S(t)/S(40)\n\\]\nIt was pretty easy to add that to the Weibull.survival() function from earlier:\nclass Weibull:\n\n    def __init__(self, k, lambd):\n        self.k = k\n        self.lambd = lambd\n    \n    ...\n\n    def survival(self, t: np.array, curr_time: Optional[int] = None) -> np.array:\n        '''Outputs the survival probability at each time point T. This is done with the survival function, \n        the complement of the Weibull Distribution's PDF.\n\n        Can also be used to calculate conditional survival with the `curr_time` argument.\n\n        Parameters\n        -----------\n            t: A numpy array with time points to calculate the survival curve,      \n                utilizing the distributions parameters\n            curr_time: Used to calculate the survival curve given already reaching \n                some point in time, `curr_time`.\n        \n        Returns\n        -------\n            St: A survival curve calculated over the inputted time period\n        '''\n        # Normalizing constant used for conditional survival\n        norm = 1 if curr_time is None else self.survival(curr_time)\n        \n        # check inputs\n        t = self._normalize_to_array(t)\n        if curr_time is not None and (t < curr_time).sum() > 1:\n            raise ValueError('t<curr_time. t must be greater than or equal to curr_time')\n        \n        St = (1 - self.cdf(t))/norm\n        return St\nAnd as we can see below, it lines up perfectly with an empirically calculated kaplan meier curve\n\nfrom lifelines import KaplanMeierFitter\n\ncurr_time = 40\nT_cond = dist.sample(1000000, left_trunc=curr_time) - curr_time\nkmf = KaplanMeierFitter()\nkmf.fit(T_cond)\nkmf.plot_survival_function(lw=5, ls=\"--\")\n\nplt.plot(dist.survival(t=np.arange(40, 200+1), curr_time=curr_time), label=\"Conditional Survival Curve\")\nplt.legend()\nplt.ylabel(\"Conditional Survival Rate\")\nplt.title(f\"Conditional Survival Given a Unit Reached Time={curr_time}\")\nplt.show()\n\n\n\n\nA convenient feature about survival curves is that if we want to know the probability of an event occurring in the next 1 time period, or 2 time periods, all we have to do is take the complement of the survival probability. So that means we can also use this method to calculate “probability of an event occurring in the next 1 time period”"
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html",
    "title": "Desiging an Experimentation Strategy",
    "section": "",
    "text": "Experiments have alot more use cases than many give them credit for. At their simplest, they’re a tool for mitigating risk when making product decisions. But at their best, they’re a tool that can help optimize an entire business.\nEvery business has its own unique problems and goals, and this post is a case study where strategy was made across experiments to fit the needs of the business."
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#early-stage-needs---starting-simple-building-trust",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#early-stage-needs---starting-simple-building-trust",
    "title": "Desiging an Experimentation Strategy",
    "section": "Early stage needs - Starting Simple, Building trust",
    "text": "Early stage needs - Starting Simple, Building trust\nWhen I joined the company, I was just the third data science hire, and there really wasn’t any experimentation practice in place beyond stakeholders watching an optimizely dashboard. Listening to stakeholders, there was a desire to build more trust in decision making and make sure their products decisions were actually improving things for the customer.\nWe started simple - a google doc template where stakeholders could explain the product feature they wanted to implement and their hypothesis on what outcome the product feature might improve. From there we worked with them to design a measurement strategy to answer their question, which wasn’t always an experiment. At the end of the day, experimentation is just one of many tools for causal inference. Having this process and flexibility helped us earn buy-in - we weren’t just there to tell them what we could and couldn’t do. We were there to educate so that we weren’t just recommending solutions, but so that stakeholders were playing a really active role in what our measurement strategy should be.\n\n\n\n\n\nWe also improved the experiment output from an optimizely dashboard to an in-house report. All of this built alot more trust in the results, and helped us move on to new questions like\n\n“Can we use different outcome metrics beyond conversion rate?”\n“Can we make experiments shorter?” (power analyses, variance reduction)\n“Can we launch a feature without hurting the business even if we can’t confirm it’s better?”” (uncertainty quantification)\n“What’s the right risk tolerance in terms of being able to detect an effect versus risking a false positive” (here’s a fun example)\n“Can we start experimenting like this on our other products?”\n“Can we learn how our change in one part of the funnel may have an impact further downstream?”\n“How can we run experiments that mitigate regret?” (Bandit algorithms)\n“Can we stop an experiment early and still have confidence in the results?” (sequential testing, alpha spending)\n“Who is the experiment working best for?” ( Multiple Hypothesis Testing, Heterogeneous Treatment Effects )\n\nAgain, we weren’t just throwing cool implementations at them we thought they needed, we were listening to what our stakeholders wanted first and then prioritizing it. As we moved up the ladder in terms of trust, I was able to get buy in to build out scalable tooling for others in the company to start usin, and we were able to scale the process to other teams.\nThere was also 1 key tenant I tried to keep in mind as our methodology became more advanced: could we directly observe the treatment effect? (simple difference in outcomes). No matter how advanced of a method we might consider, throwing some output from a blackbox model at a stakeholder was never the way to go. It can’t be understated how important directly observing the treatment effect is for trust."
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#whats-a-surrogate-index",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#whats-a-surrogate-index",
    "title": "Desiging an Experimentation Strategy",
    "section": "What’s a surrogate index?",
    "text": "What’s a surrogate index?\nA surrogate index was a new piece of research from Susan Athey, basically a way to predict what a long term treatment effect would be given intermediate outcomes that could be observed sooner. It also neatly addressed the biggest problem we were facing.\nThe basic idea is illustrated in the DAG below.\n\n\n\n\nflowchart LR\n  B(Treatment) --> c(Outcome 1)\n  B(Treatment) --> d(Outcome 2)\n  B(Treatment) --> e(Outcome 3)\n  c(Outcome 1) --> f(Long Term\\nOutcome)\n  d(Outcome 2) --> f(Long Term\\nOutcome)\n  e(Outcome 3) --> f(Long Term\\nOutcome)\n\n\n\n\n\n\n\n\nLet’s say the long term outcome can be entirely explained by 3 intermediate outcomes, such as mid-funnel metrics that can be observed sooner. A model can be fit to that relationship on historical data like so:\n\n\n\n\nflowchart LR\n  c(Outcome 1) --> f(Long Term\\nOutcome)\n  d(Outcome 2) --> f(Long Term\\nOutcome)\n  e(Outcome 3) --> f(Long Term\\nOutcome)\n\n\n\n\n\n\n\n\nThe set of intermediate outcomes is known as a surrogate index; they’re a surrogate for the long term outcome. If you have a correct model for the DAG above, than all you need to do is observe outcomes 1-3. You could then just run an experiment as you normally would, but instead of using some game-able mid-funnel metric as the primary outcome, you could measure outcomes 1-3 for both the treatment and control group, and predict what the long-term outcome would be for each group. The difference in the prediction for the treatment group and the control group ends up being identical to the true long-term treatment effect (assuming that the model and dag are specified correctly).\nAs always, it’s easier said than done. You need to make sure that the entire intermediate effect between the treatment and the long-term outcome is captured by the DAG and the model. Or in english, you need to understand the exact effect that the intermediate outcomes have on the long term outcome, and you need to be right about it; you can’t be missing anything.\nBut there are ways to validate you’re on the right track. First, knowing something about DAGs and causal inference, you can call on the local markov property; the treatment should be conditionally independent of the long term outcome after conditioning on the surrogate index. That’s easy to test. One can alse make predictions on the long term outcome with their surrogate index and see if it ended up being correct when the long-term outcome gets observed (hopefully for repeated experiments). Also do-able. Even better, if you have many historical experiments, you could do this validation on that dataset across all of the experiments.\n\n\nCode\n# for simplicity, not actually simulating a true data generating process\nnp.random.seed(99)\nx = np.random.normal(0,3,size=25)\ntrue_north_metric = x*1.4 + np.random.normal(0, 0.5, size=len(x))\npredicted_true_north = np.random.normal(true_north_metric, 1)\n\nfig, ax = plt.subplots(figsize=(7,5))\nax.scatter(predicted_true_north, true_north_metric)\nax.plot([-8,7], [-8,7], ls='--')\n\nax.set_xticks([])\nax.set_yticks([])\nax.set(xlabel='Predicted True North', ylabel='True North Metric', title='Surrogate Index vs. Long Term Outcome over many experiments')\nplt.show()\n\n\n\n\n\nUnfortunately, we didn’t have a large dataset of historical experiments. For the experiments we did have, they didn’t have enough power to detect reasonably sized changes on our true-north bottom funnel metric. Those experiment had all been designed with mid-funnel metrics in mind, and therefore had smaller sample sizes."
  },
  {
    "objectID": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#building-a-plan",
    "href": "posts/experiment_strategy/2023-06-09-experiment-strategy.html#building-a-plan",
    "title": "Desiging an Experimentation Strategy",
    "section": "Building a plan",
    "text": "Building a plan\nIn total there were 5 big problems to address, each with a different solution. There also weren’t alot of resources to devote to them. But if we could free up time for others running experiments, they could use some of that time to contribute to this roadmap.\nIn the end, the biggest priority was to implement surrogate indices, because that was having the biggest painpoint on our business - we couldnt keep working with game-able metrics that weren’t improving the business.\nWith that in mind, switchback experimentation was in direct conflict with that goal. Switchback experiments don’t allow you to observe long term outcomes because users arent the ones who are randomized, time periods are what is randomized. We could atleast monitor and set up guardrails to make sure there wasnt alot of spillover.\nThis also meant that bandit algorithms shouldn’t be part of our toolkit any time soon, even though there had been alot of interest in them. Bandit algorithms wouldn’t be ideal for learning the impact on long term outcomes, and using them to learn a surrogate index would be really difficult since the treatment propensity would be constantly changing over time.\nI’ll summarize some of the easy and hard solutions I came up with:\n\nInfrastructure\n\nSolution: templated SQL for scalable experiment queries\n\n“Who did the experiment work best for?”\n\nEasy solution(s): Scalable code for Multiple hypothesis testing with corrections\nHard solution: Heterogeneuous Treatment Effect models\n\nSample Size problems\n\nEasy solution(s): Quickly implement Informative priors. Since we had revenue as part of our overall evaluation criteria for some experiments, this provided a massive speedup\nHard solution(s): CUPED, Cohort Models\n\nSpillover Effects\n\nEasy Solution(s): Monitoring, guardrails\nHard Solution(s): Switchback Experimentation Engine\n\nOptimizing for Long-term Outcomes\n\nEasy Solution(s): Cohort monitoring, Longer experiments, long term hold-out groups\nHard solution(s): Surrogate Index approach\n\n\nThe final proposed plan ended up being the following:\n\nPartner with the data team for each product pillar to build templated SQL for their experiments to save them time.\nAutomate the secondary analysis of experiments so that stakeholders could understand who the experiments worked best for. We actually chose the easy solution for this, multiple hypothesis testing with corrections. It was easy to implement it and it aligned with our principle of making sure the treatment effect is observable. Plus, if we ever got the point of heterogenous treatment effect modeling, it’d be alot more believeable for stakeholders if they could also look at the multiple hypothesis test reporting. This was a low lift way to free up the teams time and help our stakeholders answer what they wanted. Importantly, we lumped in metric observability with this so that we could understand which experiments were impacting which metrics.\nReduce experiment duration with informative priors and CUPED. Many experiments had an overall evaluation criteria that incorporated customer revenue, Customer revenue was wide-tailed (near lognormally distributed). Using priors was an incredibly easy and insanely effective way to reduce variance for experiments, speeding them up significantly. CUPED would be a way to take that even further. We planned for a data scientist to implement this with their newly freed up time after automating secondary analyses.\nSet up monitoring for spillover effects, but don’t implement switchback experiments.\nCenter the life-insurance product’s experimentation strategy around a surrogate index. This meant running longer experiments so that there was enough power to detect effects on the true north metric. Hopefully after 6 months, we’d have enough information to build a correct surrogate index to start using.\n\nThere was still a big red flag. We’d have to run longer experiments, and we’d have to wait atleast 6 months to even start using this. That was met with alot of hesitation. Deviating from 1-2 week experiment cadences admittedly sounds a bit nuts. Companies need to grow, and to do that they need to change and try new things. But at the same time we had a problem we couldn’t avoid - the simple fact of the matter was that if we wanted to have conviction in our product changes, we’d have to wait 3-6 months to actually observe its down funnel impact.\nWe decided to compromise rigor for speed. We planned to immediately implement a surrogate index based on the small amount of data we had, begin running experiments with enough power to detect an effect on our true north metric (3-4 weeks), and use the surrogate index, a guardrail, and our usual mid-funnel outcome as joint primary outcome metrics for upcoming experiments. While not ideal, it’d allow us to start using these ideas immediately, and we’d also be able to build up a better dataset of experiments so that even if the first iteration of the surrogate index didn’t work, we could eventually learn the correct one. That, and we’d have enough power to observe treatment effects on the long term outcome (after waiting a long time).\nReally, our overall strategy was a shift to improving observability. We created better observability on how experiments were affecting different outcomes and different customers and we created better observability on how our experiments were impacting our true north metric. The funny part is, early in my career I thought this was a big red flag - the more you observe the more likely you could be to identify a false positive. But thats impractical advice. You really have to be looking at the whole picture, that way you can start to understand the business and learn more. Just do so responsibly."
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html",
    "title": "Uncertainty Intervals or p-values?",
    "section": "",
    "text": "Uncertainty Intervals are better than p-values. Sure, its better to use both, but p-values are just a point estimate and they bring no concept of uncertainty in our estimate - this can lead to situations where we expose ourselves to high downside risk.\nTake the following example for instance. Let’s say we’re running a “Do no harm” A/B test where we want to roll out an experiment as long as it doesnt harm conversion rate.\nIf you want to follow along with the code, see here."
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#the-experiment-design",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#the-experiment-design",
    "title": "Uncertainty Intervals or p-values?",
    "section": "The experiment design",
    "text": "The experiment design\nGiven the stakeholders want to rule out a drop in conversion, and ruling out small differences requires large sample sizes, we decide to design an experiment with good power to detect the presence of a 1/2% absolute drop (if one were to truly exist)\nWe ran a power analysis and found that in order to have a 90% probability of detecting (power=0.9) a 1/2% absolute drop in conversion rate with 80 percent confidence ( 𝛼=0.2 ), we need N=32500 per group\n\nStatisticians might not love this interpretation of a power analysis, but its a useful and interpretable translation and tends to coincide with what we’re aiming for anyway. In reality, frequentist power analyses assume that the null hypothesis is correct, which isn’t quite what we want, not to mention, frequentist power analyses use backwards probabilities which are just plain confusing - see here to for more\n\nNote that we’re prioritizing power here for a reason. If 𝛼 is false positive rate, and power is probability of detection, then don’t we want to prioritize our probability of detecting a drop if one truly exists? A false negative here would be more expensive then a false positive\npA = 0.1 # historical conversion rate\nabs_delta = 0.005 # minimum detectable effect to test for\n\n# Statsmodels requires an effect size \n# (aka an effect normalized by its standard deviation)\nstdev = np.sqrt( pA*(1-pA) ) # bernoulli stdev, sigma = sqrt(p(1-p))\nES = abs_delta / stdev \n\n# estimate required sample size\nsm.stats.tt_ind_solve_power(\n    -ES, \n    alpha=0.2,\n    power=0.9,\n    alternative=\"smaller\"\n)\nRunning the code above leads us to conclude are sample size should be roughly 32,500 users per group.\n\nThe experiment\nI’m going to simulate fake data for this experiment where * The control has a true conversion rate of 10% * the variant has a true conversion rate of 9.25%\nFor examples sake we’ll pretend we don’t know that the variant is worse\n\n\nClick here for code\n\n# Settings\nnp.random.seed(1325)\nN = 32500\npA = 0.1\npB = 0.0925\n\n# Simulation\ndef simulate_experiment(pA, pB, N_per_group):\n    \n    df = pd.DataFrame({\n        \"group\":[\"A\"]*N + [\"B\"]*N,\n        \"convert\":np.r_[\n             np.random.binomial(1, p=pA, size=N),\n             np.random.binomial(1, p=pB, size=N)\n        ]\n    })\n    \n    return df\n\ndf = simulate_experiment(pA, pB, N)\n\n\n\n\n\n\n\nLooking at the data above, we’re seeing a better conversion rate in group B. We run a two-proportions z-test and we find that there’s a non-significant p-value, meaning we found insufficient evidence of the variant having lower conversion than the control.\ndef pval_from_summary(tab):\n    \n    _, pval = sm.stats.proportions_ztest(\n        count=tab[\"converts\"][::-1], \n        nobs=tab[\"N\"][::-1],\n        alternative=\"smaller\"\n    )\n    return pval\n\n(df.pipe(summarize_experiment)\n   .pipe(pval_from_summary))\n\\[ p = 0.38 \\]\nWe recommend to our stakeholders to roll out the variant since it “does no harm”\nThere are some serious red flags here\n\nFirst of all, p-values are all about the null hypothesis. So just because we don’t find a significant drop in conversion rate, that doesnt mean one doesnt exist. It just means we didnt find evidence for it in this test\nThere was no visualization of the uncertainty in the result"
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#understanding-uncertainty-with-the-beta-distribution",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#understanding-uncertainty-with-the-beta-distribution",
    "title": "Uncertainty Intervals or p-values?",
    "section": "Understanding Uncertainty with the Beta Distribution",
    "text": "Understanding Uncertainty with the Beta Distribution\nFor binary outcomes, the beta distribution is highly effective for understanding uncertainty.\nIt has 2 parameters * alpha, the number of successes * beta, the number of failures\nIt’s output is easy to interpret: Its a distribution of plausible probabilities that lead to the outcome.\nSo we can simply count our successes and failures from out observed data, plug it into a beta distribution to simulate outcomes, and visualize it as a density plot to understand uncertainty\n\n\n\n\n\nIt’s also easy to work with - if we want to understand the plausible differences between groups, we can just take the differences in our estimates\n\\[\n\\delta = \\hat{p_B} - \\hat{p_A}\n\\]\n\n\n\n\n\nWith visualization, we get a very different picture than our non-significant p-value. We see that there’s plenty of plausibility that the control could be worse.\nWe can further calculate the probability of a drop, \\(P(B < A)\\), and find that theres a greater than 60% probability that the variant is worse than the control\n# P(B < A)\n(pB_hat < pA_hat).mean()\n\\[\nP(B < A) = 0.62\n\\]\nRemember when we designed the experiment? Considering our main goal was to do no harm, we might not feel so confident in that now, and rightly so, we know the variants worse since we simulated it.\nUnless we feel very confident in our choice of testing for a 1/2% drop and know that we can afford anything up to that, then we we really shouldnt roll out this variant without further evaluation\nThis is particularly important with higher uncertainty As we can see in the example below, where the observed conversion rate is better in the variant, but the downside risk is as high as a 4% drop in conversion rate\n\n\n\n\n\n\\[\np = 0.59\n\\]"
  },
  {
    "objectID": "posts/2022-04-03-explainable-ai-is-not-causal.html",
    "href": "posts/2022-04-03-explainable-ai-is-not-causal.html",
    "title": "Explainable AI is not Causal Inference",
    "section": "",
    "text": "We’re going to walk through an example that shows these tools fall victim to the same rules of causal inference as everything else. A confound is still a confound, and if you want to measure some causal effect there’s still no way around that without careful deliberation of which variables to include in your models.\nThe code for this blogpost can be found here\n\nStarting simple: simulating some fake data\nLet’s start with a simple scenario. Our goal is to estimate some causal effects. We’re going to simulate out data ourself so we know the true causal effects. We can see how good some popular “explainable AI” algorithms actually are at causal inference. We’ll simulate data from the following DAG:\n\n\n\n\n\n\nWhat’s a DAG? A dag is a directed acyclic graph, or fancy talk for a flowchart that goes in 1 direction. It’s really just a diagram of a true data generating process. They’re typically assumed based on domain knowledge (like all models), although ocassionally there are some validation checks you can perform.\nEdges in the graph are assumed to be true causal effects. So for example,\n\nX3 influences Y\nX5 influences X1 which influences Y\nSome unobserved variable U influences both X1 and Y. By unobserved, what I mean is that its some variable we don’t have data for.\n\nFor those familiar with causal inference, this DAG in particular is also riddled with confounds.\n\nOk back on track. We’ll get out one of the more popular Explainable AI tools nowadays, XGBoost. I’m going to start in the most dangerous way possible - I’m going to toss everything in the model.\n\n\nTest 1: What’s the impact of X1 on Y?\nWe know for a fact that X1 influences Y. Let’s see how well Partial Dependence Plots and SHAP values do at identifying the true causal effect\n\n\n\n\n\nThese SHAP values arent just wrong, but the effect is in the wrong direction. The reason for this: there’s a Fork Confound.\n\n\n\n\n\nSome variable Z confounds Xs true effect on Y.\n\nA very common example of a fork confound is warm weather (Z) on the relationship between ice cream sales (X) and crime (Y). Ice cream sales obviously have no influence on crime, but ice cream sales are higher during warmer weather, and crime is higher during warmer weather.\n\nSo back to our main point - Explainable AI can’t get around a fork confound. This is our first lesson on why SHAP / explainable AI is different from causal inference.\nLuckily in this case, statistics can solve this problem.\nUsing some domain knowledge about the generating process, we notice an instrument, X5, that can be used to estimate the causal effect of X1 on Y\n\n\n\n\n\nI won’t go into the details of instrumental variable analysis since the goal of this article is to highlight that Explainable AI can’t replace causal inference. To learn more about it, see Scott Cunningham’s Causal Inference the Mixtape.\nBut for now, I will show that a classic causal inference method succeeds where XGBoost and SHAP values fail\nfrom linearmodels import IV2SLS\nfrom src.dagtools import get_effect\n\n# Instrumental variable analysis\niv_model = IV2SLS.from_formula(\"Y ~ 1 + [X1 ~ X5]\", data=df).fit()\n\n# pull true effect\ntrue_effect = get_effect(DAG, \"X1\", \"Y\")\n\n# Plot\nfig, ax = plt.subplots(1,1,figsize=(6,4))\nax.set_title(\"Instrumental Variable Analysis\\nRecovers True effect\")\nplot_model_estimate(iv_model, true_effect=true_effect, feat=\"X1\", ax=ax)\n\n\n\n\n\nAs we can see, a simple statistics technique succeeds where explainable AI fails.\n\n\nWhat about estimating the effect of X4 on Y?\nThis relationship is slightly more complicated, but certainly measurable. X4 influences X2 which influences Y. Here’s the DAG again for reference\n\n\n\n\n\nThe plots below show how well explainable AI does at estimating the causal effect of this relationship.\n\n\n\n\n\nUnfortunately, they don’t pick up an effect at all! And if our goal was to increase Y we’d end up missing a pretty good lever for it. There’s another simple explanation here for why explainable AI: there’s a Pipe Confound\n\n\n\n\n\nWhen trying to measure the effect of X -> Y, conditioning on Z (aka including it in a model as a covariate with X) ends up blocking inference.\nFor more details on how a Pipe confound works, I recommend chapters 5 and 6 of Richard McElreath’s Statistical Rethinking v2 (where I borrowed the example from as well).\nThe main things to note here are that pipes are common and Explainable AI doesn’t get around them.\nWe can recover an unbiased estimate of the true effect simply with OLS\n# Fit simple OLS model\nmodel = sm.OLS.from_formula(\"Y ~ X4\", data=df).fit()\n\n# pull true effect\ntrue_effect = get_effect(DAG, \"X4\", \"X2\") * get_effect(DAG, \"X2\", \"Y\")\n\n# Plot (see notebok for plot_model_estimate function)\nfig, ax = plt.subplots(1,1,figsize=(6,4))\nax.set_title(\"Instrumental Variable Analysis\\nRecovers True effect\")\nplot_model_estimate(model, true_effect=true_effect, feat=\"X4\", ax=ax)\n\n\n\n\n\n\n\nCan we use Explainable AI for causal inference at all?\nWe can! We just need to be deliberate in which variables we include in our models, and the only way to do that right is to use DAGs! The example below looks at an XGBoost model that doesnt condition on X2 (allowing us to estimate the causal effect of X4 -> Y).\n\n\n\n\n\n\n\nTake Aways\nExplainable AI is not some magic tool for causal inference. What these tools are good at is explaining why complicated models make the decisions they do. Explainable AI tools suffer from the same limitations for causal inference as all other statistical estimators.\nAt the end of the day when causal inference is your goal, nothing beats using DAGs to inform deliberate variable selection.\nIf you’re new to the subject, I highly recommend the following resources that will teach you how to use causal inference properly:\n\nChapter’s 5 and 6 of Statistical Rethinking v2, by Richard McElreath\nCausal Inference for the Brave and True by Matheus Facure\nCausal Inference the Mixtape, by Scott Cunningham"
  },
  {
    "objectID": "posts/2022-04-05-ab-test-duration.html",
    "href": "posts/2022-04-05-ab-test-duration.html",
    "title": "How long should you run an A/B test for?",
    "section": "",
    "text": "So, How long should you run an A/B test for? Well let’s say you step into a casino with $5000 and you walk away with $6000. You just made a 20% return. Is it fair to say that a night out in the casino leads to a 20% return? Is it fair to say that our A/B test we ran for 2 days leads to a 20% lift in conversion? How do we know for sure?\nWe should run an A/B test for as long as it takes to rule out random chance.\nWhile vague, and technically not the full picture, your friendly neightborhood data scientist should be able to answer this for you. The code for this blogpost can be found here.\n\n\nSimulating a fake scenario\nLet’s play out the casino example from above. I’m going to simulate out an entirely fake, but entirely possible scenario.\nYou go to the casino one night with $5000 and decide roulette is your game of choice. You get a little into it and play 500 rounds (in one night?? for examples sake, yes). Little do you know the real probability of winning is 48.5%\nThe plot below shows the total money you had at the start of each round of roulette\n\n\n\n\n\nThis is great - after 500 rounds of this you’ve made 122% return on your initial investment of $5000 and you’re winning 51% of the time.\nPlaying roulette must lead to a 20% return right? Commited to your strategy you decide to come back over the next few weeks and play another 3000 rounds, shown below.\n\n\n\n\n\nAlright you’ve played 3500 rounds now and you have $5400 total. You’ve definitely had some runs of bad luck, but you’re still seeing a win percentage above 50% (50.1% in fact) and right now you’re heating up. You stay determined and play until you reach 15000 rounds.\n\n\n\n\n\n\n\nWhat happened?\nWe started off on a hot streak winning 51% of our rounds, but as we played more and more rounds, it became more obvious we were losing money. This is a demonstration of the law of large numbers - as we play more and more rounds, the truth comes out\nWe can visualize this process via the beta distribution below. These plots visualize all of the possible values that the true win percentage could be (the x axis), and their relative plausibilities (the y axis). The first plot can be read as follows:\n\nThe win percentage is likely to be somewhere between 42.5% and 60%, with the most likely value being around 51%\n\nAs we move from left to right, our estimated distribution converges closer and closer to the true probability of winning a round\n\n\n\n\n\nWe can also visualize this as a time series, which really makes it clear how the uncertainty becomes smaller over time and the estimated value converges to the true value.\n\n\n\n\n\n\n\nHow does this tie back to A/B testing?\nIf we don’t choose our sample size for an experiment properly, we can end up making the wrong decisions!1 The larger the sample size we choose, the more likely we’ll make the right choice.\nWe can use power analyses (sometimes referred to as simulation studies) to estimate what sample size is needed for an experiment given the desired outcome.\n\n\n\n\n\n\n\nFootnotes\n\n\nUnless you’re using bayesian inference, which can really mitigate this risk.↩︎"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Jul 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kyle Caron",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nIntroduction to Surrogate Indexes\n\n\n\n\n\n\nexperimentation\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\nDesiging an Experimentation Strategy\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n15 min\n\n\n\n\n\n\n\n\nUseful Tools for Weibull Survival Analysis\n\n\n\n\n\n\nsurvival analysis\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n12 min\n\n\n\n\n\n\n\n\nWhy do we need A/B tests? The Potential Outcomes Model\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\nMaking out of sample predictions with PyMC\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nHow long should you run an A/B test for?\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nUncertainty Intervals or p-values?\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\nExplainable AI is not Causal Inference\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kyle Caron",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nexperimentation\n\n\nCausal Inference\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsurvival analysis\n\n\n\n\n\n\n\n\n\n\n\nOct 31, 2022\n\n\n12 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  }
]