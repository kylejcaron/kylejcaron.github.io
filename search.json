[
  {
    "objectID": "posts/experiment_design.html",
    "href": "posts/experiment_design.html",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "",
    "text": "This blog post introduces the Potential Outcomes Model and introduces why experiments are often necessary to measure what we want. This topic is already covered extensively in other more rigorous resources. This post provides just another example."
  },
  {
    "objectID": "posts/experiment_design.html#a-hypothetical-world",
    "href": "posts/experiment_design.html#a-hypothetical-world",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "A Hypothetical world",
    "text": "A Hypothetical world\nWhat if we envision some hypothetical world we can observe the outcome for each customer who reached out to customer support, with and without having the treatment of receiving a promo?\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport scipy.special as sp\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nrng = np.random.default_rng(100)\n\nN=100_000\nupset = rng.normal(0, 1, N)\n\ndef sim_treatment(upset, rng, return_prob=False):\n    beta = 1.5\n    p_treatment = sp.expit( -2.5 + upset * beta)\n    if return_prob:\n        return p_treatment\n    return rng.binomial(1, p_treatment)\n\ndef sim_outcome(upset, treatment, rng):\n    eps = rng.normal(0, 150, size=len(upset))\n    ltv = 2500 + 500*treatment + -500*upset + eps \n    return ltv.round(2)\n\ndata = pd.DataFrame({\n    \"Person\": np.arange(N),\n    \"upset\": upset,\n    \"T\": sim_treatment(upset, rng),\n    \"Y(0)\": sim_outcome(upset, np.zeros(N), rng),\n    \"Y(1)\": sim_outcome(upset, np.ones(N), rng)\n}).set_index(\"Person\")\\\n  .assign(ITE = lambda d: d[\"Y(1)\"] - d[\"Y(0)\"])\\\n  .assign(Y = lambda d: np.where(d[\"T\"] == 1, d[\"Y(1)\"], d[\"Y(0)\"]) )\n\ndata.head()[[\"T\", \"Y(0)\", \"Y(1)\", \"ITE\"]]\n\n\n\n\n\n\n  \n    \n      \n      T\n      Y(0)\n      Y(1)\n      ITE\n    \n    \n      Person\n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      3108.62\n      3583.87\n      475.25\n    \n    \n      1\n      0\n      2347.01\n      2878.23\n      531.22\n    \n    \n      2\n      1\n      2176.28\n      2379.30\n      203.02\n    \n    \n      3\n      0\n      2146.09\n      2559.96\n      413.87\n    \n    \n      4\n      0\n      2806.50\n      3623.16\n      816.66\n    \n  \n\n\n\n\nAs shown above, in this hypothetical world we can see the exact individual treatment effect (ITE) for every customer.\n- Person 0 would have spent $475.25 more over their lifetime  if they received the promo\n- Person 2 would have spend $203.02 more over their lifetime if they received the promo\nIf we want to know the Average Treatment Effect (ATE, often denoted \\(\\tau\\)), all we have to do is take the mean of all of the individual treatment effects. As we can see, the ATE is about $500\n\\[\n\\tau = \\frac{1}{N} \\sum^{N}_{i=0} Y_i(1) - Y_i(0)\n\\]\n\ndata.ITE.mean()\n\n500.09949529999994\n\n\nWe can also represent this in hypothetical terms that will be useful later - the average treatment effect of the treated (ATT), and the average treatment effect of the untreated (ATU). The true ATE ends up being the weighted average of these terms, weighted by the proportion of individuals seeing the treatment, \\(\\pi\\)\n\\[\n\\begin{align}\n\\tau & = \\pi \\cdot E[\\tau | T=1] + (1-\\pi) \\cdot E[\\tau | T= 0] \\\\\n     & = \\pi \\cdot \\text{ATT} + (1-\\pi) \\cdot \\text{ATU}\n\\end{align}\n\\]\nWe can confirm that this is equivalent to the ATE from above with code\n\npi = data[\"T\"].value_counts(normalize=True)\n(pi * data.groupby(\"T\").mean()[\"ITE\"]).sum()\n\n500.0994953"
  },
  {
    "objectID": "posts/experiment_design.html#getting-hit-with-the-real-world",
    "href": "posts/experiment_design.html#getting-hit-with-the-real-world",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Getting hit with the real world",
    "text": "Getting hit with the real world\nSo how can we create a scenario where we can observe each person with and without having receive the promo? Sadly, we can‚Äôt. But is there a way to make use of data we already have? Here‚Äôs the actual data we might have access to. Notice that now the hypothetical potential outcomes are no longer visible (just like in the real world).\n\n\nCode\n# Real world data\ndf = (\n    data[[\"upset\", \"T\", \"Y(0)\", \"Y(1)\", \"ITE\", \"Y\"]]\n    .assign(**{\n        \"Y(0)\":lambda d: np.where(d[\"T\"]==1, np.NaN, d[\"Y(0)\"]),\n        \"Y(1)\":lambda d: np.where(d[\"T\"]==0, np.NaN, d[\"Y(1)\"]),\n        \"ITE\": np.NAN\n        })\n)\n\ndf.iloc[:,1:].head()\n\n\n\n\n\n\n  \n    \n      \n      T\n      Y(0)\n      Y(1)\n      ITE\n      Y\n    \n    \n      Person\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      0\n      3108.62\n      NaN\n      NaN\n      3108.62\n    \n    \n      1\n      0\n      2347.01\n      NaN\n      NaN\n      2347.01\n    \n    \n      2\n      1\n      NaN\n      2379.3\n      NaN\n      2379.30\n    \n    \n      3\n      0\n      2146.09\n      NaN\n      NaN\n      2146.09\n    \n    \n      4\n      0\n      2806.50\n      NaN\n      NaN\n      2806.50\n    \n  \n\n\n\n\nOne (unfortunately incorrect) idea might be take the average of Y(1) and subtract the average of Y(0), also known as the simple difference in outcomes (SDO).\n\\[\n\\text{SDO} = E[ Y(1) | T = 1 ] - E[ Y(0) | T = 0 ]\n\\]\n\nNotice that I use the terms \\(E[ Y(0) | T = 0 ]\\) and \\(E[ Y(1) | T = 1 ]\\). Reading these as plain english ‚Äúthe expected value (aka mean) of Y(0) given no treatment‚Äù and ‚Äúthe expected value (aka mean) of Y(1) given receiving a treatment‚Äù\n\n\n(\n    df.groupby(\"T\")\n    .mean()[[\"Y\"]].T\n    .assign(tau = lambda d: d[1] - d[0])\n    .rename(columns={0:\"E[ Y(0) | T = 0 ]\", 1:\"E[ Y(1) | T = 1 ]\"})\n    .rename_axis(None, axis=1)\n    .round(2)\n    .reset_index(drop=True)\n)\n\n\n\n\n\n  \n    \n      \n      E[ Y(0) | T = 0 ]\n      E[ Y(1) | T = 1 ]\n      tau\n    \n  \n  \n    \n      0\n      2579.46\n      2491.48\n      -87.98\n    \n  \n\n\n\n\nUnder the SDO it looks like the treatment has a negative effect - this is saying that giving customers a promo makes their LTV worse? That seems seriously wrong, and is a huge problem. It should be $500 like we saw in our hypothetical world. So what went wrong?"
  },
  {
    "objectID": "posts/experiment_design.html#selection-bias",
    "href": "posts/experiment_design.html#selection-bias",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Selection Bias",
    "text": "Selection Bias\nWe can illustrate the problem by bringing another variable into the mix - customer unhappiness (we‚Äôre pretending we can measure it directly for examples sake).\n\nfig, ax = plt.subplots(1,2, figsize=(8,3))\nax[0].set_title(\"Histogram of\\nCustomer unhappiness\")\ndf.upset.hist(ax=ax[0])\n\nax[1].set_title(\"More upset customers are\\nmore likely to receive a promo\")\nax[1].set_ylabel(\"Proportion Receiving Promo\")\ndf.groupby(df.upset//0.25*0.25).mean()[\"T\"].plot(ax=ax[1])\nplt.tight_layout()\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      upset\n      T\n      Y(0)\n      Y(1)\n      ITE\n      Y\n    \n    \n      Person\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      -1.157550\n      0\n      3108.62\n      NaN\n      NaN\n      3108.62\n    \n    \n      1\n      0.289756\n      0\n      2347.01\n      NaN\n      NaN\n      2347.01\n    \n    \n      2\n      0.780854\n      1\n      NaN\n      2379.3\n      NaN\n      2379.30\n    \n    \n      3\n      0.543974\n      0\n      2146.09\n      NaN\n      NaN\n      2146.09\n    \n    \n      4\n      -0.961383\n      0\n      2806.50\n      NaN\n      NaN\n      2806.50\n    \n  \n\n\n\n\n\n\n\nIt looks like the most unhappy customers are the most likely to receive a treatment as shown in the DAG below.\n\n\n\n\n\n\n\nG\n\n  \n\na\n\n unhappy customer   \n\nb\n\n receive promo   \n\na->b\n\n    \n\nc\n\n lifetime value   \n\nc->a\n\n    \n\nc->b\n\n   \n\n\n\n\n\nThis is an example of selection bias (more specifically, its collider bias, a common confound). When comparing customers who had the treatment vs. didnt have the treatment, we accidentally also end up comparing unhappy customers vs.¬†happier customers, and obviously unhappier customers tend to have worse lifetime value. We need to find a way to compare the impact of the treatment while controlling for the happiness of customers so that we are making a more fair comparison. For example, if we had 2 equally unhappy customers and 1 received the treatment while the other didnt, we‚Äôd get a more reasonable comparison for evaluating the treatment effect."
  },
  {
    "objectID": "posts/experiment_design.html#identification-under-selection-bias",
    "href": "posts/experiment_design.html#identification-under-selection-bias",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "Identification Under Selection bias",
    "text": "Identification Under Selection bias\nHow can we represent the scenario above with math? This is where the Potential Outcomes model starts coming into play. Note I‚Äôm borrowing this directly from Scott Cunningham. For the full proof, see his book, Causal Inference the Mixtape.\n\\[\n\\begin{align}\n\\text{Simple Difference in Outcomes}\n&= \\underbrace{E[Y^1] - E[Y^0]}_{ \\text{Average Treatment Effect}}\\\\\n&+ \\underbrace{E\\big[Y^0\\mid T=1\\big] - E\\big[Y^0\\mid T=0\\big]}_{ \\text{Selection bias}}\\\\\n& + \\underbrace{(1-\\pi)(ATT - ATU)}_{ \\text{Heterogeneous treatment effect bias}}\n\\end{align}\n\\]\nThis equation for the Potential Outcomes model basically says that anytime you make a comparison on observational data, it ends up being the sum of the true average treatment effect, selection bias, and Heterogeneous Treatment effect (HTE) bias. HTEs are just a fancy way of saying the personalized effect, aka promos might be more impactful for some users than others.\nSo how does this relate to what we did before? Well when we tried to compare users who saw the treatment vs.¬†those that didnt\n\\[\n\\text{SDO} = E[ Y(1) | T = 1 ] - E[ Y(0) | T = 0 ]\n\\]\nwe didnt take into account the fact that users who saw the treatment tend to be different than those who dont. Users who saw the treatment tend to be more unhappy by design.\nSo if we subtract out the selection bias from the SDO, aka we control for the unhappiness between customers, we can get closer to identifying the true ATE.\nNote that selection bias was \\[\nE\\big[Y^0\\mid T=1\\big] - E\\big[Y^0\\mid T=0\\big]\n\\]\nThis is just saying selection bias is the fundamental difference between users who get picked for treatment vs.¬†those who dont.\nIn our case, the fundamental difference between whether users are selected for treatment is based upon their unhappiness. So if we can subtract out the effect of unhappiness, we can subtract out the selection bias\n\ndf.groupby(\"T\").mean()[[\"upset\"]].T\n\n\n\n\n\n  \n    \n      T\n      0\n      1\n    \n  \n  \n    \n      upset\n      -0.159046\n      1.018004\n    \n  \n\n\n\n\nWe can do this with OLS. The most obvious way is to fit a model relating unhappiness to LTV, and then subtract out that effect.\n\nmodel1 = sm.OLS.from_formula(\"Y ~ upset\", data=df.loc[lambda d: d[\"T\"]==0]).fit()\nY0_hat = model1.predict(df)\n\nselection_bias = (\n    df.assign(selection_bias = Y0_hat)\n    .groupby(\"T\").mean()\n    [[\"selection_bias\"]]\n)\nselection_bias.T.round(2)\n\n\n\n\n\n  \n    \n      T\n      0\n      1\n    \n  \n  \n    \n      selection_bias\n      2579.46\n      1990.96\n    \n  \n\n\n\n\nAnd finally we can subtract out the effect, ending up with an estimate very close to the true ATE of 500\n\n(\n    df.assign(selection_bias = Y0_hat)\n    .assign(bias_corrected_Y = lambda d: d.Y - d.selection_bias)\n    .groupby(\"T\").mean()\n    [[\"bias_corrected_Y\"]].T\n    .assign(tau = lambda d: d[1] - d[0])\n    .reset_index(drop=True)\n    .rename(columns={0:\"bias corrected E[Y|T=0] \",1:\"bias corrected  E[Y|T=1]\"})\n    .rename_axis(columns=None)\n)\n\n\n\n\n\n  \n    \n      \n      bias corrected E[Y|T=0]\n      bias corrected  E[Y|T=1]\n      tau\n    \n  \n  \n    \n      0\n      -5.925096e-13\n      500.524691\n      500.524691\n    \n  \n\n\n\n\nThere‚Äôs actually an even more simple way to control for selection bias - it can just be included as a term in an OLS regression model.\n\nmodel2 = sm.OLS.from_formula(\" Y ~ T + upset\", data=df).fit()\ntable = np.array(model2.summary().tables[1].data)\npd.DataFrame(table[1:, 1:], columns=table[0,1:], index=table[1:,0])\n\n\n\n\n\n  \n    \n      \n      coef\n      std err\n      t\n      P>|t|\n      [0.025\n      0.975]\n    \n  \n  \n    \n      Intercept\n      2499.9363\n      0.516\n      4847.317\n      0.000\n      2498.925\n      2500.947\n    \n    \n      T\n      500.5529\n      1.502\n      333.191\n      0.000\n      497.608\n      503.497\n    \n    \n      upset\n      -500.0068\n      0.518\n      -965.940\n      0.000\n      -501.021\n      -498.992\n    \n  \n\n\n\n\nAs we can see above the estimate of the treatment effect is the beta coefficient for T and it closely matches our manual estimate above."
  },
  {
    "objectID": "posts/experiment_design.html#a-quick-note-on-heterogeneous-treatment-effects",
    "href": "posts/experiment_design.html#a-quick-note-on-heterogeneous-treatment-effects",
    "title": "Why do we need A/B tests? The Potential Outcomes Model",
    "section": "A quick note on Heterogeneous Treatment Effects",
    "text": "A quick note on Heterogeneous Treatment Effects\nWe‚Äôve controlled for selection bias, what about Heterogeneous Treatment Effect bias? We actually don‚Äôt need to control for these once we‚Äôve controlled for selection bias. These average treatment effect ends up being the average of all of the HTEs of individuals, which is fine because as long as we‚Äôve accounted for selection bias, the HTEs tend to cancel out. They‚Äôre essentially captured by the error term, \\(\\epsilon\\) in OLS \\[\ny = \\alpha + \\beta X + \\epsilon\n\\]\nWe can also see that in our code, where the distribution of true HTE bias from our hypothetical dataset is centered at zero. Any time we‚Äôve accounted for all selection bias, the HTE should be zero centered and cancel itself out as N increases.\n\nATE = data[\"ITE\"].mean()\nHTE = data.ITE.values - ATE\nsns.histplot(HTE)\nplt.xlabel(\"HTE\")\nplt.title(\"Distribution of HTEs (each customers difference from the ATE)\")\nplt.show()\n\n\n\n\nThe bias of HTEs for each person is just the distance their treatment effect is from the average treatment effect. Again, this follows the same property as the error term in OLS regression, which is why it can be such a powerful tool for causal inference when used correctly."
  },
  {
    "objectID": "posts/2022-04-17-out-of-sample-pymc.html",
    "href": "posts/2022-04-17-out-of-sample-pymc.html",
    "title": "Making out of sample predictions with PyMC",
    "section": "",
    "text": "Simulating data\nI simulated a 2 level hierarchical model - for interpretability, I set it up as a state > zipcode model. You can following along with the notebook here. The data is as follows\n\n\n\n\n\n\n\nUsing categorical variables\nCategorical variables are a somewhat new feature of pandas - they can store categories that aren‚Äôt in the observed data, and are an easy replacement for pd.factorize() (a common tool for those familiar with the bayesian workflow).\nWe can use these to trick pymc into thinking there‚Äôs a category with no observed data, and pymc ends up assigning the global distribution to that unobserved category, which we can simply reference in the future for any time we want to make a prediction on out of sample data.\n# Convert to categorical and add an `out_of_sample` category\ndf = df.assign(state = pd.Categorical(df.state).add_categories(\"out_of_sample\"))\\\n    .assign(zipcode = pd.Categorical(df.zipcode).add_categories(\"out_of_sample\"))\n\n\nFitting the model\nWe‚Äôll use the codes from the categorical columns to index our model coefficients, and we‚Äôll use the categories as coordinates for the model to map names to.\ncoords={\n    \"state\":df.state.cat.categories,\n    \"zipcode\":df.zipcode.cat.categories\n}\n\ndef hierarchical_normal(name, Œº, dims):\n    '''Adapted from Austin Rochford'''\n    Œî = pm.Normal('Œî_{}'.format(name), 0., 1., dims=dims)\n    œÉ = pm.Exponential('œÉ_{}'.format(name), 2.5)\n    return pm.Deterministic(name, Œº + Œî * œÉ, dims=dims)\n\n\nwith pm.Model(coords=coords) as model_nc:\n    \n    # Observed Data tracking\n    state_ = pm.Data(\"state_\", df.state.cat.codes)\n    zip_ = pm.Data(\"zip_\", df.zipcode.cat.codes)\n    obs = pm.Data(\"obs\", df.y)\n\n    # Hyperprior\n    mu_country = pm.Normal(\"mu_country\", 0, 1)\n    \n    # Prior\n    sig = pm.Exponential(\"sig\", 1)\n    \n    # Hierarchical coefficients\n    mu_state = hierarchical_normal(\"mu_state\", Œº=mu_country, dims=\"state\")\n    mu_zipcode = hierarchical_normal(\"mu_zipcode\", Œº=mu_state, dims=(\"zipcode\", \"state\") )\n    \n    # Observational model\n    y = pm.Normal(\"y\", mu_zipcode[zip_, state_], sig, observed=obs)\n    \n    # Fit \n    trace_nc = pm.sample(target_accept=0.9, return_inferencedata=True, random_seed=SEED)\nThere are a few key point that make out of sample prediction possible * Having the out_of_sample category for each indexed variable with no observed data * Passing the coords in the model statement * Using dims to reference which model coefficients have which coordinate labels * Having all of our input data wrapped in a pm.Data() statement\nThat last point is particularly important. For PyMC, if you want to make predictions on new data, you have to replace the data that the model references and the only way to do that (that I know of atleast) is to using a Theano shared variable. pm.Data() handles all of that fo you.\nSo we fit our model, lets take a quick look at the state level coefficients\npm.plot_forest(trace_nc, var_names=[\"mu_state\"])\n\n\n\n\n\nGreat, that out of sample variable seems to represent the global distribution across states - i.e.¬†if we were to make a prediction for a new state we‚Äôd potentially use that distribtion (we‚Äôll confirm further down).\nWe‚Äôll check the zip code level below as well, looking at Maine specifically\n\n\n\n\n\nAs we can see, the out_of_sample variable has a sampled value despite there being no observed data for it. Now the question is, does this align with how we‚Äôd predict new data?\nLet‚Äôs try calculating coefficients out of sample by hand and see if it aligns with the out_of_sample values\npost = trace_nc.posterior\n\n# Pull the true data from our simulation\nstate_true = mu_state_true.random(size=4000)\n\n\n# Calculate out of sample state means by drawing from global distribution\nmu_country = post[\"mu_country\"].values.reshape(4000,-1)\nœÉ_state = post[\"œÉ_mu_state\"].values.reshape(4000,-1)\nmu_state = np.random.normal(mu_country, œÉ_state)\n\n# Using the indexing trick\nstate_idx_trick = post[\"mu_state\"].sel({\"state\":[\"out_of_sample\"]}).values.ravel()\n\n# Pull the true data from simulation\nzip_true = pm.Normal.dist(mu_state_true.random(size=4000), sig_zip_true).random(size=4000)\n\n# calculate out of sample mu by hand by drawing from out of sample state prediction above\nœÉ_zipcode = post[\"œÉ_mu_zipcode\"].values.reshape(4000,-1)\nmu_zipcode = np.random.normal(mu_state, œÉ_zipcode)\n\n# Use the indexing trick\nzip_idx_trick = (post[\"mu_zipcode\"]\n                .sel({\"state\":[\"out_of_sample\"], \"zipcode\":[\"out_of_sample\"]})\n                .values.ravel())\nWe can compare these results by plotting their distributions below\n\n\n\n\n\nNotice that the manual prediction and the indexing trick are basically identical. There‚Äôs a slight difference from the ground truth, but thats to be expected since we‚Äôre fitting a model on limited data (and anyway, it‚Äôs still quite close).\n\n\nPredicting out of sample\nLet‚Äôs go ahead and actually make prediction now - we‚Äôll make predictions for the following data below\n\nThe first example is in sample\nThe second example is in sample for state, out of sample for zipcode\nThe third example is out of sample entirely\n\n\n\n\n\n\nAnd finally we‚Äôll use the model to make predictions on this new data. Notice the pm.set_data() function - remember our pm.Data() calls from before? This tells PyMC to override that with new data, so when we sample from the posterior predictive it makes predictions on the new data instead of the data used to fit the model.\n\n\nClick here for helper function code\n\n# We're making some quick convenience functions to map this new data \n# to the proper indexes from the fitted model\nzip_lookup = dict(zip(df.zipcode.cat.categories, range(len(df.zipcode.cat.categories))))\nstate_lookup = dict(zip(df.state.cat.categories, range(len(df.state.cat.categories))))\n\ndef labels_to_index(series, lookup):\n    '''Converts categories to their proper codes'''\n    series = series.copy()\n    in_sample = series.isin(lookup.keys())\n    series.loc[~in_sample] = \"out_of_sample\"\n    return series.map(lookup).values.astype(\"int8\")\n\n\nwith model_nc:\n    # Set new data for the model to make predictions on\n    pm.set_data({\n        \"state_\": X.state.pipe(labels_to_index, state_lookup),\n        \"zip_\": X.zipcode.pipe(labels_to_index, zip_lookup)\n    })\n    \n    # make predictions\n    preds = pm.sample_posterior_predictive(trace_nc)\n\n\n\n\n\nThis is exactly what we were looking for - and prediction is easy, just map any out of sample states or zipcodes to the out_of_sample category. Notice how in sample predictions have smaller uncertainty intervals and out of sample data is more uncertain - this is exactly what we‚Äôd expect. This trick makes it much easier to make predictions compared to having to write out a custom prediction function that follows the same logic as the model.\nIf you have any other easy tricks for out of sample prediction let me know!"
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html",
    "title": "Uncertainty Intervals or p-values?",
    "section": "",
    "text": "Uncertainty Intervals are better than p-values. Sure, its better to use both, but p-values are just a point estimate and they bring no concept of uncertainty in our estimate - this can lead to situations where we expose ourselves to high downside risk.\nTake the following example for instance. Let‚Äôs say we‚Äôre running a ‚ÄúDo no harm‚Äù A/B test where we want to roll out an experiment as long as it doesnt harm conversion rate.\nIf you want to follow along with the code, see here."
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#the-experiment-design",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#the-experiment-design",
    "title": "Uncertainty Intervals or p-values?",
    "section": "The experiment design",
    "text": "The experiment design\nGiven the stakeholders want to rule out a drop in conversion, and ruling out small differences requires large sample sizes, we decide to design an experiment with good power to detect the presence of a 1/2% absolute drop (if one were to truly exist)\nWe ran a power analysis and found that in order to have a 90% probability of detecting (power=0.9) a 1/2% absolute drop in conversion rate with 80 percent confidence ( ùõº=0.2 ), we need N=32500 per group\n\nStatisticians might not love this interpretation of a power analysis, but its a useful and interpretable translation and tends to coincide with what we‚Äôre aiming for anyway. In reality, frequentist power analyses assume that the null hypothesis is correct, which isn‚Äôt quite what we want, not to mention, frequentist power analyses use backwards probabilities which are just plain confusing - see here to for more\n\nNote that we‚Äôre prioritizing power here for a reason. If ùõº is false positive rate, and power is probability of detection, then don‚Äôt we want to prioritize our probability of detecting a drop if one truly exists? A false negative here would be more expensive then a false positive\npA = 0.1 # historical conversion rate\nabs_delta = 0.005 # minimum detectable effect to test for\n\n# Statsmodels requires an effect size \n# (aka an effect normalized by its standard deviation)\nstdev = np.sqrt( pA*(1-pA) ) # bernoulli stdev, sigma = sqrt(p(1-p))\nES = abs_delta / stdev \n\n# estimate required sample size\nsm.stats.tt_ind_solve_power(\n    -ES, \n    alpha=0.2,\n    power=0.9,\n    alternative=\"smaller\"\n)\nRunning the code above leads us to conclude are sample size should be roughly 32,500 users per group.\n\nThe experiment\nI‚Äôm going to simulate fake data for this experiment where * The control has a true conversion rate of 10% * the variant has a true conversion rate of 9.25%\nFor examples sake we‚Äôll pretend we don‚Äôt know that the variant is worse\n\n\nClick here for code\n\n# Settings\nnp.random.seed(1325)\nN = 32500\npA = 0.1\npB = 0.0925\n\n# Simulation\ndef simulate_experiment(pA, pB, N_per_group):\n    \n    df = pd.DataFrame({\n        \"group\":[\"A\"]*N + [\"B\"]*N,\n        \"convert\":np.r_[\n             np.random.binomial(1, p=pA, size=N),\n             np.random.binomial(1, p=pB, size=N)\n        ]\n    })\n    \n    return df\n\ndf = simulate_experiment(pA, pB, N)\n\n\n\n\n\n\n\nLooking at the data above, we‚Äôre seeing a better conversion rate in group B. We run a two-proportions z-test and we find that there‚Äôs a non-significant p-value, meaning we found insufficient evidence of the variant having lower conversion than the control.\ndef pval_from_summary(tab):\n    \n    _, pval = sm.stats.proportions_ztest(\n        count=tab[\"converts\"][::-1], \n        nobs=tab[\"N\"][::-1],\n        alternative=\"smaller\"\n    )\n    return pval\n\n(df.pipe(summarize_experiment)\n   .pipe(pval_from_summary))\n\\[ p = 0.38 \\]\nWe recommend to our stakeholders to roll out the variant since it ‚Äúdoes no harm‚Äù\nThere are some serious red flags here\n\nFirst of all, p-values are all about the null hypothesis. So just because we don‚Äôt find a significant drop in conversion rate, that doesnt mean one doesnt exist. It just means we didnt find evidence for it in this test\nThere was no visualization of the uncertainty in the result"
  },
  {
    "objectID": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#understanding-uncertainty-with-the-beta-distribution",
    "href": "posts/2022-04-11-uncertainty_intervals_over_pvals.html#understanding-uncertainty-with-the-beta-distribution",
    "title": "Uncertainty Intervals or p-values?",
    "section": "Understanding Uncertainty with the Beta Distribution",
    "text": "Understanding Uncertainty with the Beta Distribution\nFor binary outcomes, the beta distribution is highly effective for understanding uncertainty.\nIt has 2 parameters * alpha, the number of successes * beta, the number of failures\nIt‚Äôs output is easy to interpret: Its a distribution of plausible probabilities that lead to the outcome.\nSo we can simply count our successes and failures from out observed data, plug it into a beta distribution to simulate outcomes, and visualize it as a density plot to understand uncertainty\n\n\n\n\n\nIt‚Äôs also easy to work with - if we want to understand the plausible differences between groups, we can just take the differences in our estimates\n\\[\n\\delta = \\hat{p_B} - \\hat{p_A}\n\\]\n\n\n\n\n\nWith visualization, we get a very different picture than our non-significant p-value. We see that there‚Äôs plenty of plausibility that the control could be worse.\nWe can further calculate the probability of a drop, \\(P(B < A)\\), and find that theres a greater than 60% probability that the variant is worse than the control\n# P(B < A)\n(pB_hat < pA_hat).mean()\n\\[\nP(B < A) = 0.62\n\\]\nRemember when we designed the experiment? Considering our main goal was to do no harm, we might not feel so confident in that now, and rightly so, we know the variants worse since we simulated it.\nUnless we feel very confident in our choice of testing for a 1/2% drop and know that we can afford anything up to that, then we we really shouldnt roll out this variant without further evaluation\nThis is particularly important with higher uncertainty As we can see in the example below, where the observed conversion rate is better in the variant, but the downside risk is as high as a 4% drop in conversion rate\n\n\n\n\n\n\\[\np = 0.59\n\\]"
  },
  {
    "objectID": "posts/2022-04-03-explainable-ai-is-not-causal.html",
    "href": "posts/2022-04-03-explainable-ai-is-not-causal.html",
    "title": "Explainable AI is not Causal Inference",
    "section": "",
    "text": "We‚Äôre going to walk through an example that shows these tools fall victim to the same rules of causal inference as everything else. A confound is still a confound, and if you want to measure some causal effect there‚Äôs still no way around that without careful deliberation of which variables to include in your models.\nThe code for this blogpost can be found here\n\nStarting simple: simulating some fake data\nLet‚Äôs start with a simple scenario. Our goal is to estimate some causal effects. We‚Äôre going to simulate out data ourself so we know the true causal effects. We can see how good some popular ‚Äúexplainable AI‚Äù algorithms actually are at causal inference. We‚Äôll simulate data from the following DAG:\n\n\n\n\n\n\nWhat‚Äôs a DAG? A dag is a directed acyclic graph, or fancy talk for a flowchart that goes in 1 direction. It‚Äôs really just a diagram of a true data generating process. They‚Äôre typically assumed based on domain knowledge (like all models), although ocassionally there are some validation checks you can perform.\nEdges in the graph are assumed to be true causal effects. So for example,\n\nX3 influences Y\nX5 influences X1 which influences Y\nSome unobserved variable U influences both X1 and Y. By unobserved, what I mean is that its some variable we don‚Äôt have data for.\n\nFor those familiar with causal inference, this DAG in particular is also riddled with confounds.\n\nOk back on track. We‚Äôll get out one of the more popular Explainable AI tools nowadays, XGBoost. I‚Äôm going to start in the most dangerous way possible - I‚Äôm going to toss everything in the model.\n\n\nTest 1: What‚Äôs the impact of X1 on Y?\nWe know for a fact that X1 influences Y. Let‚Äôs see how well Partial Dependence Plots and SHAP values do at identifying the true causal effect\n\n\n\n\n\nThese SHAP values arent just wrong, but the effect is in the wrong direction. The reason for this: there‚Äôs a Fork Confound.\n\n\n\n\n\nSome variable Z confounds Xs true effect on Y.\n\nA very common example of a fork confound is warm weather (Z) on the relationship between ice cream sales (X) and crime (Y). Ice cream sales obviously have no influence on crime, but ice cream sales are higher during warmer weather, and crime is higher during warmer weather.\n\nSo back to our main point - Explainable AI can‚Äôt get around a fork confound. This is our first lesson on why SHAP / explainable AI is different from causal inference.\nLuckily in this case, statistics can solve this problem.\nUsing some domain knowledge about the generating process, we notice an instrument, X5, that can be used to estimate the causal effect of X1 on Y\n\n\n\n\n\nI won‚Äôt go into the details of instrumental variable analysis since the goal of this article is to highlight that Explainable AI can‚Äôt replace causal inference. To learn more about it, see Scott Cunningham‚Äôs Causal Inference the Mixtape.\nBut for now, I will show that a classic causal inference method succeeds where XGBoost and SHAP values fail\nfrom linearmodels import IV2SLS\nfrom src.dagtools import get_effect\n\n# Instrumental variable analysis\niv_model = IV2SLS.from_formula(\"Y ~ 1 + [X1 ~ X5]\", data=df).fit()\n\n# pull true effect\ntrue_effect = get_effect(DAG, \"X1\", \"Y\")\n\n# Plot\nfig, ax = plt.subplots(1,1,figsize=(6,4))\nax.set_title(\"Instrumental Variable Analysis\\nRecovers True effect\")\nplot_model_estimate(iv_model, true_effect=true_effect, feat=\"X1\", ax=ax)\n\n\n\n\n\nAs we can see, a simple statistics technique succeeds where explainable AI fails.\n\n\nWhat about estimating the effect of X4 on Y?\nThis relationship is slightly more complicated, but certainly measurable. X4 influences X2 which influences Y. Here‚Äôs the DAG again for reference\n\n\n\n\n\nThe plots below show how well explainable AI does at estimating the causal effect of this relationship.\n\n\n\n\n\nUnfortunately, they don‚Äôt pick up an effect at all! And if our goal was to increase Y we‚Äôd end up missing a pretty good lever for it. There‚Äôs another simple explanation here for why explainable AI: there‚Äôs a Pipe Confound\n\n\n\n\n\nWhen trying to measure the effect of X -> Y, conditioning on Z (aka including it in a model as a covariate with X) ends up blocking inference.\nFor more details on how a Pipe confound works, I recommend chapters 5 and 6 of Richard McElreath‚Äôs Statistical Rethinking v2 (where I borrowed the example from as well).\nThe main things to note here are that pipes are common and Explainable AI doesn‚Äôt get around them.\nWe can recover an unbiased estimate of the true effect simply with OLS\n# Fit simple OLS model\nmodel = sm.OLS.from_formula(\"Y ~ X4\", data=df).fit()\n\n# pull true effect\ntrue_effect = get_effect(DAG, \"X4\", \"X2\") * get_effect(DAG, \"X2\", \"Y\")\n\n# Plot (see notebok for plot_model_estimate function)\nfig, ax = plt.subplots(1,1,figsize=(6,4))\nax.set_title(\"Instrumental Variable Analysis\\nRecovers True effect\")\nplot_model_estimate(model, true_effect=true_effect, feat=\"X4\", ax=ax)\n\n\n\n\n\n\n\nCan we use Explainable AI for causal inference at all?\nWe can! We just need to be deliberate in which variables we include in our models, and the only way to do that right is to use DAGs! The example below looks at an XGBoost model that doesnt condition on X2 (allowing us to estimate the causal effect of X4 -> Y).\n[](/assets/img/explainable_ai_fig9.png]{fig-align=‚Äúcenter‚Äù}\n\n\nTake Aways\nExplainable AI is not some magic tool for causal inference. What these tools are good at is explaining why complicated models make the decisions they do. Explainable AI tools suffer from the same limitations for causal inference as all other statistical estimators.\nAt the end of the day when causal inference is your goal, nothing beats using DAGs to inform deliberate variable selection.\nIf you‚Äôre new to the subject, I highly recommend the following resources that will teach you how to use causal inference properly: * Chapter‚Äôs 5 and 6 of Statistical Rethinking v2, by Richard McElreath * Causal Inference for the Brave and True by Matheus Facure * Causal Inference the Mixtape, by Scott Cunningham"
  },
  {
    "objectID": "posts/2022-04-05-ab-test-duration.html",
    "href": "posts/2022-04-05-ab-test-duration.html",
    "title": "How long should you run an A/B test for?",
    "section": "",
    "text": "So, How long should you run an A/B test for? Well let‚Äôs say you step into a casino with $5000 and you walk away with $6000. You just made a 20% return. Is it fair to say that a night out in the casino leads to a 20% return? Is it fair to say that our A/B test we ran for 2 days leads to a 20% lift in conversion? How do we know for sure?\nWe should run an A/B test for as long as it takes to rule out random chance.\nWhile vague, and technically not the full picture, your friendly neightborhood data scientist should be able to answer this for you. The code for this blogpost can be found here.\n\n\nSimulating a fake scenario\nLet‚Äôs play out the casino example from above. I‚Äôm going to simulate out an entirely fake, but entirely possible scenario.\nYou go to the casino one night with $5000 and decide roulette is your game of choice. You get a little into it and play 500 rounds (in one night?? for examples sake, yes). Little do you know the real probability of winning is 48.5%\nThe plot below shows the total money you had at the start of each round of roulette\n\n\n\n\n\nThis is great - after 500 rounds of this you‚Äôve made 122% return on your initial investment of $5000 and you‚Äôre winning 51% of the time.\nPlaying roulette must lead to a 20% return right? Commited to your strategy you decide to come back over the next few weeks and play another 3000 rounds, shown below.\n\n\n\n\n\nAlright you‚Äôve played 3500 rounds now and you have $5400 total. You‚Äôve definitely had some runs of bad luck, but you‚Äôre still seeing a win percentage above 50% (50.1% in fact) and right now you‚Äôre heating up. You stay determined and play until you reach 15000 rounds.\n\n\n\n\n\n\n\nWhat happened?\nWe started off on a hot streak winning 51% of our rounds, but as we played more and more rounds, it became more obvious we were losing money. This is a demonstration of the law of large numbers - as we play more and more rounds, the truth comes out\nWe can visualize this process via the beta distribution below. These plots visualize all of the possible values that the true win percentage could be (the x axis), and their relative plausibilities (the y axis). The first plot can be read as follows:\n\nThe win percentage is likely to be somewhere between 42.5% and 60%, with the most likely value being around 51%\n\nAs we move from left to right, our estimated distribution converges closer and closer to the true probability of winning a round\n\n\n\n\n\nWe can also visualize this as a time series, which really makes it clear how the uncertainty becomes smaller over time and the estimated value converges to the true value.\n\n\n\n\n\n\n\nHow does this tie back to A/B testing?\nIf we don‚Äôt choose our sample size for an experiment properly, we can end up making the wrong decisions!1 The larger the sample size we choose, the more likely we‚Äôll make the right choice.\nWe can use power analyses (sometimes referred to as simulation studies) to estimate what sample size is needed for an experiment given the desired outcome.\n\n\n\n\n\n\n\nFootnotes\n\n\nUnless you‚Äôre using bayesian inference, which can really mitigate this risk.‚Ü©Ô∏é"
  },
  {
    "objectID": "archive.html",
    "href": "archive.html",
    "title": "Archive",
    "section": "",
    "text": "Sep 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kyle Caron",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nWhy do we need A/B tests? The Potential Outcomes Model\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\nMaking out of sample predictions with PyMC\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\nHow long should you run an A/B test for?\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nUncertainty Intervals or p-values?\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\nExplainable AI is not Causal Inference\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Kyle Caron",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\n\nexperimentation\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2022\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2022\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2022\n\n\n4 min\n\n\n\n\n\n\nNo matching items"
  }
]